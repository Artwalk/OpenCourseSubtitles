1
00:00:00,140 --> 00:00:04,340

welcome to course six as $0.99

2
00:00:04,340 --> 00:00:04,350
welcome to course six as $0.99
 

3
00:00:04,350 --> 00:00:07,490
welcome to course six as $0.99
artificial general intelligence we will

4
00:00:07,490 --> 00:00:07,500
artificial general intelligence we will
 

5
00:00:07,500 --> 00:00:11,209
artificial general intelligence we will
explore the nature of intelligence from

6
00:00:11,209 --> 00:00:11,219
explore the nature of intelligence from
 

7
00:00:11,219 --> 00:00:13,580
explore the nature of intelligence from
as much as possible and engineering

8
00:00:13,580 --> 00:00:13,590
as much as possible and engineering
 

9
00:00:13,590 --> 00:00:18,529
as much as possible and engineering
perspective you will hear many voices my

10
00:00:18,529 --> 00:00:18,539
perspective you will hear many voices my
 

11
00:00:18,539 --> 00:00:23,179
perspective you will hear many voices my
voice will be that of an engineer our

12
00:00:23,179 --> 00:00:23,189
voice will be that of an engineer our
 

13
00:00:23,189 --> 00:00:29,419
voice will be that of an engineer our
mission is to engineer intelligence the

14
00:00:29,419 --> 00:00:29,429
mission is to engineer intelligence the
 

15
00:00:29,429 --> 00:00:32,959
mission is to engineer intelligence the
MIT motto is mind in hand what that

16
00:00:32,959 --> 00:00:32,969
MIT motto is mind in hand what that
 

17
00:00:32,969 --> 00:00:37,430
MIT motto is mind in hand what that
means is we want to explore the

18
00:00:37,430 --> 00:00:37,440
means is we want to explore the
 

19
00:00:37,440 --> 00:00:41,000
means is we want to explore the
fundamental science of what makes an

20
00:00:41,000 --> 00:00:41,010
fundamental science of what makes an
 

21
00:00:41,010 --> 00:00:44,889
fundamental science of what makes an
intelligence system the core concepts

22
00:00:44,889 --> 00:00:44,899
intelligence system the core concepts
 

23
00:00:44,899 --> 00:00:48,049
intelligence system the core concepts
behind our understanding of what is

24
00:00:48,049 --> 00:00:48,059
behind our understanding of what is
 

25
00:00:48,059 --> 00:00:50,690
behind our understanding of what is
intelligence but we always want to

26
00:00:50,690 --> 00:00:50,700
intelligence but we always want to
 

27
00:00:50,700 --> 00:00:54,860
intelligence but we always want to
ground it in the creation of intelligent

28
00:00:54,860 --> 00:00:54,870
ground it in the creation of intelligent
 

29
00:00:54,870 --> 00:00:57,709
ground it in the creation of intelligent
systems we always want to be in the now

30
00:00:57,709 --> 00:00:57,719
systems we always want to be in the now
 

31
00:00:57,719 --> 00:01:01,580
systems we always want to be in the now
in today in understanding how today we

32
00:01:01,580 --> 00:01:01,590
in today in understanding how today we
 

33
00:01:01,590 --> 00:01:03,260
in today in understanding how today we
can build artificial intelligence

34
00:01:03,260 --> 00:01:03,270
can build artificial intelligence
 

35
00:01:03,270 --> 00:01:05,560
can build artificial intelligence
systems that can make for a better world

36
00:01:05,560 --> 00:01:05,570
systems that can make for a better world
 

37
00:01:05,570 --> 00:01:09,219
systems that can make for a better world
that is the core for us here at MIT

38
00:01:09,219 --> 00:01:09,229
that is the core for us here at MIT
 

39
00:01:09,229 --> 00:01:12,800
that is the core for us here at MIT
first and foremost we're scientists and

40
00:01:12,800 --> 00:01:12,810
first and foremost we're scientists and
 

41
00:01:12,810 --> 00:01:15,620
first and foremost we're scientists and
engineers our goal is to engineer

42
00:01:15,620 --> 00:01:15,630
engineers our goal is to engineer
 

43
00:01:15,630 --> 00:01:21,320
engineers our goal is to engineer
intelligence we want to provide with

44
00:01:21,320 --> 00:01:21,330
intelligence we want to provide with
 

45
00:01:21,330 --> 00:01:25,280
intelligence we want to provide with
this approach a balance to them very

46
00:01:25,280 --> 00:01:25,290
this approach a balance to them very
 

47
00:01:25,290 --> 00:01:30,050
this approach a balance to them very
important but over represented view of

48
00:01:30,050 --> 00:01:30,060
important but over represented view of
 

49
00:01:30,060 --> 00:01:32,510
important but over represented view of
artificial general intelligence the

50
00:01:32,510 --> 00:01:32,520
artificial general intelligence the
 

51
00:01:32,520 --> 00:01:36,560
artificial general intelligence the
black box reasoning view where the idea

52
00:01:36,560 --> 00:01:36,570
black box reasoning view where the idea
 

53
00:01:36,570 --> 00:01:40,190
black box reasoning view where the idea
is once we know how to create a human

54
00:01:40,190 --> 00:01:40,200
is once we know how to create a human
 

55
00:01:40,200 --> 00:01:42,289
is once we know how to create a human
level intelligence system how will

56
00:01:42,289 --> 00:01:42,299
level intelligence system how will
 

57
00:01:42,299 --> 00:01:46,730
level intelligence system how will
society be impacted will robots take

58
00:01:46,730 --> 00:01:46,740
society be impacted will robots take
 

59
00:01:46,740 --> 00:01:51,620
society be impacted will robots take
over and kill everyone will we achieve a

60
00:01:51,620 --> 00:01:51,630
over and kill everyone will we achieve a
 

61
00:01:51,630 --> 00:01:54,740
over and kill everyone will we achieve a
utopia that will remove the need to do

62
00:01:54,740 --> 00:01:54,750
utopia that will remove the need to do
 

63
00:01:54,750 --> 00:01:56,660
utopia that will remove the need to do
any of the messy jobs that will make us

64
00:01:56,660 --> 00:01:56,670
any of the messy jobs that will make us
 

65
00:01:56,670 --> 00:01:59,899
any of the messy jobs that will make us
all extremely happy those kinds of

66
00:01:59,899 --> 00:01:59,909
all extremely happy those kinds of
 

67
00:01:59,909 --> 00:02:02,270
all extremely happy those kinds of
beautiful philosophical concepts are

68
00:02:02,270 --> 00:02:02,280
beautiful philosophical concepts are
 

69
00:02:02,280 --> 00:02:03,980
beautiful philosophical concepts are
interesting to explore but that's not

70
00:02:03,980 --> 00:02:03,990
interesting to explore but that's not
 

71
00:02:03,990 --> 00:02:06,740
interesting to explore but that's not
what we're interested in doing I believe

72
00:02:06,740 --> 00:02:06,750
what we're interested in doing I believe
 

73
00:02:06,750 --> 00:02:09,949
what we're interested in doing I believe
that from an engineering perspective we

74
00:02:09,949 --> 00:02:09,959
that from an engineering perspective we
 

75
00:02:09,959 --> 00:02:13,550
that from an engineering perspective we
want to focus on the black box of a GI

76
00:02:13,550 --> 00:02:13,560
want to focus on the black box of a GI
 

77
00:02:13,560 --> 00:02:16,670
want to focus on the black box of a GI
start to build insights and intuitions

78
00:02:16,670 --> 00:02:16,680
start to build insights and intuitions
 

79
00:02:16,680 --> 00:02:18,640
start to build insights and intuitions
about how we create systems that

80
00:02:18,640 --> 00:02:18,650
about how we create systems that
 

81
00:02:18,650 --> 00:02:21,110
about how we create systems that
approach human level intelligence

82
00:02:21,110 --> 00:02:21,120
approach human level intelligence
 

83
00:02:21,120 --> 00:02:25,520
approach human level intelligence
I believe we're very far away from

84
00:02:25,520 --> 00:02:25,530
I believe we're very far away from
 

85
00:02:25,530 --> 00:02:29,120
I believe we're very far away from
creating anything resembling human level

86
00:02:29,120 --> 00:02:29,130
creating anything resembling human level
 

87
00:02:29,130 --> 00:02:34,970
creating anything resembling human level
intelligence however the dimension of

88
00:02:34,970 --> 00:02:34,980
intelligence however the dimension of
 

89
00:02:34,980 --> 00:02:39,770
intelligence however the dimension of
the metric behind the word Farr may not

90
00:02:39,770 --> 00:02:39,780
the metric behind the word Farr may not
 

91
00:02:39,780 --> 00:02:47,210
the metric behind the word Farr may not
be time in time perhaps through a few

92
00:02:47,210 --> 00:02:47,220
be time in time perhaps through a few
 

93
00:02:47,220 --> 00:02:49,220
be time in time perhaps through a few
breakthroughs maybe even one

94
00:02:49,220 --> 00:02:49,230
breakthroughs maybe even one
 

95
00:02:49,230 --> 00:02:52,190
breakthroughs maybe even one
breakthrough everything can change but

96
00:02:52,190 --> 00:02:52,200
breakthrough everything can change but
 

97
00:02:52,200 --> 00:02:55,490
breakthrough everything can change but
as we stand now our current methods as

98
00:02:55,490 --> 00:02:55,500
as we stand now our current methods as
 

99
00:02:55,500 --> 00:02:57,170
as we stand now our current methods as
we will explore from the various ideas

100
00:02:57,170 --> 00:02:57,180
we will explore from the various ideas
 

101
00:02:57,180 --> 00:02:59,509
we will explore from the various ideas
and approaches and the guest speakers

102
00:02:59,509 --> 00:02:59,519
and approaches and the guest speakers
 

103
00:02:59,519 --> 00:03:02,330
and approaches and the guest speakers
coming here over the next two weeks and

104
00:03:02,330 --> 00:03:02,340
coming here over the next two weeks and
 

105
00:03:02,340 --> 00:03:06,229
coming here over the next two weeks and
beyond our best understanding our best

106
00:03:06,229 --> 00:03:06,239
beyond our best understanding our best
 

107
00:03:06,239 --> 00:03:10,490
beyond our best understanding our best
intuition and insights are not yet at

108
00:03:10,490 --> 00:03:10,500
intuition and insights are not yet at
 

109
00:03:10,500 --> 00:03:13,790
intuition and insights are not yet at
the level of reaching without a major

110
00:03:13,790 --> 00:03:13,800
the level of reaching without a major
 

111
00:03:13,800 --> 00:03:15,650
the level of reaching without a major
leap and breakthrough a paradigm shift

112
00:03:15,650 --> 00:03:15,660
leap and breakthrough a paradigm shift
 

113
00:03:15,660 --> 00:03:19,039
leap and breakthrough a paradigm shift
towards human level intelligence so it's

114
00:03:19,039 --> 00:03:19,049
towards human level intelligence so it's
 

115
00:03:19,049 --> 00:03:21,830
towards human level intelligence so it's
not constructive to consider the impact

116
00:03:21,830 --> 00:03:21,840
not constructive to consider the impact
 

117
00:03:21,840 --> 00:03:26,140
not constructive to consider the impact
of artificial intelligence to consider

118
00:03:26,140 --> 00:03:26,150
of artificial intelligence to consider
 

119
00:03:26,150 --> 00:03:29,300
of artificial intelligence to consider
questions of safety and ethics

120
00:03:29,300 --> 00:03:29,310
questions of safety and ethics
 

121
00:03:29,310 --> 00:03:32,300
questions of safety and ethics
fundamental extremely important

122
00:03:32,300 --> 00:03:32,310
fundamental extremely important
 

123
00:03:32,310 --> 00:03:35,479
fundamental extremely important
questions we it's not constructive to

124
00:03:35,479 --> 00:03:35,489
questions we it's not constructive to
 

125
00:03:35,489 --> 00:03:37,670
questions we it's not constructive to
consider those questions without also

126
00:03:37,670 --> 00:03:37,680
consider those questions without also
 

127
00:03:37,680 --> 00:03:40,699
consider those questions without also
deeply considering the black box of the

128
00:03:40,699 --> 00:03:40,709
deeply considering the black box of the
 

129
00:03:40,709 --> 00:03:43,160
deeply considering the black box of the
actual methods of artificial

130
00:03:43,160 --> 00:03:43,170
actual methods of artificial
 

131
00:03:43,170 --> 00:03:45,680
actual methods of artificial
intelligence human level artificial

132
00:03:45,680 --> 00:03:45,690
intelligence human level artificial
 

133
00:03:45,690 --> 00:03:48,949
intelligence human level artificial
intelligence and that's what I see what

134
00:03:48,949 --> 00:03:48,959
intelligence and that's what I see what
 

135
00:03:48,959 --> 00:03:51,560
intelligence and that's what I see what
I hope this course can be its first

136
00:03:51,560 --> 00:03:51,570
I hope this course can be its first
 

137
00:03:51,570 --> 00:03:55,940
I hope this course can be its first
iteration its first exploratory attempt

138
00:03:55,940 --> 00:03:55,950
iteration its first exploratory attempt
 

139
00:03:55,950 --> 00:03:59,449
iteration its first exploratory attempt
to try to look at different approaches

140
00:03:59,449 --> 00:03:59,459
to try to look at different approaches
 

141
00:03:59,459 --> 00:04:01,099
to try to look at different approaches
of how we can engineer intelligence

142
00:04:01,099 --> 00:04:01,109
of how we can engineer intelligence
 

143
00:04:01,109 --> 00:04:04,490
of how we can engineer intelligence
that's the role of MIT it's tradition of

144
00:04:04,490 --> 00:04:04,500
that's the role of MIT it's tradition of
 

145
00:04:04,500 --> 00:04:06,830
that's the role of MIT it's tradition of
mine in hand it's to consider the big

146
00:04:06,830 --> 00:04:06,840
mine in hand it's to consider the big
 

147
00:04:06,840 --> 00:04:09,530
mine in hand it's to consider the big
picture the future impact of society 10

148
00:04:09,530 --> 00:04:09,540
picture the future impact of society 10
 

149
00:04:09,540 --> 00:04:12,800
picture the future impact of society 10
20 30 40 years out but fundamentally

150
00:04:12,800 --> 00:04:12,810
20 30 40 years out but fundamentally
 

151
00:04:12,810 --> 00:04:16,099
20 30 40 years out but fundamentally
grounded in what kind of methods do we

152
00:04:16,099 --> 00:04:16,109
grounded in what kind of methods do we
 

153
00:04:16,109 --> 00:04:17,920
grounded in what kind of methods do we
have today and what are their

154
00:04:17,920 --> 00:04:17,930
have today and what are their
 

155
00:04:17,930 --> 00:04:20,539
have today and what are their
limitations and possibilities of

156
00:04:20,539 --> 00:04:20,549
limitations and possibilities of
 

157
00:04:20,549 --> 00:04:26,120
limitations and possibilities of
achieving that the black box of a GI

158
00:04:26,120 --> 00:04:26,130

 

159
00:04:26,130 --> 00:04:30,330

in the future impact on society of

160
00:04:30,330 --> 00:04:30,340
in the future impact on society of
 

161
00:04:30,340 --> 00:04:32,340
in the future impact on society of
creating artificial intelligence systems

162
00:04:32,340 --> 00:04:32,350
creating artificial intelligence systems
 

163
00:04:32,350 --> 00:04:35,070
creating artificial intelligence systems
that get become increasingly more

164
00:04:35,070 --> 00:04:35,080
that get become increasingly more
 

165
00:04:35,080 --> 00:04:37,850
that get become increasingly more
intelligent the fundamental disagreement

166
00:04:37,850 --> 00:04:37,860
intelligent the fundamental disagreement
 

167
00:04:37,860 --> 00:04:42,120
intelligent the fundamental disagreement
lies in the fact the the very core of

168
00:04:42,120 --> 00:04:42,130
lies in the fact the the very core of
 

169
00:04:42,130 --> 00:04:45,900
lies in the fact the the very core of
that black box which is how hard is it

170
00:04:45,900 --> 00:04:45,910
that black box which is how hard is it
 

171
00:04:45,910 --> 00:04:49,290
that black box which is how hard is it
to build an AGI system how hard is it to

172
00:04:49,290 --> 00:04:49,300
to build an AGI system how hard is it to
 

173
00:04:49,300 --> 00:04:51,090
to build an AGI system how hard is it to
create a human level artificial

174
00:04:51,090 --> 00:04:51,100
create a human level artificial
 

175
00:04:51,100 --> 00:04:53,129
create a human level artificial
intelligence system that's the open

176
00:04:53,129 --> 00:04:53,139
intelligence system that's the open
 

177
00:04:53,139 --> 00:04:56,900
intelligence system that's the open
question for all of us from from Josh

178
00:04:56,900 --> 00:04:56,910
question for all of us from from Josh
 

179
00:04:56,910 --> 00:05:01,320
question for all of us from from Josh
Tenenbaum to Andrey Carpathia to folks

180
00:05:01,320 --> 00:05:01,330
Tenenbaum to Andrey Carpathia to folks
 

181
00:05:01,330 --> 00:05:04,379
Tenenbaum to Andrey Carpathia to folks
from open AI to Boston Dynamics to the

182
00:05:04,379 --> 00:05:04,389
from open AI to Boston Dynamics to the
 

183
00:05:04,389 --> 00:05:07,830
from open AI to Boston Dynamics to the
brilliant leaders in various fields of

184
00:05:07,830 --> 00:05:07,840
brilliant leaders in various fields of
 

185
00:05:07,840 --> 00:05:09,120
brilliant leaders in various fields of
artificial intelligence that will come

186
00:05:09,120 --> 00:05:09,130
artificial intelligence that will come
 

187
00:05:09,130 --> 00:05:11,310
artificial intelligence that will come
here that's the open question how hard

188
00:05:11,310 --> 00:05:11,320
here that's the open question how hard
 

189
00:05:11,320 --> 00:05:13,770
here that's the open question how hard
is it there's been a lot of incredibly

190
00:05:13,770 --> 00:05:13,780
is it there's been a lot of incredibly
 

191
00:05:13,780 --> 00:05:16,879
is it there's been a lot of incredibly
impressive results in deep learning in

192
00:05:16,879 --> 00:05:16,889
impressive results in deep learning in
 

193
00:05:16,889 --> 00:05:19,140
impressive results in deep learning in
neuroscience and computational cognitive

194
00:05:19,140 --> 00:05:19,150
neuroscience and computational cognitive
 

195
00:05:19,150 --> 00:05:26,580
neuroscience and computational cognitive
science in robotics but how far are we

196
00:05:26,580 --> 00:05:26,590
science in robotics but how far are we
 

197
00:05:26,590 --> 00:05:28,620
science in robotics but how far are we
still to go to the AGI that's the

198
00:05:28,620 --> 00:05:28,630
still to go to the AGI that's the
 

199
00:05:28,630 --> 00:05:30,570
still to go to the AGI that's the
fundamental question that we need to

200
00:05:30,570 --> 00:05:30,580
fundamental question that we need to
 

201
00:05:30,580 --> 00:05:34,080
fundamental question that we need to
explore before we consider the questions

202
00:05:34,080 --> 00:05:34,090
explore before we consider the questions
 

203
00:05:34,090 --> 00:05:38,909
explore before we consider the questions
the future impact on society and the

204
00:05:38,909 --> 00:05:38,919
the future impact on society and the
 

205
00:05:38,919 --> 00:05:40,800
the future impact on society and the
goal for this class is to build

206
00:05:40,800 --> 00:05:40,810
goal for this class is to build
 

207
00:05:40,810 --> 00:05:43,740
goal for this class is to build
intuition one talk at a time a project

208
00:05:43,740 --> 00:05:43,750
intuition one talk at a time a project
 

209
00:05:43,750 --> 00:05:48,029
intuition one talk at a time a project
at a time build intuition about where we

210
00:05:48,029 --> 00:05:48,039
at a time build intuition about where we
 

211
00:05:48,039 --> 00:05:50,040
at a time build intuition about where we
stand about what the limitations of

212
00:05:50,040 --> 00:05:50,050
stand about what the limitations of
 

213
00:05:50,050 --> 00:05:51,990
stand about what the limitations of
current approaches are how can we close

214
00:05:51,990 --> 00:05:52,000
current approaches are how can we close
 

215
00:05:52,000 --> 00:05:58,950
current approaches are how can we close
the gap a nice meme that I caught on

216
00:05:58,950 --> 00:05:58,960
the gap a nice meme that I caught on
 

217
00:05:58,960 --> 00:06:02,189
the gap a nice meme that I caught on
Twitter recently of the difference

218
00:06:02,189 --> 00:06:02,199
Twitter recently of the difference
 

219
00:06:02,199 --> 00:06:04,950
Twitter recently of the difference
between the engineering approach at the

220
00:06:04,950 --> 00:06:04,960
between the engineering approach at the
 

221
00:06:04,960 --> 00:06:09,240
between the engineering approach at the
very simplest of a Google intern typing

222
00:06:09,240 --> 00:06:09,250
very simplest of a Google intern typing
 

223
00:06:09,250 --> 00:06:11,460
very simplest of a Google intern typing
a for loop that just does a grid search

224
00:06:11,460 --> 00:06:11,470
a for loop that just does a grid search
 

225
00:06:11,470 --> 00:06:14,550
a for loop that just does a grid search
on parameters for a neural network and

226
00:06:14,550 --> 00:06:14,560
on parameters for a neural network and
 

227
00:06:14,560 --> 00:06:16,650
on parameters for a neural network and
on the right is the way media would

228
00:06:16,650 --> 00:06:16,660
on the right is the way media would
 

229
00:06:16,660 --> 00:06:21,330
on the right is the way media would
report this for loop the Google AI

230
00:06:21,330 --> 00:06:21,340
report this for loop the Google AI
 

231
00:06:21,340 --> 00:06:27,540
report this for loop the Google AI
created its own baby AI I think it's

232
00:06:27,540 --> 00:06:27,550
created its own baby AI I think it's
 

233
00:06:27,550 --> 00:06:31,490
created its own baby AI I think it's
easy for us to go one way or the other

234
00:06:31,490 --> 00:06:31,500
easy for us to go one way or the other
 

235
00:06:31,500 --> 00:06:35,310
easy for us to go one way or the other
but we'd like to do both our first goal

236
00:06:35,310 --> 00:06:35,320
but we'd like to do both our first goal
 

237
00:06:35,320 --> 00:06:37,620
but we'd like to do both our first goal
is to avoid the pitfalls of black box

238
00:06:37,620 --> 00:06:37,630
is to avoid the pitfalls of black box
 

239
00:06:37,630 --> 00:06:39,190
is to avoid the pitfalls of black box
thinking of the few

240
00:06:39,190 --> 00:06:39,200
thinking of the few
 

241
00:06:39,200 --> 00:06:41,140
thinking of the few
tourism thinking that results in hype

242
00:06:41,140 --> 00:06:41,150
tourism thinking that results in hype
 

243
00:06:41,150 --> 00:06:43,750
tourism thinking that results in hype
that's detached from scientific

244
00:06:43,750 --> 00:06:43,760
that's detached from scientific
 

245
00:06:43,760 --> 00:06:45,760
that's detached from scientific
engineering understanding of what the

246
00:06:45,760 --> 00:06:45,770
engineering understanding of what the
 

247
00:06:45,770 --> 00:06:48,040
engineering understanding of what the
actual systems are doing that's what the

248
00:06:48,040 --> 00:06:48,050
actual systems are doing that's what the
 

249
00:06:48,050 --> 00:06:51,640
actual systems are doing that's what the
media often reports that's what some of

250
00:06:51,640 --> 00:06:51,650
media often reports that's what some of
 

251
00:06:51,650 --> 00:06:56,470
media often reports that's what some of
our speakers will explore in a rigorous

252
00:06:56,470 --> 00:06:56,480
our speakers will explore in a rigorous
 

253
00:06:56,480 --> 00:06:58,960
our speakers will explore in a rigorous
way it's still an important topic to

254
00:06:58,960 --> 00:06:58,970
way it's still an important topic to
 

255
00:06:58,970 --> 00:07:01,300
way it's still an important topic to
explore Ray Kurzweil's on Wednesday

256
00:07:01,300 --> 00:07:01,310
explore Ray Kurzweil's on Wednesday
 

257
00:07:01,310 --> 00:07:04,690
explore Ray Kurzweil's on Wednesday
we'll look we'll explore this topic next

258
00:07:04,690 --> 00:07:04,700
we'll look we'll explore this topic next
 

259
00:07:04,700 --> 00:07:06,940
we'll look we'll explore this topic next
week talking about AI safety and

260
00:07:06,940 --> 00:07:06,950
week talking about AI safety and
 

261
00:07:06,950 --> 00:07:08,740
week talking about AI safety and
autonomous weapon systems we'll explore

262
00:07:08,740 --> 00:07:08,750
autonomous weapon systems we'll explore
 

263
00:07:08,750 --> 00:07:11,590
autonomous weapon systems we'll explore
this topic the future impact 10 20 years

264
00:07:11,590 --> 00:07:11,600
this topic the future impact 10 20 years
 

265
00:07:11,600 --> 00:07:14,380
this topic the future impact 10 20 years
out how do we design systems today that

266
00:07:14,380 --> 00:07:14,390
out how do we design systems today that
 

267
00:07:14,390 --> 00:07:16,210
out how do we design systems today that
would lead to safe systems tomorrow

268
00:07:16,210 --> 00:07:16,220
would lead to safe systems tomorrow
 

269
00:07:16,220 --> 00:07:18,730
would lead to safe systems tomorrow
still very important but the reality is

270
00:07:18,730 --> 00:07:18,740
still very important but the reality is
 

271
00:07:18,740 --> 00:07:20,950
still very important but the reality is
a lot of us need to put a lot more

272
00:07:20,950 --> 00:07:20,960
a lot of us need to put a lot more
 

273
00:07:20,960 --> 00:07:23,800
a lot of us need to put a lot more
emphasis on the left on the four loops

274
00:07:23,800 --> 00:07:23,810
emphasis on the left on the four loops
 

275
00:07:23,810 --> 00:07:26,800
emphasis on the left on the four loops
on creating these systems at the same

276
00:07:26,800 --> 00:07:26,810
on creating these systems at the same
 

277
00:07:26,810 --> 00:07:29,230
on creating these systems at the same
time the second goal of what we're

278
00:07:29,230 --> 00:07:29,240
time the second goal of what we're
 

279
00:07:29,240 --> 00:07:32,380
time the second goal of what we're
trying to do here is not emphasize the

280
00:07:32,380 --> 00:07:32,390
trying to do here is not emphasize the
 

281
00:07:32,390 --> 00:07:35,770
trying to do here is not emphasize the
silliness the simplicity the naive basic

282
00:07:35,770 --> 00:07:35,780
silliness the simplicity the naive basic
 

283
00:07:35,780 --> 00:07:39,550
silliness the simplicity the naive basic
nature of this for loop in the same way

284
00:07:39,550 --> 00:07:39,560
nature of this for loop in the same way
 

285
00:07:39,560 --> 00:07:42,760
nature of this for loop in the same way
as was the process in creating nuclear

286
00:07:42,760 --> 00:07:42,770
as was the process in creating nuclear
 

287
00:07:42,770 --> 00:07:47,310
as was the process in creating nuclear
weapons before during World War two

288
00:07:47,310 --> 00:07:47,320
weapons before during World War two
 

289
00:07:47,320 --> 00:07:49,900
weapons before during World War two
the idea that as an engineer as a

290
00:07:49,900 --> 00:07:49,910
the idea that as an engineer as a
 

291
00:07:49,910 --> 00:07:52,270
the idea that as an engineer as a
scientist that I'm just the scientist is

292
00:07:52,270 --> 00:07:52,280
scientist that I'm just the scientist is
 

293
00:07:52,280 --> 00:07:56,500
scientist that I'm just the scientist is
also a flawed way of thinking we have to

294
00:07:56,500 --> 00:07:56,510
also a flawed way of thinking we have to
 

295
00:07:56,510 --> 00:07:58,840
also a flawed way of thinking we have to
consider the big picture impact the

296
00:07:58,840 --> 00:07:58,850
consider the big picture impact the
 

297
00:07:58,850 --> 00:08:01,600
consider the big picture impact the
near-term negative consequences that are

298
00:08:01,600 --> 00:08:01,610
near-term negative consequences that are
 

299
00:08:01,610 --> 00:08:03,970
near-term negative consequences that are
preventable the low-hanging fruit that

300
00:08:03,970 --> 00:08:03,980
preventable the low-hanging fruit that
 

301
00:08:03,980 --> 00:08:05,740
preventable the low-hanging fruit that
can be prevented through that very

302
00:08:05,740 --> 00:08:05,750
can be prevented through that very
 

303
00:08:05,750 --> 00:08:08,910
can be prevented through that very
engineering process we have to do both

304
00:08:08,910 --> 00:08:08,920
engineering process we have to do both
 

305
00:08:08,920 --> 00:08:15,010
engineering process we have to do both
and in this engineering approach we

306
00:08:15,010 --> 00:08:15,020
and in this engineering approach we
 

307
00:08:15,020 --> 00:08:18,490
and in this engineering approach we
always have to be cautious that just

308
00:08:18,490 --> 00:08:18,500
always have to be cautious that just
 

309
00:08:18,500 --> 00:08:21,190
always have to be cautious that just
because we don't understand

310
00:08:21,190 --> 00:08:21,200
because we don't understand
 

311
00:08:21,200 --> 00:08:25,270
because we don't understand
we're just because we our intuition our

312
00:08:25,270 --> 00:08:25,280
we're just because we our intuition our
 

313
00:08:25,280 --> 00:08:27,970
we're just because we our intuition our
best understanding of the capabilities

314
00:08:27,970 --> 00:08:27,980
best understanding of the capabilities
 

315
00:08:27,980 --> 00:08:30,310
best understanding of the capabilities
of modern systems that learn that act in

316
00:08:30,310 --> 00:08:30,320
of modern systems that learn that act in
 

317
00:08:30,320 --> 00:08:32,980
of modern systems that learn that act in
this world seem limited seem far from

318
00:08:32,980 --> 00:08:32,990
this world seem limited seem far from
 

319
00:08:32,990 --> 00:08:35,230
this world seem limited seem far from
human level intelligence our ability to

320
00:08:35,230 --> 00:08:35,240
human level intelligence our ability to
 

321
00:08:35,240 --> 00:08:36,880
human level intelligence our ability to
learn and represent common sense

322
00:08:36,880 --> 00:08:36,890
learn and represent common sense
 

323
00:08:36,890 --> 00:08:41,110
learn and represent common sense
reasoning seems limited the exponential

324
00:08:41,110 --> 00:08:41,120
reasoning seems limited the exponential
 

325
00:08:41,120 --> 00:08:43,300
reasoning seems limited the exponential
potentially Exponential's could be

326
00:08:43,300 --> 00:08:43,310
potentially Exponential's could be
 

327
00:08:43,310 --> 00:08:47,500
potentially Exponential's could be
argued and he will growth of technology

328
00:08:47,500 --> 00:08:47,510
argued and he will growth of technology
 

329
00:08:47,510 --> 00:08:50,260
argued and he will growth of technology
of these ideas means that just around

330
00:08:50,260 --> 00:08:50,270
of these ideas means that just around
 

331
00:08:50,270 --> 00:08:52,380
of these ideas means that just around
the corner is a singularity

332
00:08:52,380 --> 00:08:52,390
the corner is a singularity
 

333
00:08:52,390 --> 00:08:55,320
the corner is a singularity
is a breakthrough idea that will change

334
00:08:55,320 --> 00:08:55,330
is a breakthrough idea that will change
 

335
00:08:55,330 --> 00:08:58,500
is a breakthrough idea that will change
everything we have to be cautious of

336
00:08:58,500 --> 00:08:58,510
everything we have to be cautious of
 

337
00:08:58,510 --> 00:09:02,810
everything we have to be cautious of
that moreover we have to be cautious of

338
00:09:02,810 --> 00:09:02,820
that moreover we have to be cautious of
 

339
00:09:02,820 --> 00:09:06,570
that moreover we have to be cautious of
the fact that every decade over the past

340
00:09:06,570 --> 00:09:06,580
the fact that every decade over the past
 

341
00:09:06,580 --> 00:09:09,510
the fact that every decade over the past
century our adoption of new technologies

342
00:09:09,510 --> 00:09:09,520
century our adoption of new technologies
 

343
00:09:09,520 --> 00:09:12,000
century our adoption of new technologies
has gotten faster and faster the the

344
00:09:12,000 --> 00:09:12,010
has gotten faster and faster the the
 

345
00:09:12,010 --> 00:09:15,720
has gotten faster and faster the the
rate at which a new technology from its

346
00:09:15,720 --> 00:09:15,730
rate at which a new technology from its
 

347
00:09:15,730 --> 00:09:19,290
rate at which a new technology from its
birth to its wide mass adoption has

348
00:09:19,290 --> 00:09:19,300
birth to its wide mass adoption has
 

349
00:09:19,300 --> 00:09:21,200
birth to its wide mass adoption has
shortened and shortened and shortened

350
00:09:21,200 --> 00:09:21,210
shortened and shortened and shortened
 

351
00:09:21,210 --> 00:09:26,790
shortened and shortened and shortened
that means that new idea the moment it

352
00:09:26,790 --> 00:09:26,800
that means that new idea the moment it
 

353
00:09:26,800 --> 00:09:30,990
that means that new idea the moment it
drops into the world can have widespread

354
00:09:30,990 --> 00:09:31,000
drops into the world can have widespread
 

355
00:09:31,000 --> 00:09:35,190
drops into the world can have widespread
effects overnight so as and I think the

356
00:09:35,190 --> 00:09:35,200
effects overnight so as and I think the
 

357
00:09:35,200 --> 00:09:36,900
effects overnight so as and I think the
in the engineering approach is

358
00:09:36,900 --> 00:09:36,910
in the engineering approach is
 

359
00:09:36,910 --> 00:09:39,030
in the engineering approach is
fundamentally cynical on artificial

360
00:09:39,030 --> 00:09:39,040
fundamentally cynical on artificial
 

361
00:09:39,040 --> 00:09:40,860
fundamentally cynical on artificial
general intelligence because every

362
00:09:40,860 --> 00:09:40,870
general intelligence because every
 

363
00:09:40,870 --> 00:09:43,350
general intelligence because every
aspect of its is so difficult we have to

364
00:09:43,350 --> 00:09:43,360
aspect of its is so difficult we have to
 

365
00:09:43,360 --> 00:09:44,400
aspect of its is so difficult we have to
always remember that overnight

366
00:09:44,400 --> 00:09:44,410
always remember that overnight
 

367
00:09:44,410 --> 00:09:48,330
always remember that overnight
everything can change through this

368
00:09:48,330 --> 00:09:48,340
everything can change through this
 

369
00:09:48,340 --> 00:09:51,810
everything can change through this
question of beginning to approach from a

370
00:09:51,810 --> 00:09:51,820
question of beginning to approach from a
 

371
00:09:51,820 --> 00:09:53,280
question of beginning to approach from a
deep learning perspective deep

372
00:09:53,280 --> 00:09:53,290
deep learning perspective deep
 

373
00:09:53,290 --> 00:09:54,690
deep learning perspective deep
reinforcement learning from brain

374
00:09:54,690 --> 00:09:54,700
reinforcement learning from brain
 

375
00:09:54,700 --> 00:09:56,670
reinforcement learning from brain
simulation computational cognitive

376
00:09:56,670 --> 00:09:56,680
simulation computational cognitive
 

377
00:09:56,680 --> 00:10:00,570
simulation computational cognitive
science from computational neuroscience

378
00:10:00,570 --> 00:10:00,580
science from computational neuroscience
 

379
00:10:00,580 --> 00:10:02,040
science from computational neuroscience
from cognitive architectures from

380
00:10:02,040 --> 00:10:02,050
from cognitive architectures from
 

381
00:10:02,050 --> 00:10:07,590
from cognitive architectures from
robotics from legal perspectives and

382
00:10:07,590 --> 00:10:07,600
robotics from legal perspectives and
 

383
00:10:07,600 --> 00:10:10,710
robotics from legal perspectives and
autonomous weapon systems as we begin to

384
00:10:10,710 --> 00:10:10,720
autonomous weapon systems as we begin to
 

385
00:10:10,720 --> 00:10:12,900
autonomous weapon systems as we begin to
approach these questions we need to

386
00:10:12,900 --> 00:10:12,910
approach these questions we need to
 

387
00:10:12,910 --> 00:10:15,660
approach these questions we need to
start to build intuition how far away

388
00:10:15,660 --> 00:10:15,670
start to build intuition how far away
 

389
00:10:15,670 --> 00:10:17,940
start to build intuition how far away
are we from creating intelligent systems

390
00:10:17,940 --> 00:10:17,950
are we from creating intelligent systems
 

391
00:10:17,950 --> 00:10:22,160
are we from creating intelligent systems
the singularity here is that spark that

392
00:10:22,160 --> 00:10:22,170
the singularity here is that spark that
 

393
00:10:22,170 --> 00:10:26,040
the singularity here is that spark that
moment when we're truly surprised by the

394
00:10:26,040 --> 00:10:26,050
moment when we're truly surprised by the
 

395
00:10:26,050 --> 00:10:29,720
moment when we're truly surprised by the
intelligence of the systems we create

396
00:10:29,720 --> 00:10:29,730
intelligence of the systems we create
 

397
00:10:29,730 --> 00:10:34,650
intelligence of the systems we create
I'd like to visualize it by the by a

398
00:10:34,650 --> 00:10:34,660
I'd like to visualize it by the by a
 

399
00:10:34,660 --> 00:10:36,870
I'd like to visualize it by the by a
certain analogy that we're in this dark

400
00:10:36,870 --> 00:10:36,880
certain analogy that we're in this dark
 

401
00:10:36,880 --> 00:10:40,530
certain analogy that we're in this dark
room looking for a light switch with no

402
00:10:40,530 --> 00:10:40,540
room looking for a light switch with no
 

403
00:10:40,540 --> 00:10:42,840
room looking for a light switch with no
knowledge of where the light switch is

404
00:10:42,840 --> 00:10:42,850
knowledge of where the light switch is
 

405
00:10:42,850 --> 00:10:45,210
knowledge of where the light switch is
there's going to be people that say well

406
00:10:45,210 --> 00:10:45,220
there's going to be people that say well
 

407
00:10:45,220 --> 00:10:47,610
there's going to be people that say well
it's a smaller the rooms are all smaller

408
00:10:47,610 --> 00:10:47,620
it's a smaller the rooms are all smaller
 

409
00:10:47,620 --> 00:10:49,530
it's a smaller the rooms are all smaller
right there and say anywhere we'll be

410
00:10:49,530 --> 00:10:49,540
right there and say anywhere we'll be
 

411
00:10:49,540 --> 00:10:51,660
right there and say anywhere we'll be
able to find it in any time the reality

412
00:10:51,660 --> 00:10:51,670
able to find it in any time the reality
 

413
00:10:51,670 --> 00:10:53,820
able to find it in any time the reality
is we know very little so we have to

414
00:10:53,820 --> 00:10:53,830
is we know very little so we have to
 

415
00:10:53,830 --> 00:10:55,740
is we know very little so we have to
stumble around feel our way around to

416
00:10:55,740 --> 00:10:55,750
stumble around feel our way around to
 

417
00:10:55,750 --> 00:10:59,460
stumble around feel our way around to
build the intuition a far far away we

418
00:10:59,460 --> 00:10:59,470
build the intuition a far far away we
 

419
00:10:59,470 --> 00:11:04,009
build the intuition a far far away we
really are

420
00:11:04,009 --> 00:11:04,019

 

421
00:11:04,019 --> 00:11:07,069

many will speakers here will talk about

422
00:11:07,069 --> 00:11:07,079
many will speakers here will talk about
 

423
00:11:07,079 --> 00:11:09,829
many will speakers here will talk about
how we define intelligence how we can

424
00:11:09,829 --> 00:11:09,839
how we define intelligence how we can
 

425
00:11:09,839 --> 00:11:12,169
how we define intelligence how we can
begin to see intelligence what are the

426
00:11:12,169 --> 00:11:12,179
begin to see intelligence what are the
 

427
00:11:12,179 --> 00:11:14,059
begin to see intelligence what are the
fundamental impacts of creating

428
00:11:14,059 --> 00:11:14,069
fundamental impacts of creating
 

429
00:11:14,069 --> 00:11:16,999
fundamental impacts of creating
intelligence systems I'd like to sort of

430
00:11:16,999 --> 00:11:17,009
intelligence systems I'd like to sort of
 

431
00:11:17,009 --> 00:11:22,519
intelligence systems I'd like to sort of
see the positive reason for this little

432
00:11:22,519 --> 00:11:22,529
see the positive reason for this little
 

433
00:11:22,529 --> 00:11:25,129
see the positive reason for this little
class and for these efforts that have

434
00:11:25,129 --> 00:11:25,139
class and for these efforts that have
 

435
00:11:25,139 --> 00:11:28,039
class and for these efforts that have
fascinated people throughout the century

436
00:11:28,039 --> 00:11:28,049
fascinated people throughout the century
 

437
00:11:28,049 --> 00:11:29,960
fascinated people throughout the century
of trying to create intelligent systems

438
00:11:29,960 --> 00:11:29,970
of trying to create intelligent systems
 

439
00:11:29,970 --> 00:11:32,689
of trying to create intelligent systems
is that there's something about human

440
00:11:32,689 --> 00:11:32,699
is that there's something about human
 

441
00:11:32,699 --> 00:11:39,289
is that there's something about human
beings that one that craves to explore

442
00:11:39,289 --> 00:11:39,299
beings that one that craves to explore
 

443
00:11:39,299 --> 00:11:42,429
beings that one that craves to explore
to uncover the mysteries of the universe

444
00:11:42,429 --> 00:11:42,439
to uncover the mysteries of the universe
 

445
00:11:42,439 --> 00:11:45,079
to uncover the mysteries of the universe
fundamental in itself a desire to

446
00:11:45,079 --> 00:11:45,089
fundamental in itself a desire to
 

447
00:11:45,089 --> 00:11:46,479
fundamental in itself a desire to
uncover the mysteries of the universe

448
00:11:46,479 --> 00:11:46,489
uncover the mysteries of the universe
 

449
00:11:46,489 --> 00:11:50,269
uncover the mysteries of the universe
not for a purpose and there's often an

450
00:11:50,269 --> 00:11:50,279
not for a purpose and there's often an
 

451
00:11:50,279 --> 00:11:53,799
not for a purpose and there's often an
underlying purpose of money of greed of

452
00:11:53,799 --> 00:11:53,809
underlying purpose of money of greed of
 

453
00:11:53,809 --> 00:11:57,259
underlying purpose of money of greed of
the power craving for power and so on

454
00:11:57,259 --> 00:11:57,269
the power craving for power and so on
 

455
00:11:57,269 --> 00:11:59,419
the power craving for power and so on
but there's seems to be an underlying

456
00:11:59,419 --> 00:11:59,429
but there's seems to be an underlying
 

457
00:11:59,429 --> 00:12:03,189
but there's seems to be an underlying
desire to explore nice little book an

458
00:12:03,189 --> 00:12:03,199
desire to explore nice little book an
 

459
00:12:03,199 --> 00:12:05,449
desire to explore nice little book an
exploration a very short introduction by

460
00:12:05,449 --> 00:12:05,459
exploration a very short introduction by
 

461
00:12:05,459 --> 00:12:07,549
exploration a very short introduction by
Stewart Weaver he says for all the

462
00:12:07,549 --> 00:12:07,559
Stewart Weaver he says for all the
 

463
00:12:07,559 --> 00:12:10,220
Stewart Weaver he says for all the
different forms it takes in different

464
00:12:10,220 --> 00:12:10,230
different forms it takes in different
 

465
00:12:10,230 --> 00:12:13,579
different forms it takes in different
historical periods for all the worthy

466
00:12:13,579 --> 00:12:13,589
historical periods for all the worthy
 

467
00:12:13,589 --> 00:12:17,169
historical periods for all the worthy
and unworthy motives that lie behind it

468
00:12:17,169 --> 00:12:17,179
and unworthy motives that lie behind it
 

469
00:12:17,179 --> 00:12:20,900
and unworthy motives that lie behind it
exploration travel for the sake of

470
00:12:20,900 --> 00:12:20,910
exploration travel for the sake of
 

471
00:12:20,910 --> 00:12:23,030
exploration travel for the sake of
discovery and adventure is a human

472
00:12:23,030 --> 00:12:23,040
discovery and adventure is a human
 

473
00:12:23,040 --> 00:12:27,259
discovery and adventure is a human
compulsion a human obsession even it is

474
00:12:27,259 --> 00:12:27,269
compulsion a human obsession even it is
 

475
00:12:27,269 --> 00:12:29,720
compulsion a human obsession even it is
defining element of a distinctly human

476
00:12:29,720 --> 00:12:29,730
defining element of a distinctly human
 

477
00:12:29,730 --> 00:12:32,660
defining element of a distinctly human
identity and it will never rest at any

478
00:12:32,660 --> 00:12:32,670
identity and it will never rest at any
 

479
00:12:32,670 --> 00:12:35,239
identity and it will never rest at any
frontier whether terrestrial or

480
00:12:35,239 --> 00:12:35,249
frontier whether terrestrial or
 

481
00:12:35,249 --> 00:12:41,689
frontier whether terrestrial or
extraterrestrial from 325 BCE with a

482
00:12:41,689 --> 00:12:41,699
extraterrestrial from 325 BCE with a
 

483
00:12:41,699 --> 00:12:48,139
extraterrestrial from 325 BCE with a
long 7500 mile journey on the ocean to

484
00:12:48,139 --> 00:12:48,149
long 7500 mile journey on the ocean to
 

485
00:12:48,149 --> 00:12:50,749
long 7500 mile journey on the ocean to
explore the Arctic to Christopher

486
00:12:50,749 --> 00:12:50,759
explore the Arctic to Christopher
 

487
00:12:50,759 --> 00:12:55,340
explore the Arctic to Christopher
Columbus and his flawed harshly

488
00:12:55,340 --> 00:12:55,350
Columbus and his flawed harshly
 

489
00:12:55,350 --> 00:12:58,009
Columbus and his flawed harshly
criticized the modern scholarship trip

490
00:12:58,009 --> 00:12:58,019
criticized the modern scholarship trip
 

491
00:12:58,019 --> 00:13:00,650
criticized the modern scholarship trip
that ultimately paved the way didn't

492
00:13:00,650 --> 00:13:00,660
that ultimately paved the way didn't
 

493
00:13:00,660 --> 00:13:04,340
that ultimately paved the way didn't
discover pave the way to colonization of

494
00:13:04,340 --> 00:13:04,350
discover pave the way to colonization of
 

495
00:13:04,350 --> 00:13:09,490
discover pave the way to colonization of
the Americas to the DAR

496
00:13:09,490 --> 00:13:09,500
the Americas to the DAR
 

497
00:13:09,500 --> 00:13:13,990
the Americas to the DAR
trip the voyage of the Beagle whilst

498
00:13:13,990 --> 00:13:14,000
trip the voyage of the Beagle whilst
 

499
00:13:14,000 --> 00:13:15,820
trip the voyage of the Beagle whilst
this planet has gone cycling on

500
00:13:15,820 --> 00:13:15,830
this planet has gone cycling on
 

501
00:13:15,830 --> 00:13:17,710
this planet has gone cycling on
according to the fixed law of gravity

502
00:13:17,710 --> 00:13:17,720
according to the fixed law of gravity
 

503
00:13:17,720 --> 00:13:20,590
according to the fixed law of gravity
from so simple a beginning endless forms

504
00:13:20,590 --> 00:13:20,600
from so simple a beginning endless forms
 

505
00:13:20,600 --> 00:13:23,230
from so simple a beginning endless forms
most beautiful and most wonderful have

506
00:13:23,230 --> 00:13:23,240
most beautiful and most wonderful have
 

507
00:13:23,240 --> 00:13:29,550
most beautiful and most wonderful have
been and are being evolved to the first

508
00:13:29,550 --> 00:13:29,560

 

509
00:13:29,560 --> 00:13:36,190

venture into space by Yuri Gagarin first

510
00:13:36,190 --> 00:13:36,200
venture into space by Yuri Gagarin first
 

511
00:13:36,200 --> 00:13:41,980
venture into space by Yuri Gagarin first
human in space in 1961 what he said over

512
00:13:41,980 --> 00:13:41,990
human in space in 1961 what he said over
 

513
00:13:41,990 --> 00:13:44,290
human in space in 1961 what he said over
the radio is the earth is blue it is

514
00:13:44,290 --> 00:13:44,300
the radio is the earth is blue it is
 

515
00:13:44,300 --> 00:13:47,890
the radio is the earth is blue it is
amazing this these are the words they

516
00:13:47,890 --> 00:13:47,900
amazing this these are the words they
 

517
00:13:47,900 --> 00:13:51,820
amazing this these are the words they
think drive our exploration in the

518
00:13:51,820 --> 00:13:51,830
think drive our exploration in the
 

519
00:13:51,830 --> 00:13:55,500
think drive our exploration in the
sciences in engineering and today an AI

520
00:13:55,500 --> 00:13:55,510
sciences in engineering and today an AI
 

521
00:13:55,510 --> 00:14:00,730
sciences in engineering and today an AI
and the first walk on the moon and now

522
00:14:00,730 --> 00:14:00,740
and the first walk on the moon and now
 

523
00:14:00,740 --> 00:14:10,470
and the first walk on the moon and now
the desire to colonize Mars and beyond

524
00:14:10,470 --> 00:14:10,480

 

525
00:14:10,480 --> 00:14:13,780

that's where I see this desire to create

526
00:14:13,780 --> 00:14:13,790
that's where I see this desire to create
 

527
00:14:13,790 --> 00:14:17,260
that's where I see this desire to create
intelligent systems talking about the

528
00:14:17,260 --> 00:14:17,270
intelligent systems talking about the
 

529
00:14:17,270 --> 00:14:19,420
intelligent systems talking about the
positive or negative impact of AI on

530
00:14:19,420 --> 00:14:19,430
positive or negative impact of AI on
 

531
00:14:19,430 --> 00:14:21,790
positive or negative impact of AI on
society talking about the business case

532
00:14:21,790 --> 00:14:21,800
society talking about the business case
 

533
00:14:21,800 --> 00:14:25,830
society talking about the business case
of the jobs lost jobs gain jobs created

534
00:14:25,830 --> 00:14:25,840
of the jobs lost jobs gain jobs created
 

535
00:14:25,840 --> 00:14:30,280
of the jobs lost jobs gain jobs created
diseases cured the autonomous vehicles

536
00:14:30,280 --> 00:14:30,290
diseases cured the autonomous vehicles
 

537
00:14:30,290 --> 00:14:32,470
diseases cured the autonomous vehicles
the ethical questions the safety of

538
00:14:32,470 --> 00:14:32,480
the ethical questions the safety of
 

539
00:14:32,480 --> 00:14:35,230
the ethical questions the safety of
autonomous weapons of the misuse of AI

540
00:14:35,230 --> 00:14:35,240
autonomous weapons of the misuse of AI
 

541
00:14:35,240 --> 00:14:38,350
autonomous weapons of the misuse of AI
in the financial markets underneath it

542
00:14:38,350 --> 00:14:38,360
in the financial markets underneath it
 

543
00:14:38,360 --> 00:14:40,240
in the financial markets underneath it
all and they're people many people are

544
00:14:40,240 --> 00:14:40,250
all and they're people many people are
 

545
00:14:40,250 --> 00:14:43,390
all and they're people many people are
spoken about this what drives myself and

546
00:14:43,390 --> 00:14:43,400
spoken about this what drives myself and
 

547
00:14:43,400 --> 00:14:45,490
spoken about this what drives myself and
many in the community is the desire to

548
00:14:45,490 --> 00:14:45,500
many in the community is the desire to
 

549
00:14:45,500 --> 00:14:47,710
many in the community is the desire to
explore to uncover the mystery of the

550
00:14:47,710 --> 00:14:47,720
explore to uncover the mystery of the
 

551
00:14:47,720 --> 00:14:49,900
explore to uncover the mystery of the
universe and I hope that you join me in

552
00:14:49,900 --> 00:14:49,910
universe and I hope that you join me in
 

553
00:14:49,910 --> 00:14:51,580
universe and I hope that you join me in
that very effort with the speakers that

554
00:14:51,580 --> 00:14:51,590
that very effort with the speakers that
 

555
00:14:51,590 --> 00:14:53,520
that very effort with the speakers that
come here in the next two weeks and

556
00:14:53,520 --> 00:14:53,530
come here in the next two weeks and
 

557
00:14:53,530 --> 00:14:59,650
come here in the next two weeks and
beyond the website for the course is a

558
00:14:59,650 --> 00:14:59,660
beyond the website for the course is a
 

559
00:14:59,660 --> 00:15:04,510
beyond the website for the course is a
GI that MIT died edu I am a part of an

560
00:15:04,510 --> 00:15:04,520
GI that MIT died edu I am a part of an
 

561
00:15:04,520 --> 00:15:10,120
GI that MIT died edu I am a part of an
amazing team many of whom you know AGI

562
00:15:10,120 --> 00:15:10,130
amazing team many of whom you know AGI
 

563
00:15:10,130 --> 00:15:13,750
amazing team many of whom you know AGI
at MIT that edu is the email where on

564
00:15:13,750 --> 00:15:13,760
at MIT that edu is the email where on
 

565
00:15:13,760 --> 00:15:19,450
at MIT that edu is the email where on
slack deep - MIT does slack for

566
00:15:19,450 --> 00:15:19,460
slack deep - MIT does slack for
 

567
00:15:19,460 --> 00:15:21,689
slack deep - MIT does slack for
registered MIT students

568
00:15:21,689 --> 00:15:21,699
registered MIT students
 

569
00:15:21,699 --> 00:15:24,049
registered MIT students
you create account on the website and

570
00:15:24,049 --> 00:15:24,059
you create account on the website and
 

571
00:15:24,059 --> 00:15:28,229
you create account on the website and
submit five new links and vote on ten to

572
00:15:28,229 --> 00:15:28,239
submit five new links and vote on ten to
 

573
00:15:28,239 --> 00:15:30,659
submit five new links and vote on ten to
vote AI which is an aggregator of

574
00:15:30,659 --> 00:15:30,669
vote AI which is an aggregator of
 

575
00:15:30,669 --> 00:15:32,460
vote AI which is an aggregator of
information a material we've put

576
00:15:32,460 --> 00:15:32,470
information a material we've put
 

577
00:15:32,470 --> 00:15:36,659
information a material we've put
together for the topic of AGI and submit

578
00:15:36,659 --> 00:15:36,669
together for the topic of AGI and submit
 

579
00:15:36,669 --> 00:15:40,169
together for the topic of AGI and submit
a entry to one of the competitions one

580
00:15:40,169 --> 00:15:40,179
a entry to one of the competitions one
 

581
00:15:40,179 --> 00:15:42,090
a entry to one of the competitions one
of the three competitions projects that

582
00:15:42,090 --> 00:15:42,100
of the three competitions projects that
 

583
00:15:42,100 --> 00:15:46,259
of the three competitions projects that
we have in this course and the projects

584
00:15:46,259 --> 00:15:46,269
we have in this course and the projects
 

585
00:15:46,269 --> 00:15:48,090
we have in this course and the projects
are dream vision I'll go over them in a

586
00:15:48,090 --> 00:15:48,100
are dream vision I'll go over them in a
 

587
00:15:48,100 --> 00:15:51,569
are dream vision I'll go over them in a
little bit dream vision angel ethical

588
00:15:51,569 --> 00:15:51,579
little bit dream vision angel ethical
 

589
00:15:51,579 --> 00:15:54,749
little bit dream vision angel ethical
car and the aggregator of material vote

590
00:15:54,749 --> 00:15:54,759
car and the aggregator of material vote
 

591
00:15:54,759 --> 00:15:57,359
car and the aggregator of material vote
AI we have guest speakers incredible

592
00:15:57,359 --> 00:15:57,369
AI we have guest speakers incredible
 

593
00:15:57,369 --> 00:15:59,189
AI we have guest speakers incredible
guest speakers I will go over them today

594
00:15:59,189 --> 00:15:59,199
guest speakers I will go over them today
 

595
00:15:59,199 --> 00:16:04,650
guest speakers I will go over them today
and as before with a deep learning for

596
00:16:04,650 --> 00:16:04,660
and as before with a deep learning for
 

597
00:16:04,660 --> 00:16:06,869
and as before with a deep learning for
self-driving cars course we have shirts

598
00:16:06,869 --> 00:16:06,879
self-driving cars course we have shirts
 

599
00:16:06,879 --> 00:16:10,379
self-driving cars course we have shirts
and they're free for in-person for

600
00:16:10,379 --> 00:16:10,389
and they're free for in-person for
 

601
00:16:10,389 --> 00:16:11,849
and they're free for in-person for
people that attend in person for the

602
00:16:11,849 --> 00:16:11,859
people that attend in person for the
 

603
00:16:11,859 --> 00:16:14,639
people that attend in person for the
last lecture most likely or you can

604
00:16:14,639 --> 00:16:14,649
last lecture most likely or you can
 

605
00:16:14,649 --> 00:16:17,090
last lecture most likely or you can
order them online

606
00:16:17,090 --> 00:16:17,100
order them online
 

607
00:16:17,100 --> 00:16:20,340
order them online
okay dream vision we take the Google G

608
00:16:20,340 --> 00:16:20,350
okay dream vision we take the Google G
 

609
00:16:20,350 --> 00:16:25,169
okay dream vision we take the Google G
dream idea we explore the idea of

610
00:16:25,169 --> 00:16:25,179
dream idea we explore the idea of
 

611
00:16:25,179 --> 00:16:27,779
dream idea we explore the idea of
creativity where it is I ins view of

612
00:16:27,779 --> 00:16:27,789
creativity where it is I ins view of
 

613
00:16:27,789 --> 00:16:29,999
creativity where it is I ins view of
intelligence the mark of intelligence is

614
00:16:29,999 --> 00:16:30,009
intelligence the mark of intelligence is
 

615
00:16:30,009 --> 00:16:34,199
intelligence the mark of intelligence is
creativity this this idea is something

616
00:16:34,199 --> 00:16:34,209
creativity this this idea is something
 

617
00:16:34,209 --> 00:16:36,809
creativity this this idea is something
we explore by using neural networks and

618
00:16:36,809 --> 00:16:36,819
we explore by using neural networks and
 

619
00:16:36,819 --> 00:16:41,429
we explore by using neural networks and
interesting ways to visualize what the

620
00:16:41,429 --> 00:16:41,439
interesting ways to visualize what the
 

621
00:16:41,439 --> 00:16:43,439
interesting ways to visualize what the
networks see and in so doing create

622
00:16:43,439 --> 00:16:43,449
networks see and in so doing create
 

623
00:16:43,449 --> 00:16:46,919
networks see and in so doing create
beautiful visualizations in time through

624
00:16:46,919 --> 00:16:46,929
beautiful visualizations in time through
 

625
00:16:46,929 --> 00:16:49,769
beautiful visualizations in time through
video so taking the ideas of deep dream

626
00:16:49,769 --> 00:16:49,779
video so taking the ideas of deep dream
 

627
00:16:49,779 --> 00:16:52,679
video so taking the ideas of deep dream
and combining them together with

628
00:16:52,679 --> 00:16:52,689
and combining them together with
 

629
00:16:52,689 --> 00:16:56,309
and combining them together with
multiple video streams to mix dream and

630
00:16:56,309 --> 00:16:56,319
multiple video streams to mix dream and
 

631
00:16:56,319 --> 00:17:01,049
multiple video streams to mix dream and
reality and the competition is through

632
00:17:01,049 --> 00:17:01,059
reality and the competition is through
 

633
00:17:01,059 --> 00:17:04,860
reality and the competition is through
Mechanical Turk we set up a competition

634
00:17:04,860 --> 00:17:04,870
Mechanical Turk we set up a competition
 

635
00:17:04,870 --> 00:17:07,319
Mechanical Turk we set up a competition
of who produces the most beautiful

636
00:17:07,319 --> 00:17:07,329
of who produces the most beautiful
 

637
00:17:07,329 --> 00:17:10,980
of who produces the most beautiful
visualization will provide code to

638
00:17:10,980 --> 00:17:10,990
visualization will provide code to
 

639
00:17:10,990 --> 00:17:13,590
visualization will provide code to
generate this visualization and ideas of

640
00:17:13,590 --> 00:17:13,600
generate this visualization and ideas of
 

641
00:17:13,600 --> 00:17:15,299
generate this visualization and ideas of
how you can make it more and more

642
00:17:15,299 --> 00:17:15,309
how you can make it more and more
 

643
00:17:15,309 --> 00:17:20,039
how you can make it more and more
beautiful and how to submit it to the

644
00:17:20,039 --> 00:17:20,049
beautiful and how to submit it to the
 

645
00:17:20,049 --> 00:17:24,389
beautiful and how to submit it to the
competition angel the artificial neural

646
00:17:24,389 --> 00:17:24,399
competition angel the artificial neural
 

647
00:17:24,399 --> 00:17:28,610
competition angel the artificial neural
generator of emotion and language is a

648
00:17:28,610 --> 00:17:28,620
generator of emotion and language is a
 

649
00:17:28,620 --> 00:17:32,460
generator of emotion and language is a
different twist on the Turing test where

650
00:17:32,460 --> 00:17:32,470
different twist on the Turing test where
 

651
00:17:32,470 --> 00:17:35,320
different twist on the Turing test where
we don't use words we all

652
00:17:35,320 --> 00:17:35,330
we don't use words we all
 

653
00:17:35,330 --> 00:17:38,980
we don't use words we all
using motions to speak expression of

654
00:17:38,980 --> 00:17:38,990
using motions to speak expression of
 

655
00:17:38,990 --> 00:17:42,850
using motions to speak expression of
those emotions and we create we use an

656
00:17:42,850 --> 00:17:42,860
those emotions and we create we use an
 

657
00:17:42,860 --> 00:17:48,970
those emotions and we create we use an
age a face customizable we're 26 muscles

658
00:17:48,970 --> 00:17:48,980
age a face customizable we're 26 muscles
 

659
00:17:48,980 --> 00:17:51,250
age a face customizable we're 26 muscles
all of which can be controlled with an

660
00:17:51,250 --> 00:17:51,260
all of which can be controlled with an
 

661
00:17:51,260 --> 00:17:54,580
all of which can be controlled with an
LS TM we use a neural network to train

662
00:17:54,580 --> 00:17:54,590
LS TM we use a neural network to train
 

663
00:17:54,590 --> 00:17:59,680
LS TM we use a neural network to train
the generation of emotion and the

664
00:17:59,680 --> 00:17:59,690
the generation of emotion and the
 

665
00:17:59,690 --> 00:18:03,279
the generation of emotion and the
competition in you submitting the code

666
00:18:03,279 --> 00:18:03,289
competition in you submitting the code
 

667
00:18:03,289 --> 00:18:08,100
competition in you submitting the code
to the competition is you get 10 seconds

668
00:18:08,100 --> 00:18:08,110
to the competition is you get 10 seconds
 

669
00:18:08,110 --> 00:18:12,340
to the competition is you get 10 seconds
to impress with the these expressions of

670
00:18:12,340 --> 00:18:12,350
to impress with the these expressions of
 

671
00:18:12,350 --> 00:18:13,450
to impress with the these expressions of
emotion the viewer

672
00:18:13,450 --> 00:18:13,460
emotion the viewer
 

673
00:18:13,460 --> 00:18:16,960
emotion the viewer
it's a be testing your goal is to

674
00:18:16,960 --> 00:18:16,970
it's a be testing your goal is to
 

675
00:18:16,970 --> 00:18:19,450
it's a be testing your goal is to
impress the viewer enough to where they

676
00:18:19,450 --> 00:18:19,460
impress the viewer enough to where they
 

677
00:18:19,460 --> 00:18:22,480
impress the viewer enough to where they
choose your agent versus another agent

678
00:18:22,480 --> 00:18:22,490
choose your agent versus another agent
 

679
00:18:22,490 --> 00:18:26,500
choose your agent versus another agent
and those that are most loved the agents

680
00:18:26,500 --> 00:18:26,510
and those that are most loved the agents
 

681
00:18:26,510 --> 00:18:29,049
and those that are most loved the agents
most loved will be the ones that are

682
00:18:29,049 --> 00:18:29,059
most loved will be the ones that are
 

683
00:18:29,059 --> 00:18:33,070
most loved will be the ones that are
declared winners in a twist we will add

684
00:18:33,070 --> 00:18:33,080
declared winners in a twist we will add
 

685
00:18:33,080 --> 00:18:36,700
declared winners in a twist we will add
human beings into this mix so we've

686
00:18:36,700 --> 00:18:36,710
human beings into this mix so we've
 

687
00:18:36,710 --> 00:18:39,549
human beings into this mix so we've
created a system that map's our human

688
00:18:39,549 --> 00:18:39,559
created a system that map's our human
 

689
00:18:39,559 --> 00:18:44,080
created a system that map's our human
faces myself and the TAS to where we

690
00:18:44,080 --> 00:18:44,090
faces myself and the TAS to where we
 

691
00:18:44,090 --> 00:18:45,909
faces myself and the TAS to where we
ourselves enter an outcome in the

692
00:18:45,909 --> 00:18:45,919
ourselves enter an outcome in the
 

693
00:18:45,919 --> 00:18:49,029
ourselves enter an outcome in the
competition and try to convince you to

694
00:18:49,029 --> 00:18:49,039
competition and try to convince you to
 

695
00:18:49,039 --> 00:18:50,610
competition and try to convince you to
keep us as your friend

696
00:18:50,610 --> 00:18:50,620
keep us as your friend
 

697
00:18:50,620 --> 00:18:58,080
keep us as your friend
that's the Turing test ethical car

698
00:18:58,080 --> 00:18:58,090
that's the Turing test ethical car
 

699
00:18:58,090 --> 00:19:01,450
that's the Turing test ethical car
building and the ideas of the trolley

700
00:19:01,450 --> 00:19:01,460
building and the ideas of the trolley
 

701
00:19:01,460 --> 00:19:03,580
building and the ideas of the trolley
problem and the moral machine done here

702
00:19:03,580 --> 00:19:03,590
problem and the moral machine done here
 

703
00:19:03,590 --> 00:19:05,379
problem and the moral machine done here
in the Media Lab the incredible

704
00:19:05,379 --> 00:19:05,389
in the Media Lab the incredible
 

705
00:19:05,389 --> 00:19:07,659
in the Media Lab the incredible
interesting work we take a machine

706
00:19:07,659 --> 00:19:07,669
interesting work we take a machine
 

707
00:19:07,669 --> 00:19:10,029
interesting work we take a machine
learning approach to it and take what

708
00:19:10,029 --> 00:19:10,039
learning approach to it and take what
 

709
00:19:10,039 --> 00:19:11,950
learning approach to it and take what
we've developed the deep reinforcement

710
00:19:11,950 --> 00:19:11,960
we've developed the deep reinforcement
 

711
00:19:11,960 --> 00:19:15,549
we've developed the deep reinforcement
learning competition for success 0 9 for

712
00:19:15,549 --> 00:19:15,559
learning competition for success 0 9 for
 

713
00:19:15,559 --> 00:19:19,629
learning competition for success 0 9 for
the deep traffic and we add pedestrians

714
00:19:19,629 --> 00:19:19,639
the deep traffic and we add pedestrians
 

715
00:19:19,639 --> 00:19:26,220
the deep traffic and we add pedestrians
into it stochastic astok irrational

716
00:19:26,220 --> 00:19:26,230
into it stochastic astok irrational
 

717
00:19:26,230 --> 00:19:29,289
into it stochastic astok irrational
unpredictable pedestrians and we add

718
00:19:29,289 --> 00:19:29,299
unpredictable pedestrians and we add
 

719
00:19:29,299 --> 00:19:31,990
unpredictable pedestrians and we add
human life to the loss function where

720
00:19:31,990 --> 00:19:32,000
human life to the loss function where
 

721
00:19:32,000 --> 00:19:34,750
human life to the loss function where
there's a trade-off between getting from

722
00:19:34,750 --> 00:19:34,760
there's a trade-off between getting from
 

723
00:19:34,760 --> 00:19:37,450
there's a trade-off between getting from
point A to point B so in deep traffic

724
00:19:37,450 --> 00:19:37,460
point A to point B so in deep traffic
 

725
00:19:37,460 --> 00:19:38,409
point A to point B so in deep traffic
the deep reinforcement learning

726
00:19:38,409 --> 00:19:38,419
the deep reinforcement learning
 

727
00:19:38,419 --> 00:19:40,210
the deep reinforcement learning
competition the goal was to go as fast

728
00:19:40,210 --> 00:19:40,220
competition the goal was to go as fast
 

729
00:19:40,220 --> 00:19:44,350
competition the goal was to go as fast
as possible here it's up to you to

730
00:19:44,350 --> 00:19:44,360
as possible here it's up to you to
 

731
00:19:44,360 --> 00:19:47,379
as possible here it's up to you to
decide what what your agents goal is

732
00:19:47,379 --> 00:19:47,389
decide what what your agents goal is
 

733
00:19:47,389 --> 00:19:49,690
decide what what your agents goal is
there's a parade of front

734
00:19:49,690 --> 00:19:49,700
there's a parade of front
 

735
00:19:49,700 --> 00:19:52,630
there's a parade of front
trade-off between getting from point A

736
00:19:52,630 --> 00:19:52,640
trade-off between getting from point A
 

737
00:19:52,640 --> 00:19:58,380
trade-off between getting from point A
to point B as fast as possible and

738
00:19:58,380 --> 00:19:58,390

 

739
00:19:58,390 --> 00:20:09,850

hurting pedestrians this is not a

740
00:20:09,850 --> 00:20:09,860
hurting pedestrians this is not a
 

741
00:20:09,860 --> 00:20:13,000
hurting pedestrians this is not a
ethical question it's an engineering

742
00:20:13,000 --> 00:20:13,010
ethical question it's an engineering
 

743
00:20:13,010 --> 00:20:20,320
ethical question it's an engineering
question and it's a serious one because

744
00:20:20,320 --> 00:20:20,330
question and it's a serious one because
 

745
00:20:20,330 --> 00:20:23,260
question and it's a serious one because
fundamentally in creating autonomous

746
00:20:23,260 --> 00:20:23,270
fundamentally in creating autonomous
 

747
00:20:23,270 --> 00:20:25,900
fundamentally in creating autonomous
vehicles that function in this world we

748
00:20:25,900 --> 00:20:25,910
vehicles that function in this world we
 

749
00:20:25,910 --> 00:20:28,840
vehicles that function in this world we
want them to get from point A to point B

750
00:20:28,840 --> 00:20:28,850
want them to get from point A to point B
 

751
00:20:28,850 --> 00:20:30,660
want them to get from point A to point B
as quickly as possible

752
00:20:30,660 --> 00:20:30,670
as quickly as possible
 

753
00:20:30,670 --> 00:20:34,570
as quickly as possible
the United States government insurance

754
00:20:34,570 --> 00:20:34,580
the United States government insurance
 

755
00:20:34,580 --> 00:20:38,070
the United States government insurance
companies put a price tag on human life

756
00:20:38,070 --> 00:20:38,080
companies put a price tag on human life
 

757
00:20:38,080 --> 00:20:40,870
companies put a price tag on human life
we put that power in your hands

758
00:20:40,870 --> 00:20:40,880
we put that power in your hands
 

759
00:20:40,880 --> 00:20:43,240
we put that power in your hands
in designing these agents to ask the

760
00:20:43,240 --> 00:20:43,250
in designing these agents to ask the
 

761
00:20:43,250 --> 00:20:47,710
in designing these agents to ask the
question of a how can we create machine

762
00:20:47,710 --> 00:20:47,720
question of a how can we create machine
 

763
00:20:47,720 --> 00:20:49,540
question of a how can we create machine
learning systems where the objective

764
00:20:49,540 --> 00:20:49,550
learning systems where the objective
 

765
00:20:49,550 --> 00:20:51,820
learning systems where the objective
function the loss function has human

766
00:20:51,820 --> 00:20:51,830
function the loss function has human
 

767
00:20:51,830 --> 00:20:56,740
function the loss function has human
life as part of it and vote AI is an

768
00:20:56,740 --> 00:20:56,750
life as part of it and vote AI is an
 

769
00:20:56,750 --> 00:21:02,680
life as part of it and vote AI is an
aggregator of different links different

770
00:21:02,680 --> 00:21:02,690
aggregator of different links different
 

771
00:21:02,690 --> 00:21:05,380
aggregator of different links different
articles papers videos on the topic of

772
00:21:05,380 --> 00:21:05,390
articles papers videos on the topic of
 

773
00:21:05,390 --> 00:21:07,360
articles papers videos on the topic of
artificial general intelligence where

774
00:21:07,360 --> 00:21:07,370
artificial general intelligence where
 

775
00:21:07,370 --> 00:21:11,620
artificial general intelligence where
people vote on vote quality articles up

776
00:21:11,620 --> 00:21:11,630
people vote on vote quality articles up
 

777
00:21:11,630 --> 00:21:16,660
people vote on vote quality articles up
and down and choose on the sentiment of

778
00:21:16,660 --> 00:21:16,670
and down and choose on the sentiment of
 

779
00:21:16,670 --> 00:21:18,640
and down and choose on the sentiment of
positive and negative we'd like to

780
00:21:18,640 --> 00:21:18,650
positive and negative we'd like to
 

781
00:21:18,650 --> 00:21:20,710
positive and negative we'd like to
explore the different ways to the

782
00:21:20,710 --> 00:21:20,720
explore the different ways to the
 

783
00:21:20,720 --> 00:21:22,300
explore the different ways to the
different arguments for and against

784
00:21:22,300 --> 00:21:22,310
different arguments for and against
 

785
00:21:22,310 --> 00:21:27,370
different arguments for and against
artificial general intelligence there is

786
00:21:27,370 --> 00:21:27,380
artificial general intelligence there is
 

787
00:21:27,380 --> 00:21:30,670
artificial general intelligence there is
an incredible list of speakers the best

788
00:21:30,670 --> 00:21:30,680
an incredible list of speakers the best
 

789
00:21:30,680 --> 00:21:34,110
an incredible list of speakers the best
in their disciplines from Josh Tenenbaum

790
00:21:34,110 --> 00:21:34,120
in their disciplines from Josh Tenenbaum
 

791
00:21:34,120 --> 00:21:38,440
in their disciplines from Josh Tenenbaum
PN MIT to Ray Kurzweil at Google to lisa

792
00:21:38,440 --> 00:21:38,450
PN MIT to Ray Kurzweil at Google to lisa
 

793
00:21:38,450 --> 00:21:42,040
PN MIT to Ray Kurzweil at Google to lisa
Feldman Barrett and Nader Pinsky from

794
00:21:42,040 --> 00:21:42,050
Feldman Barrett and Nader Pinsky from
 

795
00:21:42,050 --> 00:21:43,540
Feldman Barrett and Nader Pinsky from
Northeastern University

796
00:21:43,540 --> 00:21:43,550
Northeastern University
 

797
00:21:43,550 --> 00:21:48,190
Northeastern University
Andre karpati Stephen Wolfram Richard

798
00:21:48,190 --> 00:21:48,200
Andre karpati Stephen Wolfram Richard
 

799
00:21:48,200 --> 00:21:50,200
Andre karpati Stephen Wolfram Richard
Moyes mark Robert

800
00:21:50,200 --> 00:21:50,210
Moyes mark Robert
 

801
00:21:50,210 --> 00:21:55,930
Moyes mark Robert
Ilya sutskever and myself Josh Tenenbaum

802
00:21:55,930 --> 00:21:55,940
Ilya sutskever and myself Josh Tenenbaum
 

803
00:21:55,940 --> 00:21:59,200
Ilya sutskever and myself Josh Tenenbaum
tomorrow I'd like to go through each of

804
00:21:59,200 --> 00:21:59,210
tomorrow I'd like to go through each of
 

805
00:21:59,210 --> 00:22:01,330
tomorrow I'd like to go through each of
these speakers and talk about the

806
00:22:01,330 --> 00:22:01,340
these speakers and talk about the
 

807
00:22:01,340 --> 00:22:03,130
these speakers and talk about the
perspectives they bring

808
00:22:03,130 --> 00:22:03,140
perspectives they bring
 

809
00:22:03,140 --> 00:22:06,730
perspectives they bring
that to try to see the approach the

810
00:22:06,730 --> 00:22:06,740
that to try to see the approach the
 

811
00:22:06,740 --> 00:22:09,010
that to try to see the approach the
ideas they bring to the table they're

812
00:22:09,010 --> 00:22:09,020
ideas they bring to the table they're
 

813
00:22:09,020 --> 00:22:13,950
ideas they bring to the table they're
not in most cases interested in the

814
00:22:13,950 --> 00:22:13,960
not in most cases interested in the
 

815
00:22:13,960 --> 00:22:16,570
not in most cases interested in the
discussion of the future impact on

816
00:22:16,570 --> 00:22:16,580
discussion of the future impact on
 

817
00:22:16,580 --> 00:22:19,540
discussion of the future impact on
society without grounding it into the

818
00:22:19,540 --> 00:22:19,550
society without grounding it into the
 

819
00:22:19,550 --> 00:22:21,910
society without grounding it into the
expertise into the actual engineering

820
00:22:21,910 --> 00:22:21,920
expertise into the actual engineering
 

821
00:22:21,920 --> 00:22:24,040
expertise into the actual engineering
into creating these intelligent systems

822
00:22:24,040 --> 00:22:24,050
into creating these intelligent systems
 

823
00:22:24,050 --> 00:22:26,380
into creating these intelligent systems
so josh is a computational cognitive

824
00:22:26,380 --> 00:22:26,390
so josh is a computational cognitive
 

825
00:22:26,390 --> 00:22:30,730
so josh is a computational cognitive
science expert professor faculty here at

826
00:22:30,730 --> 00:22:30,740
science expert professor faculty here at
 

827
00:22:30,740 --> 00:22:34,510
science expert professor faculty here at
MIT he will talk about how we can create

828
00:22:34,510 --> 00:22:34,520
MIT he will talk about how we can create
 

829
00:22:34,520 --> 00:22:37,260
MIT he will talk about how we can create
common-sense understanding systems that

830
00:22:37,260 --> 00:22:37,270
common-sense understanding systems that
 

831
00:22:37,270 --> 00:22:39,760
common-sense understanding systems that
see a world of physical objects and

832
00:22:39,760 --> 00:22:39,770
see a world of physical objects and
 

833
00:22:39,770 --> 00:22:42,580
see a world of physical objects and
their interactions and our own

834
00:22:42,580 --> 00:22:42,590
their interactions and our own
 

835
00:22:42,590 --> 00:22:44,410
their interactions and our own
possibilities to act interact with

836
00:22:44,410 --> 00:22:44,420
possibilities to act interact with
 

837
00:22:44,420 --> 00:22:46,420
possibilities to act interact with
others the intuitive physics how do we

838
00:22:46,420 --> 00:22:46,430
others the intuitive physics how do we
 

839
00:22:46,430 --> 00:22:49,120
others the intuitive physics how do we
build into systems the intuitive physics

840
00:22:49,120 --> 00:22:49,130
build into systems the intuitive physics
 

841
00:22:49,130 --> 00:22:52,000
build into systems the intuitive physics
of the world more than just the deep

842
00:22:52,000 --> 00:22:52,010
of the world more than just the deep
 

843
00:22:52,010 --> 00:22:54,940
of the world more than just the deep
learning memorization engines that take

844
00:22:54,940 --> 00:22:54,950
learning memorization engines that take
 

845
00:22:54,950 --> 00:22:57,190
learning memorization engines that take
patterns and learn through supervised

846
00:22:57,190 --> 00:22:57,200
patterns and learn through supervised
 

847
00:22:57,200 --> 00:22:59,140
patterns and learn through supervised
way to map those patterns to

848
00:22:59,140 --> 00:22:59,150
way to map those patterns to
 

849
00:22:59,150 --> 00:23:02,590
way to map those patterns to
classification actually begin to

850
00:23:02,590 --> 00:23:02,600
classification actually begin to
 

851
00:23:02,600 --> 00:23:04,420
classification actually begin to
understand the intuitive the

852
00:23:04,420 --> 00:23:04,430
understand the intuitive the
 

853
00:23:04,430 --> 00:23:06,930
understand the intuitive the
common-sense physics of the world and

854
00:23:06,930 --> 00:23:06,940
common-sense physics of the world and
 

855
00:23:06,940 --> 00:23:10,330
common-sense physics of the world and
learn rapid model based learning learn

856
00:23:10,330 --> 00:23:10,340
learn rapid model based learning learn
 

857
00:23:10,340 --> 00:23:12,700
learn rapid model based learning learn
from nothing learn from very little just

858
00:23:12,700 --> 00:23:12,710
from nothing learn from very little just
 

859
00:23:12,710 --> 00:23:14,740
from nothing learn from very little just
like we do as children just like we do

860
00:23:14,740 --> 00:23:14,750
like we do as children just like we do
 

861
00:23:14,750 --> 00:23:16,930
like we do as children just like we do
as human being successfully often only

862
00:23:16,930 --> 00:23:16,940
as human being successfully often only
 

863
00:23:16,940 --> 00:23:19,480
as human being successfully often only
need one example to learn a concept how

864
00:23:19,480 --> 00:23:19,490
need one example to learn a concept how
 

865
00:23:19,490 --> 00:23:22,090
need one example to learn a concept how
do we create systems that learn from

866
00:23:22,090 --> 00:23:22,100
do we create systems that learn from
 

867
00:23:22,100 --> 00:23:26,220
do we create systems that learn from
very few sometimes a single example and

868
00:23:26,220 --> 00:23:26,230
very few sometimes a single example and
 

869
00:23:26,230 --> 00:23:28,870
very few sometimes a single example and
integrate ideas from various disciplines

870
00:23:28,870 --> 00:23:28,880
integrate ideas from various disciplines
 

871
00:23:28,880 --> 00:23:31,870
integrate ideas from various disciplines
of course from neural networks but also

872
00:23:31,870 --> 00:23:31,880
of course from neural networks but also
 

873
00:23:31,880 --> 00:23:33,640
of course from neural networks but also
probabilistic generative models and

874
00:23:33,640 --> 00:23:33,650
probabilistic generative models and
 

875
00:23:33,650 --> 00:23:37,060
probabilistic generative models and
symbol processing architectures it's

876
00:23:37,060 --> 00:23:37,070
symbol processing architectures it's
 

877
00:23:37,070 --> 00:23:40,750
symbol processing architectures it's
going to be incredible of course from a

878
00:23:40,750 --> 00:23:40,760
going to be incredible of course from a
 

879
00:23:40,760 --> 00:23:42,810
going to be incredible of course from a
from a different area of the world

880
00:23:42,810 --> 00:23:42,820
from a different area of the world
 

881
00:23:42,820 --> 00:23:46,600
from a different area of the world
another incredible thinker intellectual

882
00:23:46,600 --> 00:23:46,610
another incredible thinker intellectual
 

883
00:23:46,610 --> 00:23:49,060
another incredible thinker intellectual
speaker is Ray Kurzweil he'll be here on

884
00:23:49,060 --> 00:23:49,070
speaker is Ray Kurzweil he'll be here on
 

885
00:23:49,070 --> 00:23:53,670
speaker is Ray Kurzweil he'll be here on
Wednesday and 1:00 p.m. and he will do a

886
00:23:53,670 --> 00:23:53,680
Wednesday and 1:00 p.m. and he will do a
 

887
00:23:53,680 --> 00:23:58,570
Wednesday and 1:00 p.m. and he will do a
whirlwind discussion of where we stand

888
00:23:58,570 --> 00:23:58,580
whirlwind discussion of where we stand
 

889
00:23:58,580 --> 00:24:00,070
whirlwind discussion of where we stand
with intelligence creating intelligent

890
00:24:00,070 --> 00:24:00,080
with intelligence creating intelligent
 

891
00:24:00,080 --> 00:24:02,710
with intelligence creating intelligent
systems how we see natural intelligence

892
00:24:02,710 --> 00:24:02,720
systems how we see natural intelligence
 

893
00:24:02,720 --> 00:24:04,870
systems how we see natural intelligence
our own human intelligence how we define

894
00:24:04,870 --> 00:24:04,880
our own human intelligence how we define
 

895
00:24:04,880 --> 00:24:07,570
our own human intelligence how we define
it how we understand it and how that

896
00:24:07,570 --> 00:24:07,580
it how we understand it and how that
 

897
00:24:07,580 --> 00:24:10,750
it how we understand it and how that
transfers to the increasing exponential

898
00:24:10,750 --> 00:24:10,760
transfers to the increasing exponential
 

899
00:24:10,760 --> 00:24:12,850
transfers to the increasing exponential
growth of development of artificial

900
00:24:12,850 --> 00:24:12,860
growth of development of artificial
 

901
00:24:12,860 --> 00:24:16,900
growth of development of artificial
general intelligence

902
00:24:16,900 --> 00:24:16,910

 

903
00:24:16,910 --> 00:24:19,180

something I'm myself very excited about

904
00:24:19,180 --> 00:24:19,190
something I'm myself very excited about
 

905
00:24:19,190 --> 00:24:24,190
something I'm myself very excited about
is Lisa Feldman Barrett coming here on

906
00:24:24,190 --> 00:24:24,200
is Lisa Feldman Barrett coming here on
 

907
00:24:24,200 --> 00:24:27,430
is Lisa Feldman Barrett coming here on
Thursday she's written a book I believe

908
00:24:27,430 --> 00:24:27,440
Thursday she's written a book I believe
 

909
00:24:27,440 --> 00:24:31,230
Thursday she's written a book I believe
how emotions are made she argues that

910
00:24:31,230 --> 00:24:31,240
how emotions are made she argues that
 

911
00:24:31,240 --> 00:24:35,100
how emotions are made she argues that
emotions are created that there is a

912
00:24:35,100 --> 00:24:35,110
emotions are created that there is a
 

913
00:24:35,110 --> 00:24:38,350
emotions are created that there is a
distinction there's a detachment between

914
00:24:38,350 --> 00:24:38,360
distinction there's a detachment between
 

915
00:24:38,360 --> 00:24:41,350
distinction there's a detachment between
what we feel in our bodies the physical

916
00:24:41,350 --> 00:24:41,360
what we feel in our bodies the physical
 

917
00:24:41,360 --> 00:24:43,870
what we feel in our bodies the physical
state of our bodies and the expression

918
00:24:43,870 --> 00:24:43,880
state of our bodies and the expression
 

919
00:24:43,880 --> 00:24:47,070
state of our bodies and the expression
of emotion from from body to the

920
00:24:47,070 --> 00:24:47,080
of emotion from from body to the
 

921
00:24:47,080 --> 00:24:49,720
of emotion from from body to the
contextually grounded to the face

922
00:24:49,720 --> 00:24:49,730
contextually grounded to the face
 

923
00:24:49,730 --> 00:24:52,870
contextually grounded to the face
expressing that emotion which means now

924
00:24:52,870 --> 00:24:52,880
expressing that emotion which means now
 

925
00:24:52,880 --> 00:24:55,000
expressing that emotion which means now
why is there as a person who is

926
00:24:55,000 --> 00:24:55,010
why is there as a person who is
 

927
00:24:55,010 --> 00:24:57,640
why is there as a person who is
psychology person in a fundamental

928
00:24:57,640 --> 00:24:57,650
psychology person in a fundamental
 

929
00:24:57,650 --> 00:24:59,860
psychology person in a fundamental
engineering computer science topic like

930
00:24:59,860 --> 00:24:59,870
engineering computer science topic like
 

931
00:24:59,870 --> 00:25:03,520
engineering computer science topic like
AGI because if emotions are created in

932
00:25:03,520 --> 00:25:03,530
AGI because if emotions are created in
 

933
00:25:03,530 --> 00:25:04,780
AGI because if emotions are created in
the way she argues and she'll

934
00:25:04,780 --> 00:25:04,790
the way she argues and she'll
 

935
00:25:04,790 --> 00:25:06,340
the way she argues and she'll
systematically break it down

936
00:25:06,340 --> 00:25:06,350
systematically break it down
 

937
00:25:06,350 --> 00:25:10,630
systematically break it down
that means we're learning societal as

938
00:25:10,630 --> 00:25:10,640
that means we're learning societal as
 

939
00:25:10,640 --> 00:25:12,220
that means we're learning societal as
human beings were learning societal

940
00:25:12,220 --> 00:25:12,230
human beings were learning societal
 

941
00:25:12,230 --> 00:25:14,590
human beings were learning societal
norms of how to express emotion the idea

942
00:25:14,590 --> 00:25:14,600
norms of how to express emotion the idea
 

943
00:25:14,600 --> 00:25:16,120
norms of how to express emotion the idea
of emotional intelligence is learned

944
00:25:16,120 --> 00:25:16,130
of emotional intelligence is learned
 

945
00:25:16,130 --> 00:25:19,060
of emotional intelligence is learned
which means we can have machines learn

946
00:25:19,060 --> 00:25:19,070
which means we can have machines learn
 

947
00:25:19,070 --> 00:25:22,090
which means we can have machines learn
this idea it's a machine learn like it's

948
00:25:22,090 --> 00:25:22,100
this idea it's a machine learn like it's
 

949
00:25:22,100 --> 00:25:23,470
this idea it's a machine learn like it's
a human learning problem it's a machine

950
00:25:23,470 --> 00:25:23,480
a human learning problem it's a machine
 

951
00:25:23,480 --> 00:25:27,130
a human learning problem it's a machine
learning problem in a little bit of a

952
00:25:27,130 --> 00:25:27,140
learning problem in a little bit of a
 

953
00:25:27,140 --> 00:25:29,620
learning problem in a little bit of a
twist she asked that instead of giving

954
00:25:29,620 --> 00:25:29,630
twist she asked that instead of giving
 

955
00:25:29,630 --> 00:25:32,760
twist she asked that instead of giving
it talk I have a conversation with her

956
00:25:32,760 --> 00:25:32,770
it talk I have a conversation with her
 

957
00:25:32,770 --> 00:25:35,050
it talk I have a conversation with her
so it's going to be a little bit

958
00:25:35,050 --> 00:25:35,060
so it's going to be a little bit
 

959
00:25:35,060 --> 00:25:38,170
so it's going to be a little bit
challenging and fun and she's great

960
00:25:38,170 --> 00:25:38,180
challenging and fun and she's great
 

961
00:25:38,180 --> 00:25:41,620
challenging and fun and she's great
looking forward to it and we'll explore

962
00:25:41,620 --> 00:25:41,630
looking forward to it and we'll explore
 

963
00:25:41,630 --> 00:25:45,340
looking forward to it and we'll explore
different ways that we can get emotion

964
00:25:45,340 --> 00:25:45,350
different ways that we can get emotion
 

965
00:25:45,350 --> 00:25:47,860
different ways that we can get emotion
expressed through video through audio

966
00:25:47,860 --> 00:25:47,870
expressed through video through audio
 

967
00:25:47,870 --> 00:25:51,430
expressed through video through audio
through the project the angel project

968
00:25:51,430 --> 00:25:51,440
through the project the angel project
 

969
00:25:51,440 --> 00:25:53,710
through the project the angel project
that I mentioned so there's been worked

970
00:25:53,710 --> 00:25:53,720
that I mentioned so there's been worked
 

971
00:25:53,720 --> 00:25:57,190
that I mentioned so there's been worked
in reenacting intelligence so well

972
00:25:57,190 --> 00:25:57,200
in reenacting intelligence so well
 

973
00:25:57,200 --> 00:26:00,640
in reenacting intelligence so well
reenacting mapping face to face mapping

974
00:26:00,640 --> 00:26:00,650
reenacting mapping face to face mapping
 

975
00:26:00,650 --> 00:26:02,530
reenacting mapping face to face mapping
different emotions on video that was

976
00:26:02,530 --> 00:26:02,540
different emotions on video that was
 

977
00:26:02,540 --> 00:26:05,020
different emotions on video that was
previously recorded so if you can

978
00:26:05,020 --> 00:26:05,030
previously recorded so if you can
 

979
00:26:05,030 --> 00:26:08,380
previously recorded so if you can
imagine that means we can take emotions

980
00:26:08,380 --> 00:26:08,390
imagine that means we can take emotions
 

981
00:26:08,390 --> 00:26:10,870
imagine that means we can take emotions
that we've created the kind of emotion

982
00:26:10,870 --> 00:26:10,880
that we've created the kind of emotion
 

983
00:26:10,880 --> 00:26:13,150
that we've created the kind of emotion
creation we've been discussing and remap

984
00:26:13,150 --> 00:26:13,160
creation we've been discussing and remap
 

985
00:26:13,160 --> 00:26:17,440
creation we've been discussing and remap
it on previous video that's one way to

986
00:26:17,440 --> 00:26:17,450
it on previous video that's one way to
 

987
00:26:17,450 --> 00:26:19,840
it on previous video that's one way to
see intelligence is taking raw human

988
00:26:19,840 --> 00:26:19,850
see intelligence is taking raw human
 

989
00:26:19,850 --> 00:26:23,080
see intelligence is taking raw human
data that we already have and mapping

990
00:26:23,080 --> 00:26:23,090
data that we already have and mapping
 

991
00:26:23,090 --> 00:26:25,550
data that we already have and mapping
new computer-generated

992
00:26:25,550 --> 00:26:25,560
new computer-generated
 

993
00:26:25,560 --> 00:26:28,880
new computer-generated
the the underlying fundamentals of human

994
00:26:28,880 --> 00:26:28,890
the the underlying fundamentals of human
 

995
00:26:28,890 --> 00:26:31,610
the the underlying fundamentals of human
but the surface appearance the

996
00:26:31,610 --> 00:26:31,620
but the surface appearance the
 

997
00:26:31,620 --> 00:26:34,330
but the surface appearance the
representation of emotion visual or

998
00:26:34,330 --> 00:26:34,340
representation of emotion visual or
 

999
00:26:34,340 --> 00:26:40,640
representation of emotion visual or
auditory is generated by a computer it

1000
00:26:40,640 --> 00:26:40,650
auditory is generated by a computer it
 

1001
00:26:40,650 --> 00:27:13,480
auditory is generated by a computer it
could be in the embodied form

1002
00:27:13,480 --> 00:27:13,490

 

1003
00:27:13,490 --> 00:27:17,140

[Music]

1004
00:27:17,140 --> 00:27:17,150

 

1005
00:27:17,150 --> 00:27:19,910

very important to note for those

1006
00:27:19,910 --> 00:27:19,920
very important to note for those
 

1007
00:27:19,920 --> 00:27:22,730
very important to note for those
captivated by Sofia in the press or have

1008
00:27:22,730 --> 00:27:22,740
captivated by Sofia in the press or have
 

1009
00:27:22,740 --> 00:27:24,550
captivated by Sofia in the press or have
seen these videos

1010
00:27:24,550 --> 00:27:24,560
seen these videos
 

1011
00:27:24,560 --> 00:27:29,240
seen these videos
Sofia is an art exhibit she's not a

1012
00:27:29,240 --> 00:27:29,250
Sofia is an art exhibit she's not a
 

1013
00:27:29,250 --> 00:27:31,420
Sofia is an art exhibit she's not a
strong natural language processing

1014
00:27:31,420 --> 00:27:31,430
strong natural language processing
 

1015
00:27:31,430 --> 00:27:35,540
strong natural language processing
system this is not an AGI system but

1016
00:27:35,540 --> 00:27:35,550
system this is not an AGI system but
 

1017
00:27:35,550 --> 00:27:38,420
system this is not an AGI system but
it's a beautiful visualization of

1018
00:27:38,420 --> 00:27:38,430
it's a beautiful visualization of
 

1019
00:27:38,430 --> 00:27:40,490
it's a beautiful visualization of
embodying of Hollow

1020
00:27:40,490 --> 00:27:40,500
embodying of Hollow
 

1021
00:27:40,500 --> 00:27:42,830
embodying of Hollow
it's a beautiful visualization of how

1022
00:27:42,830 --> 00:27:42,840
it's a beautiful visualization of how
 

1023
00:27:42,840 --> 00:27:44,810
it's a beautiful visualization of how
easy it is to trick us human beings that

1024
00:27:44,810 --> 00:27:44,820
easy it is to trick us human beings that
 

1025
00:27:44,820 --> 00:27:47,150
easy it is to trick us human beings that
there's intelligence underlying

1026
00:27:47,150 --> 00:27:47,160
there's intelligence underlying
 

1027
00:27:47,160 --> 00:27:50,150
there's intelligence underlying
something that the emotional expression

1028
00:27:50,150 --> 00:27:50,160
something that the emotional expression
 

1029
00:27:50,160 --> 00:27:52,190
something that the emotional expression
the physical embodiment and the

1030
00:27:52,190 --> 00:27:52,200
the physical embodiment and the
 

1031
00:27:52,200 --> 00:27:55,820
the physical embodiment and the
emotional expression that has a that has

1032
00:27:55,820 --> 00:27:55,830
emotional expression that has a that has
 

1033
00:27:55,830 --> 00:27:58,430
emotional expression that has a that has
some degree of humor that has some

1034
00:27:58,430 --> 00:27:58,440
some degree of humor that has some
 

1035
00:27:58,440 --> 00:28:01,610
some degree of humor that has some
degree of wit and intelligence is enough

1036
00:28:01,610 --> 00:28:01,620
degree of wit and intelligence is enough
 

1037
00:28:01,620 --> 00:28:04,160
degree of wit and intelligence is enough
to captivate us so that's an argument

1038
00:28:04,160 --> 00:28:04,170
to captivate us so that's an argument
 

1039
00:28:04,170 --> 00:28:06,320
to captivate us so that's an argument
for not creating intelligence from

1040
00:28:06,320 --> 00:28:06,330
for not creating intelligence from
 

1041
00:28:06,330 --> 00:28:09,290
for not creating intelligence from
scratch but having machines at the very

1042
00:28:09,290 --> 00:28:09,300
scratch but having machines at the very
 

1043
00:28:09,300 --> 00:28:12,440
scratch but having machines at the very
surface the display of that emotion the

1044
00:28:12,440 --> 00:28:12,450
surface the display of that emotion the
 

1045
00:28:12,450 --> 00:28:15,560
surface the display of that emotion the
generation the mapping of the visual and

1046
00:28:15,560 --> 00:28:15,570
generation the mapping of the visual and
 

1047
00:28:15,570 --> 00:28:18,530
generation the mapping of the visual and
auditory elements worth underneath it is

1048
00:28:18,530 --> 00:28:18,540
auditory elements worth underneath it is
 

1049
00:28:18,540 --> 00:28:21,020
auditory elements worth underneath it is
really trivial technology that's

1050
00:28:21,020 --> 00:28:21,030
really trivial technology that's
 

1051
00:28:21,030 --> 00:28:23,360
really trivial technology that's
fundamentally relying on humans like in

1052
00:28:23,360 --> 00:28:23,370
fundamentally relying on humans like in
 

1053
00:28:23,370 --> 00:28:26,900
fundamentally relying on humans like in
the sophia's case and in the simplest

1054
00:28:26,900 --> 00:28:26,910
the sophia's case and in the simplest
 

1055
00:28:26,910 --> 00:28:32,300
the sophia's case and in the simplest
form we remove all elements of Hajus a

1056
00:28:32,300 --> 00:28:32,310
form we remove all elements of Hajus a
 

1057
00:28:32,310 --> 00:28:35,450
form we remove all elements of Hajus a
attractive appearance from from an agent

1058
00:28:35,450 --> 00:28:35,460
attractive appearance from from an agent
 

1059
00:28:35,460 --> 00:28:37,460
attractive appearance from from an agent
we really keep it to the simplest

1060
00:28:37,460 --> 00:28:37,470
we really keep it to the simplest
 

1061
00:28:37,470 --> 00:28:39,170
we really keep it to the simplest
muscles

1062
00:28:39,170 --> 00:28:39,180
muscles
 

1063
00:28:39,180 --> 00:28:41,390
muscles
characteristics of the face and see with

1064
00:28:41,390 --> 00:28:41,400
characteristics of the face and see with
 

1065
00:28:41,400 --> 00:28:43,490
characteristics of the face and see with
26 muscles controlled by a neural

1066
00:28:43,490 --> 00:28:43,500
26 muscles controlled by a neural
 

1067
00:28:43,500 --> 00:28:45,560
26 muscles controlled by a neural
network through time so recurrent neural

1068
00:28:45,560 --> 00:28:45,570
network through time so recurrent neural
 

1069
00:28:45,570 --> 00:28:48,860
network through time so recurrent neural
network I was TM how can we explore the

1070
00:28:48,860 --> 00:28:48,870
network I was TM how can we explore the
 

1071
00:28:48,870 --> 00:28:51,710
network I was TM how can we explore the
generation of emotion can we get this

1072
00:28:51,710 --> 00:28:51,720
generation of emotion can we get this
 

1073
00:28:51,720 --> 00:28:53,150
generation of emotion can we get this
thing and this is an open question for

1074
00:28:53,150 --> 00:28:53,160
thing and this is an open question for
 

1075
00:28:53,160 --> 00:28:54,950
thing and this is an open question for
us too we just created the system we

1076
00:28:54,950 --> 00:28:54,960
us too we just created the system we
 

1077
00:28:54,960 --> 00:28:57,050
us too we just created the system we
don't know if we can can we get it to

1078
00:28:57,050 --> 00:28:57,060
don't know if we can can we get it to
 

1079
00:28:57,060 --> 00:29:00,860
don't know if we can can we get it to
make us feel something make us feel

1080
00:29:00,860 --> 00:29:00,870
make us feel something make us feel
 

1081
00:29:00,870 --> 00:29:05,270
make us feel something make us feel
something by watching it express its

1082
00:29:05,270 --> 00:29:05,280
something by watching it express its
 

1083
00:29:05,280 --> 00:29:08,420
something by watching it express its
feelings can it become human before our

1084
00:29:08,420 --> 00:29:08,430
feelings can it become human before our
 

1085
00:29:08,430 --> 00:29:11,330
feelings can it become human before our
eyes can I learn to by competing against

1086
00:29:11,330 --> 00:29:11,340
eyes can I learn to by competing against
 

1087
00:29:11,340 --> 00:29:15,650
eyes can I learn to by competing against
other agents a be testing on Turk a

1088
00:29:15,650 --> 00:29:15,660
other agents a be testing on Turk a
 

1089
00:29:15,660 --> 00:29:19,040
other agents a be testing on Turk a
Mechanical Turk can the winners be very

1090
00:29:19,040 --> 00:29:19,050
Mechanical Turk can the winners be very
 

1091
00:29:19,050 --> 00:29:24,130
Mechanical Turk can the winners be very
convincing to make us feel entertained

1092
00:29:24,130 --> 00:29:24,140
convincing to make us feel entertained
 

1093
00:29:24,140 --> 00:29:28,610
convincing to make us feel entertained
pity love maybe some of you will fall in

1094
00:29:28,610 --> 00:29:28,620
pity love maybe some of you will fall in
 

1095
00:29:28,620 --> 00:29:34,580
pity love maybe some of you will fall in
love with angel here Nate dibinsky on

1096
00:29:34,580 --> 00:29:34,590
love with angel here Nate dibinsky on
 

1097
00:29:34,590 --> 00:29:36,230
love with angel here Nate dibinsky on
Friday will talk about cognitive

1098
00:29:36,230 --> 00:29:36,240
Friday will talk about cognitive
 

1099
00:29:36,240 --> 00:29:38,990
Friday will talk about cognitive
modeling architectures so you will speak

1100
00:29:38,990 --> 00:29:39,000
modeling architectures so you will speak
 

1101
00:29:39,000 --> 00:29:40,910
modeling architectures so you will speak
about the cognitive modeling aspect can

1102
00:29:40,910 --> 00:29:40,920
about the cognitive modeling aspect can
 

1103
00:29:40,920 --> 00:29:45,020
about the cognitive modeling aspect can
we have a ma can we model cognition in

1104
00:29:45,020 --> 00:29:45,030
we have a ma can we model cognition in
 

1105
00:29:45,030 --> 00:29:47,660
we have a ma can we model cognition in
some kind of systematic way to try to

1106
00:29:47,660 --> 00:29:47,670
some kind of systematic way to try to
 

1107
00:29:47,670 --> 00:29:49,250
some kind of systematic way to try to
build intuition of how complicated

1108
00:29:49,250 --> 00:29:49,260
build intuition of how complicated
 

1109
00:29:49,260 --> 00:29:57,380
build intuition of how complicated
cognition is Andre karpati famous for

1110
00:29:57,380 --> 00:29:57,390
cognition is Andre karpati famous for
 

1111
00:29:57,390 --> 00:29:59,960
cognition is Andre karpati famous for
being the state-of-the-art human on the

1112
00:29:59,960 --> 00:29:59,970
being the state-of-the-art human on the
 

1113
00:29:59,970 --> 00:30:02,450
being the state-of-the-art human on the
imagenet challenge the representative

1114
00:30:02,450 --> 00:30:02,460
imagenet challenge the representative
 

1115
00:30:02,460 --> 00:30:07,010
imagenet challenge the representative
the 95% accuracy performance among other

1116
00:30:07,010 --> 00:30:07,020
the 95% accuracy performance among other
 

1117
00:30:07,020 --> 00:30:09,590
the 95% accuracy performance among other
things he's also famous for his now a

1118
00:30:09,590 --> 00:30:09,600
things he's also famous for his now a
 

1119
00:30:09,600 --> 00:30:14,410
things he's also famous for his now a
Tesla he will talk about the role the

1120
00:30:14,410 --> 00:30:14,420
Tesla he will talk about the role the
 

1121
00:30:14,420 --> 00:30:17,180
Tesla he will talk about the role the
limitations the possibilities of deep

1122
00:30:17,180 --> 00:30:17,190
limitations the possibilities of deep
 

1123
00:30:17,190 --> 00:30:22,550
limitations the possibilities of deep
learning we'll talk as I have spoken

1124
00:30:22,550 --> 00:30:22,560
learning we'll talk as I have spoken
 

1125
00:30:22,560 --> 00:30:25,730
learning we'll talk as I have spoken
about in the past few weeks and

1126
00:30:25,730 --> 00:30:25,740
about in the past few weeks and
 

1127
00:30:25,740 --> 00:30:29,420
about in the past few weeks and
throughout about our misunderstanding or

1128
00:30:29,420 --> 00:30:29,430
throughout about our misunderstanding or
 

1129
00:30:29,430 --> 00:30:32,360
throughout about our misunderstanding or
our flawed intuition about what are the

1130
00:30:32,360 --> 00:30:32,370
our flawed intuition about what are the
 

1131
00:30:32,370 --> 00:30:34,160
our flawed intuition about what are the
difficult and what are the easy problems

1132
00:30:34,160 --> 00:30:34,170
difficult and what are the easy problems
 

1133
00:30:34,170 --> 00:30:37,840
difficult and what are the easy problems
in deep learning and the power of

1134
00:30:37,840 --> 00:30:37,850
in deep learning and the power of
 

1135
00:30:37,850 --> 00:30:40,700
in deep learning and the power of
representational learning the ability of

1136
00:30:40,700 --> 00:30:40,710
representational learning the ability of
 

1137
00:30:40,710 --> 00:30:42,500
representational learning the ability of
neural networks to form deeper and

1138
00:30:42,500 --> 00:30:42,510
neural networks to form deeper and
 

1139
00:30:42,510 --> 00:30:44,300
neural networks to form deeper and
deeper representations of the underlying

1140
00:30:44,300 --> 00:30:44,310
deeper representations of the underlying
 

1141
00:30:44,310 --> 00:30:49,310
deeper representations of the underlying
raw data that ultimately forms

1142
00:30:49,310 --> 00:30:49,320

 

1143
00:30:49,320 --> 00:30:51,350

takes complex information that's hard to

1144
00:30:51,350 --> 00:30:51,360
takes complex information that's hard to
 

1145
00:30:51,360 --> 00:30:54,560
takes complex information that's hard to
make sense of and convert it into useful

1146
00:30:54,560 --> 00:30:54,570
make sense of and convert it into useful
 

1147
00:30:54,570 --> 00:31:00,080
make sense of and convert it into useful
actionable knowledge that is from a

1148
00:31:00,080 --> 00:31:00,090
actionable knowledge that is from a
 

1149
00:31:00,090 --> 00:31:04,399
actionable knowledge that is from a
certain lens in a certain a certain lens

1150
00:31:04,399 --> 00:31:04,409
certain lens in a certain a certain lens
 

1151
00:31:04,409 --> 00:31:07,639
certain lens in a certain a certain lens
in a certain problem space can be

1152
00:31:07,639 --> 00:31:07,649
in a certain problem space can be
 

1153
00:31:07,649 --> 00:31:10,700
in a certain problem space can be
clearly defined as understanding of the

1154
00:31:10,700 --> 00:31:10,710
clearly defined as understanding of the
 

1155
00:31:10,710 --> 00:31:13,129
clearly defined as understanding of the
complex information understanding is

1156
00:31:13,129 --> 00:31:13,139
complex information understanding is
 

1157
00:31:13,139 --> 00:31:14,870
complex information understanding is
ultimately taking complex information

1158
00:31:14,870 --> 00:31:14,880
ultimately taking complex information
 

1159
00:31:14,880 --> 00:31:17,659
ultimately taking complex information
and reducing it to its simple essential

1160
00:31:17,659 --> 00:31:17,669
and reducing it to its simple essential
 

1161
00:31:17,669 --> 00:31:21,740
and reducing it to its simple essential
elements representational learning is in

1162
00:31:21,740 --> 00:31:21,750
elements representational learning is in
 

1163
00:31:21,750 --> 00:31:25,519
elements representational learning is in
the trivial case here in drawing having

1164
00:31:25,519 --> 00:31:25,529
the trivial case here in drawing having
 

1165
00:31:25,529 --> 00:31:27,320
the trivial case here in drawing having
to draw a straight line to separate the

1166
00:31:27,320 --> 00:31:27,330
to draw a straight line to separate the
 

1167
00:31:27,330 --> 00:31:29,810
to draw a straight line to separate the
blue and the red curves that's

1168
00:31:29,810 --> 00:31:29,820
blue and the red curves that's
 

1169
00:31:29,820 --> 00:31:32,869
blue and the red curves that's
impossible to do in the in initial input

1170
00:31:32,869 --> 00:31:32,879
impossible to do in the in initial input
 

1171
00:31:32,879 --> 00:31:35,269
impossible to do in the in initial input
space on the Left what the act of

1172
00:31:35,269 --> 00:31:35,279
space on the Left what the act of
 

1173
00:31:35,279 --> 00:31:37,789
space on the Left what the act of
learning is for deep neural networks in

1174
00:31:37,789 --> 00:31:37,799
learning is for deep neural networks in
 

1175
00:31:37,799 --> 00:31:40,310
learning is for deep neural networks in
this formulation is to construct a

1176
00:31:40,310 --> 00:31:40,320
this formulation is to construct a
 

1177
00:31:40,320 --> 00:31:42,289
this formulation is to construct a
topology under which there exists a

1178
00:31:42,289 --> 00:31:42,299
topology under which there exists a
 

1179
00:31:42,299 --> 00:31:44,840
topology under which there exists a
straight line to accurately classify

1180
00:31:44,840 --> 00:31:44,850
straight line to accurately classify
 

1181
00:31:44,850 --> 00:31:48,139
straight line to accurately classify
blue versus red that's the problem and

1182
00:31:48,139 --> 00:31:48,149
blue versus red that's the problem and
 

1183
00:31:48,149 --> 00:31:50,210
blue versus red that's the problem and
for a simple blue and red line

1184
00:31:50,210 --> 00:31:50,220
for a simple blue and red line
 

1185
00:31:50,220 --> 00:31:52,519
for a simple blue and red line
it seems trivial here but this works in

1186
00:31:52,519 --> 00:31:52,529
it seems trivial here but this works in
 

1187
00:31:52,529 --> 00:31:55,009
it seems trivial here but this works in
the general case for arbitrary input

1188
00:31:55,009 --> 00:31:55,019
the general case for arbitrary input
 

1189
00:31:55,019 --> 00:31:57,200
the general case for arbitrary input
spaces for arbitrary nonlinear highly

1190
00:31:57,200 --> 00:31:57,210
spaces for arbitrary nonlinear highly
 

1191
00:31:57,210 --> 00:32:00,980
spaces for arbitrary nonlinear highly
dimensional input spaces and the ability

1192
00:32:00,980 --> 00:32:00,990
dimensional input spaces and the ability
 

1193
00:32:00,990 --> 00:32:04,539
dimensional input spaces and the ability
to automatically learn features to learn

1194
00:32:04,539 --> 00:32:04,549
to automatically learn features to learn
 

1195
00:32:04,549 --> 00:32:08,629
to automatically learn features to learn
hierarchical representations of the raw

1196
00:32:08,629 --> 00:32:08,639
hierarchical representations of the raw
 

1197
00:32:08,639 --> 00:32:10,279
hierarchical representations of the raw
sensory data means that you could do a

1198
00:32:10,279 --> 00:32:10,289
sensory data means that you could do a
 

1199
00:32:10,289 --> 00:32:12,619
sensory data means that you could do a
lot more with data which means you can

1200
00:32:12,619 --> 00:32:12,629
lot more with data which means you can
 

1201
00:32:12,629 --> 00:32:14,749
lot more with data which means you can
expand further and further and further

1202
00:32:14,749 --> 00:32:14,759
expand further and further and further
 

1203
00:32:14,759 --> 00:32:17,560
expand further and further and further
to create intelligent systems that

1204
00:32:17,560 --> 00:32:17,570
to create intelligent systems that
 

1205
00:32:17,570 --> 00:32:19,789
to create intelligent systems that
operate successfully with real-world

1206
00:32:19,789 --> 00:32:19,799
operate successfully with real-world
 

1207
00:32:19,799 --> 00:32:21,680
operate successfully with real-world
data that's what representational

1208
00:32:21,680 --> 00:32:21,690
data that's what representational
 

1209
00:32:21,690 --> 00:32:23,690
data that's what representational
learning means that deep learning allows

1210
00:32:23,690 --> 00:32:23,700
learning means that deep learning allows
 

1211
00:32:23,700 --> 00:32:26,180
learning means that deep learning allows
because the arbitrary number of features

1212
00:32:26,180 --> 00:32:26,190
because the arbitrary number of features
 

1213
00:32:26,190 --> 00:32:28,490
because the arbitrary number of features
that can be automatically determined you

1214
00:32:28,490 --> 00:32:28,500
that can be automatically determined you
 

1215
00:32:28,500 --> 00:32:31,310
that can be automatically determined you
can learn a lot of things about a pretty

1216
00:32:31,310 --> 00:32:31,320
can learn a lot of things about a pretty
 

1217
00:32:31,320 --> 00:32:32,799
can learn a lot of things about a pretty
complex world

1218
00:32:32,799 --> 00:32:32,809
complex world
 

1219
00:32:32,809 --> 00:32:35,419
complex world
unfortunately there needs to be a lot of

1220
00:32:35,419 --> 00:32:35,429
unfortunately there needs to be a lot of
 

1221
00:32:35,429 --> 00:32:37,190
unfortunately there needs to be a lot of
supervised data there still needs to be

1222
00:32:37,190 --> 00:32:37,200
supervised data there still needs to be
 

1223
00:32:37,200 --> 00:32:42,769
supervised data there still needs to be
a lot of human input Andre and others

1224
00:32:42,769 --> 00:32:42,779
a lot of human input Andre and others
 

1225
00:32:42,779 --> 00:32:45,950
a lot of human input Andre and others
Josh will talk about the difference

1226
00:32:45,950 --> 00:32:45,960
Josh will talk about the difference
 

1227
00:32:45,960 --> 00:32:48,529
Josh will talk about the difference
between our human brain our biological

1228
00:32:48,529 --> 00:32:48,539
between our human brain our biological
 

1229
00:32:48,539 --> 00:32:50,480
between our human brain our biological
neural network and the artificial neural

1230
00:32:50,480 --> 00:32:50,490
neural network and the artificial neural
 

1231
00:32:50,490 --> 00:32:54,769
neural network and the artificial neural
network the full human brain with 100

1232
00:32:54,769 --> 00:32:54,779
network the full human brain with 100
 

1233
00:32:54,779 --> 00:32:57,649
network the full human brain with 100
billion neurons 1000 trillion synapses

1234
00:32:57,649 --> 00:32:57,659
billion neurons 1000 trillion synapses
 

1235
00:32:57,659 --> 00:33:01,369
billion neurons 1000 trillion synapses
and the biggest neural networks out

1236
00:33:01,369 --> 00:33:01,379
and the biggest neural networks out
 

1237
00:33:01,379 --> 00:33:03,289
and the biggest neural networks out
there the artificial neural network

1238
00:33:03,289 --> 00:33:03,299
there the artificial neural network
 

1239
00:33:03,299 --> 00:33:07,010
there the artificial neural network
having much smaller 60 million synapses

1240
00:33:07,010 --> 00:33:07,020
having much smaller 60 million synapses
 

1241
00:33:07,020 --> 00:33:10,909
having much smaller 60 million synapses
for ResNet 152 the biggest difference

1242
00:33:10,909 --> 00:33:10,919
for ResNet 152 the biggest difference
 

1243
00:33:10,919 --> 00:33:13,940
for ResNet 152 the biggest difference
the parameter is a human brain being

1244
00:33:13,940 --> 00:33:13,950
the parameter is a human brain being
 

1245
00:33:13,950 --> 00:33:16,399
the parameter is a human brain being
several orders of magnitude more

1246
00:33:16,399 --> 00:33:16,409
several orders of magnitude more
 

1247
00:33:16,409 --> 00:33:19,340
several orders of magnitude more
synapses the topology being much more

1248
00:33:19,340 --> 00:33:19,350
synapses the topology being much more
 

1249
00:33:19,350 --> 00:33:22,909
synapses the topology being much more
complex chaotic the asynchronous nature

1250
00:33:22,909 --> 00:33:22,919
complex chaotic the asynchronous nature
 

1251
00:33:22,919 --> 00:33:24,799
complex chaotic the asynchronous nature
of the human brain and the learning

1252
00:33:24,799 --> 00:33:24,809
of the human brain and the learning
 

1253
00:33:24,809 --> 00:33:27,740
of the human brain and the learning
algorithm of artificial neural networks

1254
00:33:27,740 --> 00:33:27,750
algorithm of artificial neural networks
 

1255
00:33:27,750 --> 00:33:30,649
algorithm of artificial neural networks
is trivial and constrained with

1256
00:33:30,649 --> 00:33:30,659
is trivial and constrained with
 

1257
00:33:30,659 --> 00:33:32,419
is trivial and constrained with
backpropagation is essentially an

1258
00:33:32,419 --> 00:33:32,429
backpropagation is essentially an
 

1259
00:33:32,429 --> 00:33:35,110
backpropagation is essentially an
optimization function over over a

1260
00:33:35,110 --> 00:33:35,120
optimization function over over a
 

1261
00:33:35,120 --> 00:33:37,820
optimization function over over a
clearly defined loss function from the

1262
00:33:37,820 --> 00:33:37,830
clearly defined loss function from the
 

1263
00:33:37,830 --> 00:33:41,299
clearly defined loss function from the
output to the to the input using back

1264
00:33:41,299 --> 00:33:41,309
output to the to the input using back
 

1265
00:33:41,309 --> 00:33:43,880
output to the to the input using back
propagation to teach to adjust the

1266
00:33:43,880 --> 00:33:43,890
propagation to teach to adjust the
 

1267
00:33:43,890 --> 00:33:45,710
propagation to teach to adjust the
weights on that network the learning

1268
00:33:45,710 --> 00:33:45,720
weights on that network the learning
 

1269
00:33:45,720 --> 00:33:49,549
weights on that network the learning
algorithm for our human brain is mostly

1270
00:33:49,549 --> 00:33:49,559
algorithm for our human brain is mostly
 

1271
00:33:49,559 --> 00:33:51,440
algorithm for our human brain is mostly
unknown but it's certainly much more

1272
00:33:51,440 --> 00:33:51,450
unknown but it's certainly much more
 

1273
00:33:51,450 --> 00:33:56,360
unknown but it's certainly much more
complicated than back propagation the

1274
00:33:56,360 --> 00:33:56,370
complicated than back propagation the
 

1275
00:33:56,370 --> 00:33:58,519
complicated than back propagation the
power consumption the human brain is a

1276
00:33:58,519 --> 00:33:58,529
power consumption the human brain is a
 

1277
00:33:58,529 --> 00:34:00,470
power consumption the human brain is a
lot more efficient than artificial

1278
00:34:00,470 --> 00:34:00,480
lot more efficient than artificial
 

1279
00:34:00,480 --> 00:34:03,769
lot more efficient than artificial
neural networks and there's a very kind

1280
00:34:03,769 --> 00:34:03,779
neural networks and there's a very kind
 

1281
00:34:03,779 --> 00:34:08,480
neural networks and there's a very kind
of artificial trivial supervised

1282
00:34:08,480 --> 00:34:08,490
of artificial trivial supervised
 

1283
00:34:08,490 --> 00:34:11,060
of artificial trivial supervised
learning process for training artificial

1284
00:34:11,060 --> 00:34:11,070
learning process for training artificial
 

1285
00:34:11,070 --> 00:34:12,619
learning process for training artificial
neural networks you have to have a

1286
00:34:12,619 --> 00:34:12,629
neural networks you have to have a
 

1287
00:34:12,629 --> 00:34:14,510
neural networks you have to have a
training stage and you have to have an

1288
00:34:14,510 --> 00:34:14,520
training stage and you have to have an
 

1289
00:34:14,520 --> 00:34:17,480
training stage and you have to have an
evaluation stage and once the network is

1290
00:34:17,480 --> 00:34:17,490
evaluation stage and once the network is
 

1291
00:34:17,490 --> 00:34:19,190
evaluation stage and once the network is
trained there's no clear way to continue

1292
00:34:19,190 --> 00:34:19,200
trained there's no clear way to continue
 

1293
00:34:19,200 --> 00:34:21,349
trained there's no clear way to continue
training it or there's a there's a lot

1294
00:34:21,349 --> 00:34:21,359
training it or there's a there's a lot
 

1295
00:34:21,359 --> 00:34:23,240
training it or there's a there's a lot
of ways but they're inefficient it's not

1296
00:34:23,240 --> 00:34:23,250
of ways but they're inefficient it's not
 

1297
00:34:23,250 --> 00:34:27,320
of ways but they're inefficient it's not
designed to do online learning naturally

1298
00:34:27,320 --> 00:34:27,330
designed to do online learning naturally
 

1299
00:34:27,330 --> 00:34:30,379
designed to do online learning naturally
to always be learning is designed to be

1300
00:34:30,379 --> 00:34:30,389
to always be learning is designed to be
 

1301
00:34:30,389 --> 00:34:34,220
to always be learning is designed to be
to learn and then be applied obviously

1302
00:34:34,220 --> 00:34:34,230
to learn and then be applied obviously
 

1303
00:34:34,230 --> 00:34:37,129
to learn and then be applied obviously
our human brains are always learning but

1304
00:34:37,129 --> 00:34:37,139
our human brains are always learning but
 

1305
00:34:37,139 --> 00:34:39,169
our human brains are always learning but
the beautiful fascinating thing is that

1306
00:34:39,169 --> 00:34:39,179
the beautiful fascinating thing is that
 

1307
00:34:39,179 --> 00:34:42,260
the beautiful fascinating thing is that
they're both distributed computation

1308
00:34:42,260 --> 00:34:42,270
they're both distributed computation
 

1309
00:34:42,270 --> 00:34:46,000
they're both distributed computation
systems on a large scale so it's not a

1310
00:34:46,000 --> 00:34:46,010
systems on a large scale so it's not a
 

1311
00:34:46,010 --> 00:34:48,740
systems on a large scale so it's not a
there's it doesn't ultimately boil down

1312
00:34:48,740 --> 00:34:48,750
there's it doesn't ultimately boil down
 

1313
00:34:48,750 --> 00:34:52,490
there's it doesn't ultimately boil down
to a single compute unit the computation

1314
00:34:52,490 --> 00:34:52,500
to a single compute unit the computation
 

1315
00:34:52,500 --> 00:34:54,560
to a single compute unit the computation
is distributed the back propagation

1316
00:34:54,560 --> 00:34:54,570
is distributed the back propagation
 

1317
00:34:54,570 --> 00:34:56,690
is distributed the back propagation
learning process is distributed can be

1318
00:34:56,690 --> 00:34:56,700
learning process is distributed can be
 

1319
00:34:56,700 --> 00:34:59,510
learning process is distributed can be
paralyzed in a GPU massively paralyzed

1320
00:34:59,510 --> 00:34:59,520
paralyzed in a GPU massively paralyzed
 

1321
00:34:59,520 --> 00:35:03,200
paralyzed in a GPU massively paralyzed
the underlying computational unit of a

1322
00:35:03,200 --> 00:35:03,210
the underlying computational unit of a
 

1323
00:35:03,210 --> 00:35:05,210
the underlying computational unit of a
neuron is trivial but can be stacked

1324
00:35:05,210 --> 00:35:05,220
neuron is trivial but can be stacked
 

1325
00:35:05,220 --> 00:35:07,760
neuron is trivial but can be stacked
together to form forward neural networks

1326
00:35:07,760 --> 00:35:07,770
together to form forward neural networks
 

1327
00:35:07,770 --> 00:35:11,570
together to form forward neural networks
recurrent neural networks to represent

1328
00:35:11,570 --> 00:35:11,580
recurrent neural networks to represent
 

1329
00:35:11,580 --> 00:35:14,349
recurrent neural networks to represent
both spatial information with images and

1330
00:35:14,349 --> 00:35:14,359
both spatial information with images and
 

1331
00:35:14,359 --> 00:35:16,790
both spatial information with images and
temporal information we

1332
00:35:16,790 --> 00:35:16,800
temporal information we
 

1333
00:35:16,800 --> 00:35:21,350
temporal information we
the audio speech text sequences of

1334
00:35:21,350 --> 00:35:21,360
the audio speech text sequences of
 

1335
00:35:21,360 --> 00:35:26,090
the audio speech text sequences of
images and video and so on mapping from

1336
00:35:26,090 --> 00:35:26,100
images and video and so on mapping from
 

1337
00:35:26,100 --> 00:35:28,700
images and video and so on mapping from
one-to-one one-to-many many-to-one so

1338
00:35:28,700 --> 00:35:28,710
one-to-one one-to-many many-to-one so
 

1339
00:35:28,710 --> 00:35:31,820
one-to-one one-to-many many-to-one so
the mapping any kind of structure vector

1340
00:35:31,820 --> 00:35:31,830
the mapping any kind of structure vector
 

1341
00:35:31,830 --> 00:35:35,690
the mapping any kind of structure vector
and time data as an input to any kind of

1342
00:35:35,690 --> 00:35:35,700
and time data as an input to any kind of
 

1343
00:35:35,700 --> 00:35:38,470
and time data as an input to any kind of
classification regression sequences

1344
00:35:38,470 --> 00:35:38,480
classification regression sequences
 

1345
00:35:38,480 --> 00:35:41,630
classification regression sequences
captioning video audio as output

1346
00:35:41,630 --> 00:35:41,640
captioning video audio as output
 

1347
00:35:41,640 --> 00:35:45,560
captioning video audio as output
learning in the general sense but in a

1348
00:35:45,560 --> 00:35:45,570
learning in the general sense but in a
 

1349
00:35:45,570 --> 00:35:49,400
learning in the general sense but in a
domain that's precisely defined for the

1350
00:35:49,400 --> 00:35:49,410
domain that's precisely defined for the
 

1351
00:35:49,410 --> 00:35:54,230
domain that's precisely defined for the
supervised training process we can think

1352
00:35:54,230 --> 00:35:54,240
supervised training process we can think
 

1353
00:35:54,240 --> 00:35:57,530
supervised training process we can think
of the in deep learning case you can

1354
00:35:57,530 --> 00:35:57,540
of the in deep learning case you can
 

1355
00:35:57,540 --> 00:36:00,080
of the in deep learning case you can
think of the supervised methods where

1356
00:36:00,080 --> 00:36:00,090
think of the supervised methods where
 

1357
00:36:00,090 --> 00:36:02,230
think of the supervised methods where
humans have to annotate the data as

1358
00:36:02,230 --> 00:36:02,240
humans have to annotate the data as
 

1359
00:36:02,240 --> 00:36:04,760
humans have to annotate the data as
memorization of the data we can think of

1360
00:36:04,760 --> 00:36:04,770
memorization of the data we can think of
 

1361
00:36:04,770 --> 00:36:07,630
memorization of the data we can think of
the exciting new and growing field of

1362
00:36:07,630 --> 00:36:07,640
the exciting new and growing field of
 

1363
00:36:07,640 --> 00:36:09,800
the exciting new and growing field of
semi-supervised learning where most of

1364
00:36:09,800 --> 00:36:09,810
semi-supervised learning where most of
 

1365
00:36:09,810 --> 00:36:12,050
semi-supervised learning where most of
the data through or through generative

1366
00:36:12,050 --> 00:36:12,060
the data through or through generative
 

1367
00:36:12,060 --> 00:36:13,870
the data through or through generative
adversarial networks or through

1368
00:36:13,870 --> 00:36:13,880
adversarial networks or through
 

1369
00:36:13,880 --> 00:36:16,460
adversarial networks or through
significant data augmentation clever

1370
00:36:16,460 --> 00:36:16,470
significant data augmentation clever
 

1371
00:36:16,470 --> 00:36:18,740
significant data augmentation clever
data augmentation most of it is done

1372
00:36:18,740 --> 00:36:18,750
data augmentation most of it is done
 

1373
00:36:18,750 --> 00:36:21,050
data augmentation most of it is done
automatically the annotation process or

1374
00:36:21,050 --> 00:36:21,060
automatically the annotation process or
 

1375
00:36:21,060 --> 00:36:22,790
automatically the annotation process or
through simulation and then

1376
00:36:22,790 --> 00:36:22,800
through simulation and then
 

1377
00:36:22,800 --> 00:36:25,630
through simulation and then
reinforcement learning where most of the

1378
00:36:25,630 --> 00:36:25,640
reinforcement learning where most of the
 

1379
00:36:25,640 --> 00:36:28,280
reinforcement learning where most of the
most of the labels are extremely sparse

1380
00:36:28,280 --> 00:36:28,290
most of the labels are extremely sparse
 

1381
00:36:28,290 --> 00:36:30,890
most of the labels are extremely sparse
and come rarely and so the system has to

1382
00:36:30,890 --> 00:36:30,900
and come rarely and so the system has to
 

1383
00:36:30,900 --> 00:36:32,780
and come rarely and so the system has to
figure out how to operate in the world

1384
00:36:32,780 --> 00:36:32,790
figure out how to operate in the world
 

1385
00:36:32,790 --> 00:36:34,850
figure out how to operate in the world
with very little human input very little

1386
00:36:34,850 --> 00:36:34,860
with very little human input very little
 

1387
00:36:34,860 --> 00:36:39,530
with very little human input very little
human data we can think of that as

1388
00:36:39,530 --> 00:36:39,540
human data we can think of that as
 

1389
00:36:39,540 --> 00:36:41,360
human data we can think of that as
reasoning because you take very little

1390
00:36:41,360 --> 00:36:41,370
reasoning because you take very little
 

1391
00:36:41,370 --> 00:36:43,310
reasoning because you take very little
information from our teachers the humans

1392
00:36:43,310 --> 00:36:43,320
information from our teachers the humans
 

1393
00:36:43,320 --> 00:36:45,860
information from our teachers the humans
and transfer it across generalize it

1394
00:36:45,860 --> 00:36:45,870
and transfer it across generalize it
 

1395
00:36:45,870 --> 00:36:48,560
and transfer it across generalize it
across to reason about the world and

1396
00:36:48,560 --> 00:36:48,570
across to reason about the world and
 

1397
00:36:48,570 --> 00:36:51,230
across to reason about the world and
finally unsupervised learning the

1398
00:36:51,230 --> 00:36:51,240
finally unsupervised learning the
 

1399
00:36:51,240 --> 00:36:53,120
finally unsupervised learning the
excitement of the community that promise

1400
00:36:53,120 --> 00:36:53,130
excitement of the community that promise
 

1401
00:36:53,130 --> 00:36:55,580
excitement of the community that promise
the hope you could think of that as

1402
00:36:55,580 --> 00:36:55,590
the hope you could think of that as
 

1403
00:36:55,590 --> 00:36:57,200
the hope you could think of that as
understanding because ultimately it's

1404
00:36:57,200 --> 00:36:57,210
understanding because ultimately it's
 

1405
00:36:57,210 --> 00:36:59,870
understanding because ultimately it's
taking data with very little or no human

1406
00:36:59,870 --> 00:36:59,880
taking data with very little or no human
 

1407
00:36:59,880 --> 00:37:01,970
taking data with very little or no human
input and forming representations that

1408
00:37:01,970 --> 00:37:01,980
input and forming representations that
 

1409
00:37:01,980 --> 00:37:04,100
input and forming representations that
that data is how we think of

1410
00:37:04,100 --> 00:37:04,110
that data is how we think of
 

1411
00:37:04,110 --> 00:37:07,670
that data is how we think of
understanding requiring making sense of

1412
00:37:07,670 --> 00:37:07,680
understanding requiring making sense of
 

1413
00:37:07,680 --> 00:37:11,270
understanding requiring making sense of
the world without strict input of how to

1414
00:37:11,270 --> 00:37:11,280
the world without strict input of how to
 

1415
00:37:11,280 --> 00:37:14,030
the world without strict input of how to
make sense of the world the kind of

1416
00:37:14,030 --> 00:37:14,040
make sense of the world the kind of
 

1417
00:37:14,040 --> 00:37:18,020
make sense of the world the kind of
process of discovering information maybe

1418
00:37:18,020 --> 00:37:18,030
process of discovering information maybe
 

1419
00:37:18,030 --> 00:37:20,330
process of discovering information maybe
discovering new ideas new ways to

1420
00:37:20,330 --> 00:37:20,340
discovering new ideas new ways to
 

1421
00:37:20,340 --> 00:37:22,400
discovering new ideas new ways to
simplify the world to represent the

1422
00:37:22,400 --> 00:37:22,410
simplify the world to represent the
 

1423
00:37:22,410 --> 00:37:24,560
simplify the world to represent the
world that you can do new things with it

1424
00:37:24,560 --> 00:37:24,570
world that you can do new things with it
 

1425
00:37:24,570 --> 00:37:26,530
world that you can do new things with it
the new is the key element there

1426
00:37:26,530 --> 00:37:26,540
the new is the key element there
 

1427
00:37:26,540 --> 00:37:28,850
the new is the key element there
understanding

1428
00:37:28,850 --> 00:37:28,860
understanding
 

1429
00:37:28,860 --> 00:37:32,930
understanding
and Andre and Ilya and others will talk

1430
00:37:32,930 --> 00:37:32,940
and Andre and Ilya and others will talk
 

1431
00:37:32,940 --> 00:37:35,330
and Andre and Ilya and others will talk
about the certainly the past but the

1432
00:37:35,330 --> 00:37:35,340
about the certainly the past but the
 

1433
00:37:35,340 --> 00:37:37,160
about the certainly the past but the
future of deep learning where is it

1434
00:37:37,160 --> 00:37:37,170
future of deep learning where is it
 

1435
00:37:37,170 --> 00:37:38,000
future of deep learning where is it
going to go

1436
00:37:38,000 --> 00:37:38,010
going to go
 

1437
00:37:38,010 --> 00:37:42,620
going to go
is it overhyped underhyped what is the

1438
00:37:42,620 --> 00:37:42,630
is it overhyped underhyped what is the
 

1439
00:37:42,630 --> 00:37:45,590
is it overhyped underhyped what is the
future will the compute of cpu GPU si

1440
00:37:45,590 --> 00:37:45,600
future will the compute of cpu GPU si
 

1441
00:37:45,600 --> 00:37:48,770
future will the compute of cpu GPU si
Asics continue with the breakthroughs

1442
00:37:48,770 --> 00:37:48,780
Asics continue with the breakthroughs
 

1443
00:37:48,780 --> 00:37:51,350
Asics continue with the breakthroughs
the Moore's law in its various forms of

1444
00:37:51,350 --> 00:37:51,360
the Moore's law in its various forms of
 

1445
00:37:51,360 --> 00:37:53,840
the Moore's law in its various forms of
massive parallelization continue and the

1446
00:37:53,840 --> 00:37:53,850
massive parallelization continue and the
 

1447
00:37:53,850 --> 00:37:56,930
massive parallelization continue and the
large datasets with tens of millions of

1448
00:37:56,930 --> 00:37:56,940
large datasets with tens of millions of
 

1449
00:37:56,940 --> 00:37:59,980
large datasets with tens of millions of
images grow to billions and trillions

1450
00:37:59,980 --> 00:37:59,990
images grow to billions and trillions
 

1451
00:37:59,990 --> 00:38:03,020
images grow to billions and trillions
will the algorithms improve is there a

1452
00:38:03,020 --> 00:38:03,030
will the algorithms improve is there a
 

1453
00:38:03,030 --> 00:38:05,630
will the algorithms improve is there a
groundbreaking idea that's still coming

1454
00:38:05,630 --> 00:38:05,640
groundbreaking idea that's still coming
 

1455
00:38:05,640 --> 00:38:08,390
groundbreaking idea that's still coming
look with Jeff hiddens capsule networks

1456
00:38:08,390 --> 00:38:08,400
look with Jeff hiddens capsule networks
 

1457
00:38:08,400 --> 00:38:10,790
look with Jeff hiddens capsule networks
is there fundamental architectural

1458
00:38:10,790 --> 00:38:10,800
is there fundamental architectural
 

1459
00:38:10,800 --> 00:38:12,800
is there fundamental architectural
changes to neural networks that we can

1460
00:38:12,800 --> 00:38:12,810
changes to neural networks that we can
 

1461
00:38:12,810 --> 00:38:15,110
changes to neural networks that we can
come up with that will change everything

1462
00:38:15,110 --> 00:38:15,120
come up with that will change everything
 

1463
00:38:15,120 --> 00:38:17,480
come up with that will change everything
that will ease the learning process

1464
00:38:17,480 --> 00:38:17,490
that will ease the learning process
 

1465
00:38:17,490 --> 00:38:18,740
that will ease the learning process
they'll make the learning process more

1466
00:38:18,740 --> 00:38:18,750
they'll make the learning process more
 

1467
00:38:18,750 --> 00:38:20,990
they'll make the learning process more
efficient or we'll be able to represent

1468
00:38:20,990 --> 00:38:21,000
efficient or we'll be able to represent
 

1469
00:38:21,000 --> 00:38:24,500
efficient or we'll be able to represent
higher and higher orders of information

1470
00:38:24,500 --> 00:38:24,510
higher and higher orders of information
 

1471
00:38:24,510 --> 00:38:27,890
higher and higher orders of information
such that you can transform knowledge

1472
00:38:27,890 --> 00:38:27,900
such that you can transform knowledge
 

1473
00:38:27,900 --> 00:38:30,620
such that you can transform knowledge
between domains and the software

1474
00:38:30,620 --> 00:38:30,630
between domains and the software
 

1475
00:38:30,630 --> 00:38:33,410
between domains and the software
architectures that support intensive

1476
00:38:33,410 --> 00:38:33,420
architectures that support intensive
 

1477
00:38:33,420 --> 00:38:36,350
architectures that support intensive
florida pi torch i would say last year

1478
00:38:36,350 --> 00:38:36,360
florida pi torch i would say last year
 

1479
00:38:36,360 --> 00:38:38,390
florida pi torch i would say last year
and this year will be the year of deep

1480
00:38:38,390 --> 00:38:38,400
and this year will be the year of deep
 

1481
00:38:38,400 --> 00:38:40,100
and this year will be the year of deep
learning frameworks so those will

1482
00:38:40,100 --> 00:38:40,110
learning frameworks so those will
 

1483
00:38:40,110 --> 00:38:41,810
learning frameworks so those will
certainly keep coming in their various

1484
00:38:41,810 --> 00:38:41,820
certainly keep coming in their various
 

1485
00:38:41,820 --> 00:38:44,360
certainly keep coming in their various
forms and the financial backing is

1486
00:38:44,360 --> 00:38:44,370
forms and the financial backing is
 

1487
00:38:44,370 --> 00:38:49,370
forms and the financial backing is
growing and growing the open challenges

1488
00:38:49,370 --> 00:38:49,380
growing and growing the open challenges
 

1489
00:38:49,380 --> 00:38:52,280
growing and growing the open challenges
for deep learning really a lot of this

1490
00:38:52,280 --> 00:38:52,290
for deep learning really a lot of this
 

1491
00:38:52,290 --> 00:38:54,320
for deep learning really a lot of this
course is kind of connected to deep

1492
00:38:54,320 --> 00:38:54,330
course is kind of connected to deep
 

1493
00:38:54,330 --> 00:38:56,870
course is kind of connected to deep
learning because that's where a lot of

1494
00:38:56,870 --> 00:38:56,880
learning because that's where a lot of
 

1495
00:38:56,880 --> 00:39:00,550
learning because that's where a lot of
the recent breakthroughs that inspire us

1496
00:39:00,550 --> 00:39:00,560
the recent breakthroughs that inspire us
 

1497
00:39:00,560 --> 00:39:03,350
the recent breakthroughs that inspire us
to think about intelligence systems come

1498
00:39:03,350 --> 00:39:03,360
to think about intelligence systems come
 

1499
00:39:03,360 --> 00:39:06,350
to think about intelligence systems come
from but the challenges of many the need

1500
00:39:06,350 --> 00:39:06,360
from but the challenges of many the need
 

1501
00:39:06,360 --> 00:39:07,940
from but the challenges of many the need
the ability to transfer between

1502
00:39:07,940 --> 00:39:07,950
the ability to transfer between
 

1503
00:39:07,950 --> 00:39:10,550
the ability to transfer between
different domains as in reinforcement

1504
00:39:10,550 --> 00:39:10,560
different domains as in reinforcement
 

1505
00:39:10,560 --> 00:39:13,400
different domains as in reinforcement
learning and robotics the need for huge

1506
00:39:13,400 --> 00:39:13,410
learning and robotics the need for huge
 

1507
00:39:13,410 --> 00:39:16,580
learning and robotics the need for huge
data in an official learning that we

1508
00:39:16,580 --> 00:39:16,590
data in an official learning that we
 

1509
00:39:16,590 --> 00:39:20,420
data in an official learning that we
still need supervised data an ability to

1510
00:39:20,420 --> 00:39:20,430
still need supervised data an ability to
 

1511
00:39:20,430 --> 00:39:22,880
still need supervised data an ability to
learn in an unsupervised way is a huge

1512
00:39:22,880 --> 00:39:22,890
learn in an unsupervised way is a huge
 

1513
00:39:22,890 --> 00:39:26,510
learn in an unsupervised way is a huge
problem and not fully automated learning

1514
00:39:26,510 --> 00:39:26,520
problem and not fully automated learning
 

1515
00:39:26,520 --> 00:39:28,880
problem and not fully automated learning
there's still a degree a significant

1516
00:39:28,880 --> 00:39:28,890
there's still a degree a significant
 

1517
00:39:28,890 --> 00:39:31,550
there's still a degree a significant
degree of hyper parameter necessary with

1518
00:39:31,550 --> 00:39:31,560
degree of hyper parameter necessary with
 

1519
00:39:31,560 --> 00:39:33,740
degree of hyper parameter necessary with
the reward functions the loss functions

1520
00:39:33,740 --> 00:39:33,750
the reward functions the loss functions
 

1521
00:39:33,750 --> 00:39:36,560
the reward functions the loss functions
are ultimately defined by humans and

1522
00:39:36,560 --> 00:39:36,570
are ultimately defined by humans and
 

1523
00:39:36,570 --> 00:39:39,770
are ultimately defined by humans and
therefore are deeply flawed when we

1524
00:39:39,770 --> 00:39:39,780
therefore are deeply flawed when we
 

1525
00:39:39,780 --> 00:39:41,990
therefore are deeply flawed when we
release those systems into the real

1526
00:39:41,990 --> 00:39:42,000
release those systems into the real
 

1527
00:39:42,000 --> 00:39:42,410
release those systems into the real
world

1528
00:39:42,410 --> 00:39:42,420
world
 

1529
00:39:42,420 --> 00:39:44,329
world
there is no ground truth for the testing

1530
00:39:44,329 --> 00:39:44,339
there is no ground truth for the testing
 

1531
00:39:44,339 --> 00:39:47,960
there is no ground truth for the testing
set and the goal isn't achieving a class

1532
00:39:47,960 --> 00:39:47,970
set and the goal isn't achieving a class
 

1533
00:39:47,970 --> 00:39:51,410
set and the goal isn't achieving a class
high classification on a trivial image

1534
00:39:51,410 --> 00:39:51,420
high classification on a trivial image
 

1535
00:39:51,420 --> 00:39:53,900
high classification on a trivial image
classification localization detection

1536
00:39:53,900 --> 00:39:53,910
classification localization detection
 

1537
00:39:53,910 --> 00:39:56,420
classification localization detection
problem but rather to have a autonomous

1538
00:39:56,420 --> 00:39:56,430
problem but rather to have a autonomous
 

1539
00:39:56,430 --> 00:40:00,370
problem but rather to have a autonomous
vehicle that doesn't kill pedestrians or

1540
00:40:00,370 --> 00:40:00,380
vehicle that doesn't kill pedestrians or
 

1541
00:40:00,380 --> 00:40:03,609
vehicle that doesn't kill pedestrians or
an industrial robot that operates in

1542
00:40:03,609 --> 00:40:03,619
an industrial robot that operates in
 

1543
00:40:03,619 --> 00:40:06,349
an industrial robot that operates in
jointly with other human beings and all

1544
00:40:06,349 --> 00:40:06,359
jointly with other human beings and all
 

1545
00:40:06,359 --> 00:40:09,109
jointly with other human beings and all
the edge cases that come up how does

1546
00:40:09,109 --> 00:40:09,119
the edge cases that come up how does
 

1547
00:40:09,119 --> 00:40:11,299
the edge cases that come up how does
deep learning methods how do machine

1548
00:40:11,299 --> 00:40:11,309
deep learning methods how do machine
 

1549
00:40:11,309 --> 00:40:12,769
deep learning methods how do machine
learning methods generalize over the

1550
00:40:12,769 --> 00:40:12,779
learning methods generalize over the
 

1551
00:40:12,779 --> 00:40:14,180
learning methods generalize over the
edge cases the weird stuff that happens

1552
00:40:14,180 --> 00:40:14,190
edge cases the weird stuff that happens
 

1553
00:40:14,190 --> 00:40:16,130
edge cases the weird stuff that happens
in the real world those are all the

1554
00:40:16,130 --> 00:40:16,140
in the real world those are all the
 

1555
00:40:16,140 --> 00:40:19,160
in the real world those are all the
problems there Stephen Wolfram will be

1556
00:40:19,160 --> 00:40:19,170
problems there Stephen Wolfram will be
 

1557
00:40:19,170 --> 00:40:23,990
problems there Stephen Wolfram will be
here on Monday evening at 7 p.m. has

1558
00:40:23,990 --> 00:40:24,000
here on Monday evening at 7 p.m. has
 

1559
00:40:24,000 --> 00:40:26,809
here on Monday evening at 7 p.m. has
done a lot of amazing things I would say

1560
00:40:26,809 --> 00:40:26,819
done a lot of amazing things I would say
 

1561
00:40:26,819 --> 00:40:29,870
done a lot of amazing things I would say
is very interesting from his recent

1562
00:40:29,870 --> 00:40:29,880
is very interesting from his recent
 

1563
00:40:29,880 --> 00:40:31,519
is very interesting from his recent
interest in knowledge based programming

1564
00:40:31,519 --> 00:40:31,529
interest in knowledge based programming
 

1565
00:40:31,529 --> 00:40:34,970
interest in knowledge based programming
Wolfram Alpha I think is the fuel for

1566
00:40:34,970 --> 00:40:34,980
Wolfram Alpha I think is the fuel for
 

1567
00:40:34,980 --> 00:40:38,000
Wolfram Alpha I think is the fuel for
most middle school and high school

1568
00:40:38,000 --> 00:40:38,010
most middle school and high school
 

1569
00:40:38,010 --> 00:40:41,089
most middle school and high school
students now for the first time taking

1570
00:40:41,089 --> 00:40:41,099
students now for the first time taking
 

1571
00:40:41,099 --> 00:40:43,309
students now for the first time taking
calculus I pray probably go to Wolfram

1572
00:40:43,309 --> 00:40:43,319
calculus I pray probably go to Wolfram
 

1573
00:40:43,319 --> 00:40:44,990
calculus I pray probably go to Wolfram
Alpha to answer their own questions but

1574
00:40:44,990 --> 00:40:45,000
Alpha to answer their own questions but
 

1575
00:40:45,000 --> 00:40:48,009
Alpha to answer their own questions but
more seriously there is a a deep

1576
00:40:48,009 --> 00:40:48,019
more seriously there is a a deep
 

1577
00:40:48,019 --> 00:40:50,539
more seriously there is a a deep
connected graph of knowledge is being

1578
00:40:50,539 --> 00:40:50,549
connected graph of knowledge is being
 

1579
00:40:50,549 --> 00:40:53,329
connected graph of knowledge is being
built there with the Wolfram or Wolfram

1580
00:40:53,329 --> 00:40:53,339
built there with the Wolfram or Wolfram
 

1581
00:40:53,339 --> 00:40:56,299
built there with the Wolfram or Wolfram
Alpha and Wolfram language that steel

1582
00:40:56,299 --> 00:40:56,309
Alpha and Wolfram language that steel
 

1583
00:40:56,309 --> 00:40:59,480
Alpha and Wolfram language that steel
will explore in terms of language an

1584
00:40:59,480 --> 00:40:59,490
will explore in terms of language an
 

1585
00:40:59,490 --> 00:41:02,660
will explore in terms of language an
interesting thing he was part of the

1586
00:41:02,660 --> 00:41:02,670
interesting thing he was part of the
 

1587
00:41:02,670 --> 00:41:05,809
interesting thing he was part of the
team on arrival that worked on the

1588
00:41:05,809 --> 00:41:05,819
team on arrival that worked on the
 

1589
00:41:05,819 --> 00:41:07,670
team on arrival that worked on the
language if for those of you are

1590
00:41:07,670 --> 00:41:07,680
language if for those of you are
 

1591
00:41:07,680 --> 00:41:10,880
language if for those of you are
familiar the arrival were a alien

1592
00:41:10,880 --> 00:41:10,890
familiar the arrival were a alien
 

1593
00:41:10,890 --> 00:41:15,559
familiar the arrival were a alien
species spoke with us us humans through

1594
00:41:15,559 --> 00:41:15,569
species spoke with us us humans through
 

1595
00:41:15,569 --> 00:41:17,720
species spoke with us us humans through
a very interesting beautiful complicated

1596
00:41:17,720 --> 00:41:17,730
a very interesting beautiful complicated
 

1597
00:41:17,730 --> 00:41:20,269
a very interesting beautiful complicated
language and he was brought in as a

1598
00:41:20,269 --> 00:41:20,279
language and he was brought in as a
 

1599
00:41:20,279 --> 00:41:22,640
language and he was brought in as a
representative human to interpret that

1600
00:41:22,640 --> 00:41:22,650
representative human to interpret that
 

1601
00:41:22,650 --> 00:41:26,089
representative human to interpret that
language just like in the movie he was

1602
00:41:26,089 --> 00:41:26,099
language just like in the movie he was
 

1603
00:41:26,099 --> 00:41:28,700
language just like in the movie he was
represent that in real life and you used

1604
00:41:28,700 --> 00:41:28,710
represent that in real life and you used
 

1605
00:41:28,710 --> 00:41:30,920
represent that in real life and you used
the skills that him and his son

1606
00:41:30,920 --> 00:41:30,930
the skills that him and his son
 

1607
00:41:30,930 --> 00:41:33,319
the skills that him and his son
Christopher used to analyze this

1608
00:41:33,319 --> 00:41:33,329
Christopher used to analyze this
 

1609
00:41:33,329 --> 00:41:35,180
Christopher used to analyze this
language very interesting that process

1610
00:41:35,180 --> 00:41:35,190
language very interesting that process
 

1611
00:41:35,190 --> 00:41:37,220
language very interesting that process
is extremely interesting I hope he talks

1612
00:41:37,220 --> 00:41:37,230
is extremely interesting I hope he talks
 

1613
00:41:37,230 --> 00:41:39,470
is extremely interesting I hope he talks
about it and his background with

1614
00:41:39,470 --> 00:41:39,480
about it and his background with
 

1615
00:41:39,480 --> 00:41:44,480
about it and his background with
Mathematica and new kind of science the

1616
00:41:44,480 --> 00:41:44,490
Mathematica and new kind of science the
 

1617
00:41:44,490 --> 00:41:49,730
Mathematica and new kind of science the
sort of another set of ideas that have

1618
00:41:49,730 --> 00:41:49,740
sort of another set of ideas that have
 

1619
00:41:49,740 --> 00:41:54,920
sort of another set of ideas that have
inspired people in terms of creating

1620
00:41:54,920 --> 00:41:54,930
inspired people in terms of creating
 

1621
00:41:54,930 --> 00:41:59,890
inspired people in terms of creating
intelligence systems is the idea that

1622
00:41:59,890 --> 00:41:59,900
intelligence systems is the idea that
 

1623
00:41:59,900 --> 00:42:04,400
intelligence systems is the idea that
from very simple things were very simple

1624
00:42:04,400 --> 00:42:04,410
from very simple things were very simple
 

1625
00:42:04,410 --> 00:42:07,730
from very simple things were very simple
rules extremely complex patterns can

1626
00:42:07,730 --> 00:42:07,740
rules extremely complex patterns can
 

1627
00:42:07,740 --> 00:42:10,490
rules extremely complex patterns can
emerge his work was cellular automata

1628
00:42:10,490 --> 00:42:10,500
emerge his work was cellular automata
 

1629
00:42:10,500 --> 00:42:14,440
emerge his work was cellular automata
did just that taking extremely simple

1630
00:42:14,440 --> 00:42:14,450
did just that taking extremely simple
 

1631
00:42:14,450 --> 00:42:17,809
did just that taking extremely simple
mathematical constructs here with

1632
00:42:17,809 --> 00:42:17,819
mathematical constructs here with
 

1633
00:42:17,819 --> 00:42:19,880
mathematical constructs here with
cellular automata these are these are

1634
00:42:19,880 --> 00:42:19,890
cellular automata these are these are
 

1635
00:42:19,890 --> 00:42:23,660
cellular automata these are these are
grids of computational units that switch

1636
00:42:23,660 --> 00:42:23,670
grids of computational units that switch
 

1637
00:42:23,670 --> 00:42:25,849
grids of computational units that switch
on and off in some kind of a predefined

1638
00:42:25,849 --> 00:42:25,859
on and off in some kind of a predefined
 

1639
00:42:25,859 --> 00:42:28,220
on and off in some kind of a predefined
way and only operate locally based on

1640
00:42:28,220 --> 00:42:28,230
way and only operate locally based on
 

1641
00:42:28,230 --> 00:42:30,589
way and only operate locally based on
their local neighborhood and somehow

1642
00:42:30,589 --> 00:42:30,599
their local neighborhood and somehow
 

1643
00:42:30,599 --> 00:42:32,180
their local neighborhood and somehow
based on different kinds of rules

1644
00:42:32,180 --> 00:42:32,190
based on different kinds of rules
 

1645
00:42:32,190 --> 00:42:34,460
based on different kinds of rules
different patterns emerge here's a three

1646
00:42:34,460 --> 00:42:34,470
different patterns emerge here's a three
 

1647
00:42:34,470 --> 00:42:36,230
different patterns emerge here's a three
dimensional cellular automata with a

1648
00:42:36,230 --> 00:42:36,240
dimensional cellular automata with a
 

1649
00:42:36,240 --> 00:42:38,329
dimensional cellular automata with a
simple rule starting with nothing with a

1650
00:42:38,329 --> 00:42:38,339
simple rule starting with nothing with a
 

1651
00:42:38,339 --> 00:42:40,579
simple rule starting with nothing with a
single cell they grow in really

1652
00:42:40,579 --> 00:42:40,589
single cell they grow in really
 

1653
00:42:40,589 --> 00:42:43,250
single cell they grow in really
interesting complex ways this emergent

1654
00:42:43,250 --> 00:42:43,260
interesting complex ways this emergent
 

1655
00:42:43,260 --> 00:42:46,010
interesting complex ways this emergent
complexity is inspiring it's the same

1656
00:42:46,010 --> 00:42:46,020
complexity is inspiring it's the same
 

1657
00:42:46,020 --> 00:42:48,589
complexity is inspiring it's the same
kind of thing that inspires us about

1658
00:42:48,589 --> 00:42:48,599
kind of thing that inspires us about
 

1659
00:42:48,599 --> 00:42:50,329
kind of thing that inspires us about
neural networks that you can take a

1660
00:42:50,329 --> 00:42:50,339
neural networks that you can take a
 

1661
00:42:50,339 --> 00:42:52,520
neural networks that you can take a
simple computational unit and when

1662
00:42:52,520 --> 00:42:52,530
simple computational unit and when
 

1663
00:42:52,530 --> 00:42:54,589
simple computational unit and when
combined together in arbitrary ways can

1664
00:42:54,589 --> 00:42:54,599
combined together in arbitrary ways can
 

1665
00:42:54,599 --> 00:42:58,579
combined together in arbitrary ways can
form complex representations that's also

1666
00:42:58,579 --> 00:42:58,589
form complex representations that's also
 

1667
00:42:58,589 --> 00:43:00,589
form complex representations that's also
very interesting you can see knowledge

1668
00:43:00,589 --> 00:43:00,599
very interesting you can see knowledge
 

1669
00:43:00,599 --> 00:43:02,359
very interesting you can see knowledge
from a knowledge perspective you can see

1670
00:43:02,359 --> 00:43:02,369
from a knowledge perspective you can see
 

1671
00:43:02,369 --> 00:43:04,099
from a knowledge perspective you can see
knowledge formation in the same kind of

1672
00:43:04,099 --> 00:43:04,109
knowledge formation in the same kind of
 

1673
00:43:04,109 --> 00:43:08,539
knowledge formation in the same kind of
way simplicity at a mass distributed

1674
00:43:08,539 --> 00:43:08,549
way simplicity at a mass distributed
 

1675
00:43:08,549 --> 00:43:13,099
way simplicity at a mass distributed
scale resulting in complexity next

1676
00:43:13,099 --> 00:43:13,109
scale resulting in complexity next
 

1677
00:43:13,109 --> 00:43:13,780
scale resulting in complexity next
Tuesday

1678
00:43:13,780 --> 00:43:13,790
Tuesday
 

1679
00:43:13,790 --> 00:43:17,510
Tuesday
Richard Moyes from article 36 coming all

1680
00:43:17,510 --> 00:43:17,520
Richard Moyes from article 36 coming all
 

1681
00:43:17,520 --> 00:43:20,299
Richard Moyes from article 36 coming all
the way from UK for us we'll talk about

1682
00:43:20,299 --> 00:43:20,309
the way from UK for us we'll talk about
 

1683
00:43:20,309 --> 00:43:23,359
the way from UK for us we'll talk about
it works with autonomous weapons systems

1684
00:43:23,359 --> 00:43:23,369
it works with autonomous weapons systems
 

1685
00:43:23,369 --> 00:43:27,140
it works with autonomous weapons systems
works with also nuclear weapons but

1686
00:43:27,140 --> 00:43:27,150
works with also nuclear weapons but
 

1687
00:43:27,150 --> 00:43:29,530
works with also nuclear weapons but
primarily autonomous weapon systems and

1688
00:43:29,530 --> 00:43:29,540
primarily autonomous weapon systems and
 

1689
00:43:29,540 --> 00:43:33,740
primarily autonomous weapon systems and
concern legal policy and technological

1690
00:43:33,740 --> 00:43:33,750
concern legal policy and technological
 

1691
00:43:33,750 --> 00:43:36,140
concern legal policy and technological
aspects of banning these weapons there's

1692
00:43:36,140 --> 00:43:36,150
aspects of banning these weapons there's
 

1693
00:43:36,150 --> 00:43:38,839
aspects of banning these weapons there's
been a lot of agreement about the safety

1694
00:43:38,839 --> 00:43:38,849
been a lot of agreement about the safety
 

1695
00:43:38,849 --> 00:43:41,720
been a lot of agreement about the safety
hazards of autonomous systems that make

1696
00:43:41,720 --> 00:43:41,730
hazards of autonomous systems that make
 

1697
00:43:41,730 --> 00:43:44,740
hazards of autonomous systems that make
decisions to kill a human being

1698
00:43:44,740 --> 00:43:44,750
decisions to kill a human being
 

1699
00:43:44,750 --> 00:43:49,059
decisions to kill a human being
mark Robert CEO of Boston Dynamics

1700
00:43:49,059 --> 00:43:49,069
mark Robert CEO of Boston Dynamics
 

1701
00:43:49,069 --> 00:43:52,579
mark Robert CEO of Boston Dynamics
previously a long time ago faculty here

1702
00:43:52,579 --> 00:43:52,589
previously a long time ago faculty here
 

1703
00:43:52,589 --> 00:43:55,730
previously a long time ago faculty here
at MIT will talk about will bring robots

1704
00:43:55,730 --> 00:43:55,740
at MIT will talk about will bring robots
 

1705
00:43:55,740 --> 00:43:59,930
at MIT will talk about will bring robots
and talk to us about his work of robots

1706
00:43:59,930 --> 00:43:59,940
and talk to us about his work of robots
 

1707
00:43:59,940 --> 00:44:02,960
and talk to us about his work of robots
in the real world as doing a lot of

1708
00:44:02,960 --> 00:44:02,970
in the real world as doing a lot of
 

1709
00:44:02,970 --> 00:44:04,400
in the real world as doing a lot of
exciting stuff with humanoid robotics

1710
00:44:04,400 --> 00:44:04,410
exciting stuff with humanoid robotics
 

1711
00:44:04,410 --> 00:44:07,670
exciting stuff with humanoid robotics
and any kind of robots operating on legs

1712
00:44:07,670 --> 00:44:07,680
and any kind of robots operating on legs
 

1713
00:44:07,680 --> 00:44:08,780
and any kind of robots operating on legs
it's incredible

1714
00:44:08,780 --> 00:44:08,790
it's incredible
 

1715
00:44:08,790 --> 00:44:11,330
it's incredible
work extremely exciting and gets to

1716
00:44:11,330 --> 00:44:11,340
work extremely exciting and gets to
 

1717
00:44:11,340 --> 00:44:13,340
work extremely exciting and gets to
explore the idea of how difficult it is

1718
00:44:13,340 --> 00:44:13,350
explore the idea of how difficult it is
 

1719
00:44:13,350 --> 00:44:15,170
explore the idea of how difficult it is
to build these robot systems that

1720
00:44:15,170 --> 00:44:15,180
to build these robot systems that
 

1721
00:44:15,180 --> 00:44:19,810
to build these robot systems that
operate in the real world from both the

1722
00:44:19,810 --> 00:44:19,820
operate in the real world from both the
 

1723
00:44:19,820 --> 00:44:25,520
operate in the real world from both the
control aspect and from the way the

1724
00:44:25,520 --> 00:44:25,530
control aspect and from the way the
 

1725
00:44:25,530 --> 00:44:27,700
control aspect and from the way the
final result is perceived by our society

1726
00:44:27,700 --> 00:44:27,710
final result is perceived by our society
 

1727
00:44:27,710 --> 00:44:30,530
final result is perceived by our society
it's very interesting to see when

1728
00:44:30,530 --> 00:44:30,540
it's very interesting to see when
 

1729
00:44:30,540 --> 00:44:33,770
it's very interesting to see when
intelligence in robotics is embodied and

1730
00:44:33,770 --> 00:44:33,780
intelligence in robotics is embodied and
 

1731
00:44:33,780 --> 00:44:36,620
intelligence in robotics is embodied and
then taking in by us and what that

1732
00:44:36,620 --> 00:44:36,630
then taking in by us and what that
 

1733
00:44:36,630 --> 00:44:41,170
then taking in by us and what that
inspires fear excitement hope concern

1734
00:44:41,170 --> 00:44:41,180
inspires fear excitement hope concern
 

1735
00:44:41,180 --> 00:44:46,840
inspires fear excitement hope concern
and all the above Ilya sutskever is

1736
00:44:46,840 --> 00:44:46,850
and all the above Ilya sutskever is
 

1737
00:44:46,850 --> 00:44:49,340
and all the above Ilya sutskever is
expert in many aspects of machine

1738
00:44:49,340 --> 00:44:49,350
expert in many aspects of machine
 

1739
00:44:49,350 --> 00:44:49,910
expert in many aspects of machine
learning

1740
00:44:49,910 --> 00:44:49,920
learning
 

1741
00:44:49,920 --> 00:44:53,720
learning
he is the co-founder of open AI I'll

1742
00:44:53,720 --> 00:44:53,730
he is the co-founder of open AI I'll
 

1743
00:44:53,730 --> 00:44:57,800
he is the co-founder of open AI I'll
talk about their different aspects of

1744
00:44:57,800 --> 00:44:57,810
talk about their different aspects of
 

1745
00:44:57,810 --> 00:44:59,510
talk about their different aspects of
game playing that they've recently been

1746
00:44:59,510 --> 00:44:59,520
game playing that they've recently been
 

1747
00:44:59,520 --> 00:45:02,090
game playing that they've recently been
exploring about using deeper enforcement

1748
00:45:02,090 --> 00:45:02,100
exploring about using deeper enforcement
 

1749
00:45:02,100 --> 00:45:07,490
exploring about using deeper enforcement
learning to play our K games in D on the

1750
00:45:07,490 --> 00:45:07,500
learning to play our K games in D on the
 

1751
00:45:07,500 --> 00:45:10,040
learning to play our K games in D on the
deep mind side using deep reinforcement

1752
00:45:10,040 --> 00:45:10,050
deep mind side using deep reinforcement
 

1753
00:45:10,050 --> 00:45:11,780
deep mind side using deep reinforcement
learning to beat the best in the world

1754
00:45:11,780 --> 00:45:11,790
learning to beat the best in the world
 

1755
00:45:11,790 --> 00:45:16,210
learning to beat the best in the world
that the game of go in 2017 the big

1756
00:45:16,210 --> 00:45:16,220
that the game of go in 2017 the big
 

1757
00:45:16,220 --> 00:45:18,200
that the game of go in 2017 the big
fascinating breakthrough achieved by

1758
00:45:18,200 --> 00:45:18,210
fascinating breakthrough achieved by
 

1759
00:45:18,210 --> 00:45:20,720
fascinating breakthrough achieved by
that team with alphago zero training an

1760
00:45:20,720 --> 00:45:20,730
that team with alphago zero training an
 

1761
00:45:20,730 --> 00:45:22,610
that team with alphago zero training an
agent that through self play playing

1762
00:45:22,610 --> 00:45:22,620
agent that through self play playing
 

1763
00:45:22,620 --> 00:45:25,940
agent that through self play playing
itself not on expert games so truly from

1764
00:45:25,940 --> 00:45:25,950
itself not on expert games so truly from
 

1765
00:45:25,950 --> 00:45:28,100
itself not on expert games so truly from
scratch learning to beat the best in the

1766
00:45:28,100 --> 00:45:28,110
scratch learning to beat the best in the
 

1767
00:45:28,110 --> 00:45:30,350
scratch learning to beat the best in the
world including the previous iteration

1768
00:45:30,350 --> 00:45:30,360
world including the previous iteration
 

1769
00:45:30,360 --> 00:45:31,100
world including the previous iteration
of alphago

1770
00:45:31,100 --> 00:45:31,110
of alphago
 

1771
00:45:31,110 --> 00:45:34,330
of alphago
we will explore what aspects of the

1772
00:45:34,330 --> 00:45:34,340
we will explore what aspects of the
 

1773
00:45:34,340 --> 00:45:37,790
we will explore what aspects of the
stack of intelligent robotic systems

1774
00:45:37,790 --> 00:45:37,800
stack of intelligent robotic systems
 

1775
00:45:37,800 --> 00:45:39,800
stack of intelligent robotic systems
intelligent agents can be learned in

1776
00:45:39,800 --> 00:45:39,810
intelligent agents can be learned in
 

1777
00:45:39,810 --> 00:45:42,440
intelligent agents can be learned in
this way so deep learning the

1778
00:45:42,440 --> 00:45:42,450
this way so deep learning the
 

1779
00:45:42,450 --> 00:45:44,260
this way so deep learning the
memorization the supervised learning

1780
00:45:44,260 --> 00:45:44,270
memorization the supervised learning
 

1781
00:45:44,270 --> 00:45:47,770
memorization the supervised learning
memorization approach it looks at the

1782
00:45:47,770 --> 00:45:47,780
memorization approach it looks at the
 

1783
00:45:47,780 --> 00:45:50,090
memorization approach it looks at the
sensor data feature extraction

1784
00:45:50,090 --> 00:45:50,100
sensor data feature extraction
 

1785
00:45:50,100 --> 00:45:52,030
sensor data feature extraction
representation learning aspect of this

1786
00:45:52,030 --> 00:45:52,040
representation learning aspect of this
 

1787
00:45:52,040 --> 00:45:54,710
representation learning aspect of this
taking the sensor data from camera light

1788
00:45:54,710 --> 00:45:54,720
taking the sensor data from camera light
 

1789
00:45:54,720 --> 00:45:58,270
taking the sensor data from camera light
our audio extracting the features

1790
00:45:58,270 --> 00:45:58,280
our audio extracting the features
 

1791
00:45:58,280 --> 00:46:00,770
our audio extracting the features
forming higher-order representations and

1792
00:46:00,770 --> 00:46:00,780
forming higher-order representations and
 

1793
00:46:00,780 --> 00:46:02,480
forming higher-order representations and
on those representations learning to

1794
00:46:02,480 --> 00:46:02,490
on those representations learning to
 

1795
00:46:02,490 --> 00:46:04,180
on those representations learning to
actually accomplish some kind of

1796
00:46:04,180 --> 00:46:04,190
actually accomplish some kind of
 

1797
00:46:04,190 --> 00:46:07,250
actually accomplish some kind of
classification regression task figuring

1798
00:46:07,250 --> 00:46:07,260
classification regression task figuring
 

1799
00:46:07,260 --> 00:46:09,830
classification regression task figuring
out based on the representation what is

1800
00:46:09,830 --> 00:46:09,840
out based on the representation what is
 

1801
00:46:09,840 --> 00:46:11,990
out based on the representation what is
going on in the raw sensory data and

1802
00:46:11,990 --> 00:46:12,000
going on in the raw sensory data and
 

1803
00:46:12,000 --> 00:46:14,270
going on in the raw sensory data and
then combining that data together to

1804
00:46:14,270 --> 00:46:14,280
then combining that data together to
 

1805
00:46:14,280 --> 00:46:17,600
then combining that data together to
reason about it and finally in the

1806
00:46:17,600 --> 00:46:17,610
reason about it and finally in the
 

1807
00:46:17,610 --> 00:46:19,460
reason about it and finally in the
robotic domains taking it all together

1808
00:46:19,460 --> 00:46:19,470
robotic domains taking it all together
 

1809
00:46:19,470 --> 00:46:22,220
robotic domains taking it all together
as with human

1810
00:46:22,220 --> 00:46:22,230
as with human
 

1811
00:46:22,230 --> 00:46:25,350
as with human
industrial robotics autonomous vehicles

1812
00:46:25,350 --> 00:46:25,360
industrial robotics autonomous vehicles
 

1813
00:46:25,360 --> 00:46:27,240
industrial robotics autonomous vehicles
taking all together and actually acting

1814
00:46:27,240 --> 00:46:27,250
taking all together and actually acting
 

1815
00:46:27,250 --> 00:46:30,810
taking all together and actually acting
in this world with the effectors and the

1816
00:46:30,810 --> 00:46:30,820
in this world with the effectors and the
 

1817
00:46:30,820 --> 00:46:32,760
in this world with the effectors and the
open question is how much of this AI

1818
00:46:32,760 --> 00:46:32,770
open question is how much of this AI
 

1819
00:46:32,770 --> 00:46:36,510
open question is how much of this AI
stack can be learned that's something

1820
00:46:36,510 --> 00:46:36,520
stack can be learned that's something
 

1821
00:46:36,520 --> 00:46:40,320
stack can be learned that's something
for us to discuss to think about that a

1822
00:46:40,320 --> 00:46:40,330
for us to discuss to think about that a
 

1823
00:46:40,330 --> 00:46:41,610
for us to discuss to think about that a
Leo will touch on with deeper

1824
00:46:41,610 --> 00:46:41,620
Leo will touch on with deeper
 

1825
00:46:41,620 --> 00:46:43,410
Leo will touch on with deeper
enforcement learning we can certainly

1826
00:46:43,410 --> 00:46:43,420
enforcement learning we can certainly
 

1827
00:46:43,420 --> 00:46:45,720
enforcement learning we can certainly
learn representations and perform

1828
00:46:45,720 --> 00:46:45,730
learn representations and perform
 

1829
00:46:45,730 --> 00:46:47,460
learn representations and perform
classifications state-of-the-art better

1830
00:46:47,460 --> 00:46:47,470
classifications state-of-the-art better
 

1831
00:46:47,470 --> 00:46:48,890
classifications state-of-the-art better
than human and image classification

1832
00:46:48,890 --> 00:46:48,900
than human and image classification
 

1833
00:46:48,900 --> 00:46:54,780
than human and image classification
imagenet and segmentation tasks and the

1834
00:46:54,780 --> 00:46:54,790
imagenet and segmentation tasks and the
 

1835
00:46:54,790 --> 00:46:56,190
imagenet and segmentation tasks and the
excitement of deep learning is what's

1836
00:46:56,190 --> 00:46:56,200
excitement of deep learning is what's
 

1837
00:46:56,200 --> 00:46:57,660
excitement of deep learning is what's
highlighted there in the red box can be

1838
00:46:57,660 --> 00:46:57,670
highlighted there in the red box can be
 

1839
00:46:57,670 --> 00:46:58,680
highlighted there in the red box can be
done end to ends

1840
00:46:58,680 --> 00:46:58,690
done end to ends
 

1841
00:46:58,690 --> 00:47:00,990
done end to ends
raw sensory data out to the knowledge to

1842
00:47:00,990 --> 00:47:01,000
raw sensory data out to the knowledge to
 

1843
00:47:01,000 --> 00:47:03,510
raw sensory data out to the knowledge to
the output to the classification can we

1844
00:47:03,510 --> 00:47:03,520
the output to the classification can we
 

1845
00:47:03,520 --> 00:47:05,400
the output to the classification can we
begin to reason is the open question

1846
00:47:05,400 --> 00:47:05,410
begin to reason is the open question
 

1847
00:47:05,410 --> 00:47:08,250
begin to reason is the open question
with the knowledge based programming

1848
00:47:08,250 --> 00:47:08,260
with the knowledge based programming
 

1849
00:47:08,260 --> 00:47:10,140
with the knowledge based programming
that Stephen Wolfram will talk about can

1850
00:47:10,140 --> 00:47:10,150
that Stephen Wolfram will talk about can
 

1851
00:47:10,150 --> 00:47:13,050
that Stephen Wolfram will talk about can
we begin to take these automatically

1852
00:47:13,050 --> 00:47:13,060
we begin to take these automatically
 

1853
00:47:13,060 --> 00:47:15,930
we begin to take these automatically
generated high order representations and

1854
00:47:15,930 --> 00:47:15,940
generated high order representations and
 

1855
00:47:15,940 --> 00:47:18,510
generated high order representations and
combine them together to form knowledge

1856
00:47:18,510 --> 00:47:18,520
combine them together to form knowledge
 

1857
00:47:18,520 --> 00:47:22,800
combine them together to form knowledge
bases to form aggregate graphs of ideas

1858
00:47:22,800 --> 00:47:22,810
bases to form aggregate graphs of ideas
 

1859
00:47:22,810 --> 00:47:27,060
bases to form aggregate graphs of ideas
that can then be used to reason and can

1860
00:47:27,060 --> 00:47:27,070
that can then be used to reason and can
 

1861
00:47:27,070 --> 00:47:30,840
that can then be used to reason and can
we then combine them together to act in

1862
00:47:30,840 --> 00:47:30,850
we then combine them together to act in
 

1863
00:47:30,850 --> 00:47:33,330
we then combine them together to act in
the world for whether in simulation with

1864
00:47:33,330 --> 00:47:33,340
the world for whether in simulation with
 

1865
00:47:33,340 --> 00:47:36,780
the world for whether in simulation with
arcade games or simulation of autonomous

1866
00:47:36,780 --> 00:47:36,790
arcade games or simulation of autonomous
 

1867
00:47:36,790 --> 00:47:38,520
arcade games or simulation of autonomous
vehicles or biotic systems or actually

1868
00:47:38,520 --> 00:47:38,530
vehicles or biotic systems or actually
 

1869
00:47:38,530 --> 00:47:40,350
vehicles or biotic systems or actually
in the physical world with robots moving

1870
00:47:40,350 --> 00:47:40,360
in the physical world with robots moving
 

1871
00:47:40,360 --> 00:47:43,500
in the physical world with robots moving
about can that end end from raw sensory

1872
00:47:43,500 --> 00:47:43,510
about can that end end from raw sensory
 

1873
00:47:43,510 --> 00:47:47,490
about can that end end from raw sensory
data to action be learned that's the

1874
00:47:47,490 --> 00:47:47,500
data to action be learned that's the
 

1875
00:47:47,500 --> 00:47:50,670
data to action be learned that's the
open question for for artificial general

1876
00:47:50,670 --> 00:47:50,680
open question for for artificial general
 

1877
00:47:50,680 --> 00:47:53,700
open question for for artificial general
intelligence for this class can this

1878
00:47:53,700 --> 00:47:53,710
intelligence for this class can this
 

1879
00:47:53,710 --> 00:47:58,470
intelligence for this class can this
entire process be end to end can we

1880
00:47:58,470 --> 00:47:58,480
entire process be end to end can we
 

1881
00:47:58,480 --> 00:48:01,620
entire process be end to end can we
build systems and how do we do it that

1882
00:48:01,620 --> 00:48:01,630
build systems and how do we do it that
 

1883
00:48:01,630 --> 00:48:03,720
build systems and how do we do it that
achieve this process end to end in the

1884
00:48:03,720 --> 00:48:03,730
achieve this process end to end in the
 

1885
00:48:03,730 --> 00:48:06,090
achieve this process end to end in the
same way that humans do we're born in

1886
00:48:06,090 --> 00:48:06,100
same way that humans do we're born in
 

1887
00:48:06,100 --> 00:48:08,250
same way that humans do we're born in
this raw sensory environment taking in

1888
00:48:08,250 --> 00:48:08,260
this raw sensory environment taking in
 

1889
00:48:08,260 --> 00:48:11,460
this raw sensory environment taking in
very little information and learn to

1890
00:48:11,460 --> 00:48:11,470
very little information and learn to
 

1891
00:48:11,470 --> 00:48:15,800
very little information and learn to
operate successfully in arbitrary

1892
00:48:15,800 --> 00:48:15,810
operate successfully in arbitrary
 

1893
00:48:15,810 --> 00:48:20,850
operate successfully in arbitrary
constraints arbitrary goals and to do so

1894
00:48:20,850 --> 00:48:20,860
constraints arbitrary goals and to do so
 

1895
00:48:20,860 --> 00:48:24,780
constraints arbitrary goals and to do so
we have lectures we have three projects

1896
00:48:24,780 --> 00:48:24,790
we have lectures we have three projects
 

1897
00:48:24,790 --> 00:48:28,080
we have lectures we have three projects
and we have guest speakers from various

1898
00:48:28,080 --> 00:48:28,090
and we have guest speakers from various
 

1899
00:48:28,090 --> 00:48:31,590
and we have guest speakers from various
disciplines I hope that all these voices

1900
00:48:31,590 --> 00:48:31,600
disciplines I hope that all these voices
 

1901
00:48:31,600 --> 00:48:34,640
disciplines I hope that all these voices
will be heard and will feed a

1902
00:48:34,640 --> 00:48:34,650
will be heard and will feed a
 

1903
00:48:34,650 --> 00:48:35,970
will be heard and will feed a
conversation

1904
00:48:35,970 --> 00:48:35,980
conversation
 

1905
00:48:35,980 --> 00:48:39,480
conversation
artificial intelligence and it's

1906
00:48:39,480 --> 00:48:39,490
artificial intelligence and it's
 

1907
00:48:39,490 --> 00:48:43,530
artificial intelligence and it's
positive and it's concerning effects in

1908
00:48:43,530 --> 00:48:43,540
positive and it's concerning effects in
 

1909
00:48:43,540 --> 00:48:45,750
positive and it's concerning effects in
society and how do we move forward from

1910
00:48:45,750 --> 00:48:45,760
society and how do we move forward from
 

1911
00:48:45,760 --> 00:48:48,810
society and how do we move forward from
an engineering approach the topics will

1912
00:48:48,810 --> 00:48:48,820
an engineering approach the topics will
 

1913
00:48:48,820 --> 00:48:50,430
an engineering approach the topics will
be deep learning deep reinforcement

1914
00:48:50,430 --> 00:48:50,440
be deep learning deep reinforcement
 

1915
00:48:50,440 --> 00:48:52,650
be deep learning deep reinforcement
learning cognitive modeling

1916
00:48:52,650 --> 00:48:52,660
learning cognitive modeling
 

1917
00:48:52,660 --> 00:48:54,900
learning cognitive modeling
computational cognitive science emotion

1918
00:48:54,900 --> 00:48:54,910
computational cognitive science emotion
 

1919
00:48:54,910 --> 00:48:57,120
computational cognitive science emotion
creation knowledge based programming AI

1920
00:48:57,120 --> 00:48:57,130
creation knowledge based programming AI
 

1921
00:48:57,130 --> 00:48:59,220
creation knowledge based programming AI
safety with autonomous weapon systems

1922
00:48:59,220 --> 00:48:59,230
safety with autonomous weapon systems
 

1923
00:48:59,230 --> 00:49:01,710
safety with autonomous weapon systems
and personal robotics with human

1924
00:49:01,710 --> 00:49:01,720
and personal robotics with human
 

1925
00:49:01,720 --> 00:49:03,450
and personal robotics with human
centered artificial intelligence that's

1926
00:49:03,450 --> 00:49:03,460
centered artificial intelligence that's
 

1927
00:49:03,460 --> 00:49:04,859
centered artificial intelligence that's
for the first two weeks of this class

1928
00:49:04,859 --> 00:49:04,869
for the first two weeks of this class
 

1929
00:49:04,869 --> 00:49:07,800
for the first two weeks of this class
that's the the part where if you're

1930
00:49:07,800 --> 00:49:07,810
that's the the part where if you're
 

1931
00:49:07,810 --> 00:49:09,270
that's the the part where if you're
actually registered students that's

1932
00:49:09,270 --> 00:49:09,280
actually registered students that's
 

1933
00:49:09,280 --> 00:49:10,560
actually registered students that's
where you need to submit the project

1934
00:49:10,560 --> 00:49:10,570
where you need to submit the project
 

1935
00:49:10,570 --> 00:49:13,560
where you need to submit the project
that's when we all meet here every every

1936
00:49:13,560 --> 00:49:13,570
that's when we all meet here every every
 

1937
00:49:13,570 --> 00:49:15,960
that's when we all meet here every every
night with the incredible speakers but

1938
00:49:15,960 --> 00:49:15,970
night with the incredible speakers but
 

1939
00:49:15,970 --> 00:49:17,849
night with the incredible speakers but
this will continue we already have

1940
00:49:17,849 --> 00:49:17,859
this will continue we already have
 

1941
00:49:17,859 --> 00:49:19,290
this will continue we already have
several speakers scheduled in the next

1942
00:49:19,290 --> 00:49:19,300
several speakers scheduled in the next
 

1943
00:49:19,300 --> 00:49:22,349
several speakers scheduled in the next
couple of months yet to be announced but

1944
00:49:22,349 --> 00:49:22,359
couple of months yet to be announced but
 

1945
00:49:22,359 --> 00:49:24,690
couple of months yet to be announced but
they're incredible and we have

1946
00:49:24,690 --> 00:49:24,700
they're incredible and we have
 

1947
00:49:24,700 --> 00:49:27,900
they're incredible and we have
conversations on video and we have new

1948
00:49:27,900 --> 00:49:27,910
conversations on video and we have new
 

1949
00:49:27,910 --> 00:49:28,740
conversations on video and we have new
projects

1950
00:49:28,740 --> 00:49:28,750
projects
 

1951
00:49:28,750 --> 00:49:31,890
projects
I hope this continues throughout 2018 on

1952
00:49:31,890 --> 00:49:31,900
I hope this continues throughout 2018 on
 

1953
00:49:31,900 --> 00:49:35,340
I hope this continues throughout 2018 on
the topics of IAI ethics and bias

1954
00:49:35,340 --> 00:49:35,350
the topics of IAI ethics and bias
 

1955
00:49:35,350 --> 00:49:38,849
the topics of IAI ethics and bias
there's a lot of incredible work in we

1956
00:49:38,849 --> 00:49:38,859
there's a lot of incredible work in we
 

1957
00:49:38,859 --> 00:49:40,890
there's a lot of incredible work in we
now have a speaker there coming on the

1958
00:49:40,890 --> 00:49:40,900
now have a speaker there coming on the
 

1959
00:49:40,900 --> 00:49:42,480
now have a speaker there coming on the
topic of how do we create artificial

1960
00:49:42,480 --> 00:49:42,490
topic of how do we create artificial
 

1961
00:49:42,490 --> 00:49:44,340
topic of how do we create artificial
intelligence systems that I do not

1962
00:49:44,340 --> 00:49:44,350
intelligence systems that I do not
 

1963
00:49:44,350 --> 00:49:47,340
intelligence systems that I do not
discriminate do not form the kind of

1964
00:49:47,340 --> 00:49:47,350
discriminate do not form the kind of
 

1965
00:49:47,350 --> 00:49:49,640
discriminate do not form the kind of
biases that us humans do in this world

1966
00:49:49,640 --> 00:49:49,650
biases that us humans do in this world
 

1967
00:49:49,650 --> 00:49:53,180
biases that us humans do in this world
that are operating under social norms

1968
00:49:53,180 --> 00:49:53,190
that are operating under social norms
 

1969
00:49:53,190 --> 00:49:56,690
that are operating under social norms
but our reasoning beyond the flawed

1970
00:49:56,690 --> 00:49:56,700
but our reasoning beyond the flawed
 

1971
00:49:56,700 --> 00:50:01,460
but our reasoning beyond the flawed
aspects of those social norms with bias

1972
00:50:01,460 --> 00:50:01,470
aspects of those social norms with bias
 

1973
00:50:01,470 --> 00:50:04,109
aspects of those social norms with bias
creativity as with a project of dream

1974
00:50:04,109 --> 00:50:04,119
creativity as with a project of dream
 

1975
00:50:04,119 --> 00:50:07,020
creativity as with a project of dream
vision and beyond there is so much

1976
00:50:07,020 --> 00:50:07,030
vision and beyond there is so much
 

1977
00:50:07,030 --> 00:50:09,840
vision and beyond there is so much
exciting work in charin using machine

1978
00:50:09,840 --> 00:50:09,850
exciting work in charin using machine
 

1979
00:50:09,850 --> 00:50:12,390
exciting work in charin using machine
learning methods to create beautiful art

1980
00:50:12,390 --> 00:50:12,400
learning methods to create beautiful art
 

1981
00:50:12,400 --> 00:50:17,520
learning methods to create beautiful art
and music brain simulation neuroscience

1982
00:50:17,520 --> 00:50:17,530
and music brain simulation neuroscience
 

1983
00:50:17,530 --> 00:50:19,470
and music brain simulation neuroscience
competition in neuroscience shockingly

1984
00:50:19,470 --> 00:50:19,480
competition in neuroscience shockingly
 

1985
00:50:19,480 --> 00:50:21,330
competition in neuroscience shockingly
in the first two weeks we don't have a

1986
00:50:21,330 --> 00:50:21,340
in the first two weeks we don't have a
 

1987
00:50:21,340 --> 00:50:25,109
in the first two weeks we don't have a
competition neuroscience speaker which

1988
00:50:25,109 --> 00:50:25,119
competition neuroscience speaker which
 

1989
00:50:25,119 --> 00:50:27,120
competition neuroscience speaker which
is a fascinating perspective brain

1990
00:50:27,120 --> 00:50:27,130
is a fascinating perspective brain
 

1991
00:50:27,130 --> 00:50:30,260
is a fascinating perspective brain
simulation or neuroscience in general

1992
00:50:30,260 --> 00:50:30,270
simulation or neuroscience in general
 

1993
00:50:30,270 --> 00:50:32,160
simulation or neuroscience in general
computational neuroscience is a

1994
00:50:32,160 --> 00:50:32,170
computational neuroscience is a
 

1995
00:50:32,170 --> 00:50:35,130
computational neuroscience is a
fascinating approach from the from the

1996
00:50:35,130 --> 00:50:35,140
fascinating approach from the from the
 

1997
00:50:35,140 --> 00:50:38,460
fascinating approach from the from the
muck of actual brain work to get the

1998
00:50:38,460 --> 00:50:38,470
muck of actual brain work to get the
 

1999
00:50:38,470 --> 00:50:40,349
muck of actual brain work to get the
perspective of how our brain works and

2000
00:50:40,349 --> 00:50:40,359
perspective of how our brain works and
 

2001
00:50:40,359 --> 00:50:42,090
perspective of how our brain works and
how we can create something that mimics

2002
00:50:42,090 --> 00:50:42,100
how we can create something that mimics
 

2003
00:50:42,100 --> 00:50:45,090
how we can create something that mimics
that resembles the fundamentals of what

2004
00:50:45,090 --> 00:50:45,100
that resembles the fundamentals of what
 

2005
00:50:45,100 --> 00:50:47,940
that resembles the fundamentals of what
makes our brain intelligent and finally

2006
00:50:47,940 --> 00:50:47,950
makes our brain intelligent and finally
 

2007
00:50:47,950 --> 00:50:49,799
makes our brain intelligent and finally
the touring test the traditional

2008
00:50:49,799 --> 00:50:49,809
the touring test the traditional
 

2009
00:50:49,809 --> 00:50:52,679
the touring test the traditional
definition of intelligence defined by

2010
00:50:52,679 --> 00:50:52,689
definition of intelligence defined by
 

2011
00:50:52,689 --> 00:50:54,989
definition of intelligence defined by
Alan Turing was grounded in natural

2012
00:50:54,989 --> 00:50:54,999
Alan Turing was grounded in natural
 

2013
00:50:54,999 --> 00:50:56,729
Alan Turing was grounded in natural
language processing creating chat BOTS

2014
00:50:56,729 --> 00:50:56,739
language processing creating chat BOTS
 

2015
00:50:56,739 --> 00:50:59,489
language processing creating chat BOTS
that impress us that amaze us and trick

2016
00:50:59,489 --> 00:50:59,499
that impress us that amaze us and trick
 

2017
00:50:59,499 --> 00:51:02,339
that impress us that amaze us and trick
us into thinking they're human we will

2018
00:51:02,339 --> 00:51:02,349
us into thinking they're human we will
 

2019
00:51:02,349 --> 00:51:05,880
us into thinking they're human we will
have a project and a speaker on natural

2020
00:51:05,880 --> 00:51:05,890
have a project and a speaker on natural
 

2021
00:51:05,890 --> 00:51:09,479
have a project and a speaker on natural
language processing in March with that

2022
00:51:09,479 --> 00:51:09,489
language processing in March with that
 

2023
00:51:09,489 --> 00:51:11,130
language processing in March with that
I'd like to thank you for coming today

2024
00:51:11,130 --> 00:51:11,140
I'd like to thank you for coming today
 

2025
00:51:11,140 --> 00:51:12,749
I'd like to thank you for coming today
and look forward to seeing your

2026
00:51:12,749 --> 00:51:12,759
and look forward to seeing your
 

2027
00:51:12,759 --> 00:51:15,239
and look forward to seeing your
submissions for the three projects thank

2028
00:51:15,239 --> 00:51:15,249
submissions for the three projects thank
 

2029
00:51:15,249 --> 00:51:16,240
submissions for the three projects thank
you very much

2030
00:51:16,240 --> 00:51:16,250
you very much
 

2031
00:51:16,250 --> 00:51:20,199
you very much
[Applause]

