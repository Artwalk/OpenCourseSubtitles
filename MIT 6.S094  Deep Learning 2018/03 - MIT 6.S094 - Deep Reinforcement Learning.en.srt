1
00:00:00,000 --> 00:00:01,760

today we will talk about deep

2
00:00:01,760 --> 00:00:01,770
today we will talk about deep
 

3
00:00:01,770 --> 00:00:08,089
today we will talk about deep
reinforcement learning the question we

4
00:00:08,089 --> 00:00:08,099
reinforcement learning the question we
 

5
00:00:08,099 --> 00:00:11,060
reinforcement learning the question we
would like to explore it's to which

6
00:00:11,060 --> 00:00:11,070
would like to explore it's to which
 

7
00:00:11,070 --> 00:00:15,589
would like to explore it's to which
degree we can teach systems to act to

8
00:00:15,589 --> 00:00:15,599
degree we can teach systems to act to
 

9
00:00:15,599 --> 00:00:19,390
degree we can teach systems to act to
perceive and act in this world from data

10
00:00:19,390 --> 00:00:19,400
perceive and act in this world from data
 

11
00:00:19,400 --> 00:00:22,640
perceive and act in this world from data
so let's take a step back and think of

12
00:00:22,640 --> 00:00:22,650
so let's take a step back and think of
 

13
00:00:22,650 --> 00:00:25,880
so let's take a step back and think of
what is the full range of tasks then

14
00:00:25,880 --> 00:00:25,890
what is the full range of tasks then
 

15
00:00:25,890 --> 00:00:27,560
what is the full range of tasks then
artificial intelligence system needs to

16
00:00:27,560 --> 00:00:27,570
artificial intelligence system needs to
 

17
00:00:27,570 --> 00:00:30,710
artificial intelligence system needs to
accomplish here's the stack from top to

18
00:00:30,710 --> 00:00:30,720
accomplish here's the stack from top to
 

19
00:00:30,720 --> 00:00:34,819
accomplish here's the stack from top to
bottom top the input bottom output the

20
00:00:34,819 --> 00:00:34,829
bottom top the input bottom output the
 

21
00:00:34,829 --> 00:00:36,680
bottom top the input bottom output the
environment at the top the world that

22
00:00:36,680 --> 00:00:36,690
environment at the top the world that
 

23
00:00:36,690 --> 00:00:40,160
environment at the top the world that
the agent is operating in sensed by

24
00:00:40,160 --> 00:00:40,170
the agent is operating in sensed by
 

25
00:00:40,170 --> 00:00:43,369
the agent is operating in sensed by
sensors taking in the world outside and

26
00:00:43,369 --> 00:00:43,379
sensors taking in the world outside and
 

27
00:00:43,379 --> 00:00:45,260
sensors taking in the world outside and
converting it to raw data interpretable

28
00:00:45,260 --> 00:00:45,270
converting it to raw data interpretable
 

29
00:00:45,270 --> 00:00:50,150
converting it to raw data interpretable
by machines sensor data and from that

30
00:00:50,150 --> 00:00:50,160
by machines sensor data and from that
 

31
00:00:50,160 --> 00:00:54,319
by machines sensor data and from that
raw sensor data you extract features you

32
00:00:54,319 --> 00:00:54,329
raw sensor data you extract features you
 

33
00:00:54,329 --> 00:00:58,340
raw sensor data you extract features you
extract structure from that data such

34
00:00:58,340 --> 00:00:58,350
extract structure from that data such
 

35
00:00:58,350 --> 00:01:01,479
extract structure from that data such
that you can input it make sense of it

36
00:01:01,479 --> 00:01:01,489
that you can input it make sense of it
 

37
00:01:01,489 --> 00:01:04,820
that you can input it make sense of it
discriminate separate understand the

38
00:01:04,820 --> 00:01:04,830
discriminate separate understand the
 

39
00:01:04,830 --> 00:01:10,490
discriminate separate understand the
data and as we discussed you form higher

40
00:01:10,490 --> 00:01:10,500
data and as we discussed you form higher
 

41
00:01:10,500 --> 00:01:12,469
data and as we discussed you form higher
and higher order representations a

42
00:01:12,469 --> 00:01:12,479
and higher order representations a
 

43
00:01:12,479 --> 00:01:15,289
and higher order representations a
hierarchy of representations based on

44
00:01:15,289 --> 00:01:15,299
hierarchy of representations based on
 

45
00:01:15,299 --> 00:01:17,270
hierarchy of representations based on
which the machine learning techniques

46
00:01:17,270 --> 00:01:17,280
which the machine learning techniques
 

47
00:01:17,280 --> 00:01:23,749
which the machine learning techniques
can then be applied once the machine

48
00:01:23,749 --> 00:01:23,759
can then be applied once the machine
 

49
00:01:23,759 --> 00:01:26,060
can then be applied once the machine
learning techniques the understanding as

50
00:01:26,060 --> 00:01:26,070
learning techniques the understanding as
 

51
00:01:26,070 --> 00:01:28,940
learning techniques the understanding as
I mentioned converts the data into

52
00:01:28,940 --> 00:01:28,950
I mentioned converts the data into
 

53
00:01:28,950 --> 00:01:30,679
I mentioned converts the data into
features into higher order

54
00:01:30,679 --> 00:01:30,689
features into higher order
 

55
00:01:30,689 --> 00:01:32,539
features into higher order
representations and into simple

56
00:01:32,539 --> 00:01:32,549
representations and into simple
 

57
00:01:32,549 --> 00:01:35,270
representations and into simple
actionable useful information we

58
00:01:35,270 --> 00:01:35,280
actionable useful information we
 

59
00:01:35,280 --> 00:01:36,890
actionable useful information we
aggregate that information into

60
00:01:36,890 --> 00:01:36,900
aggregate that information into
 

61
00:01:36,900 --> 00:01:38,929
aggregate that information into
knowledge we take the pieces of

62
00:01:38,929 --> 00:01:38,939
knowledge we take the pieces of
 

63
00:01:38,939 --> 00:01:40,630
knowledge we take the pieces of
knowledge extracted from the data

64
00:01:40,630 --> 00:01:40,640
knowledge extracted from the data
 

65
00:01:40,640 --> 00:01:42,800
knowledge extracted from the data
through the machine learning techniques

66
00:01:42,800 --> 00:01:42,810
through the machine learning techniques
 

67
00:01:42,810 --> 00:01:48,649
through the machine learning techniques
and to build a taxonomy a library of

68
00:01:48,649 --> 00:01:48,659
and to build a taxonomy a library of
 

69
00:01:48,659 --> 00:01:51,850
and to build a taxonomy a library of
knowledge and with that knowledge we

70
00:01:51,850 --> 00:01:51,860
knowledge and with that knowledge we
 

71
00:01:51,860 --> 00:01:57,560
knowledge and with that knowledge we
reason an aging estas to reason to

72
00:01:57,560 --> 00:01:57,570
reason an aging estas to reason to
 

73
00:01:57,570 --> 00:02:01,940
reason an aging estas to reason to
aggregate to connect pieces of data it's

74
00:02:01,940 --> 00:02:01,950
aggregate to connect pieces of data it's
 

75
00:02:01,950 --> 00:02:04,399
aggregate to connect pieces of data it's
seen in the recent past or the distant

76
00:02:04,399 --> 00:02:04,409
seen in the recent past or the distant
 

77
00:02:04,409 --> 00:02:06,770
seen in the recent past or the distant
past to make sense of the world that's

78
00:02:06,770 --> 00:02:06,780
past to make sense of the world that's
 

79
00:02:06,780 --> 00:02:09,740
past to make sense of the world that's
operating in and finally to make a plan

80
00:02:09,740 --> 00:02:09,750
operating in and finally to make a plan
 

81
00:02:09,750 --> 00:02:11,900
operating in and finally to make a plan
of how to act in that world based on its

82
00:02:11,900 --> 00:02:11,910
of how to act in that world based on its
 

83
00:02:11,910 --> 00:02:13,930
of how to act in that world based on its
objectives based on what it wants to

84
00:02:13,930 --> 00:02:13,940
objectives based on what it wants to
 

85
00:02:13,940 --> 00:02:17,890
objectives based on what it wants to
accomplished as I mentioned a simple but

86
00:02:17,890 --> 00:02:17,900
accomplished as I mentioned a simple but
 

87
00:02:17,900 --> 00:02:19,300
accomplished as I mentioned a simple but
commonly accepted definition of

88
00:02:19,300 --> 00:02:19,310
commonly accepted definition of
 

89
00:02:19,310 --> 00:02:22,360
commonly accepted definition of
intelligence is a system that's able to

90
00:02:22,360 --> 00:02:22,370
intelligence is a system that's able to
 

91
00:02:22,370 --> 00:02:25,570
intelligence is a system that's able to
accomplish complex goals so system

92
00:02:25,570 --> 00:02:25,580
accomplish complex goals so system
 

93
00:02:25,580 --> 00:02:27,250
accomplish complex goals so system
that's operating in the environment in

94
00:02:27,250 --> 00:02:27,260
that's operating in the environment in
 

95
00:02:27,260 --> 00:02:29,890
that's operating in the environment in
this world must have a goal must have an

96
00:02:29,890 --> 00:02:29,900
this world must have a goal must have an
 

97
00:02:29,900 --> 00:02:32,530
this world must have a goal must have an
objective function a reward function and

98
00:02:32,530 --> 00:02:32,540
objective function a reward function and
 

99
00:02:32,540 --> 00:02:34,810
objective function a reward function and
based on that it forms a plan and takes

100
00:02:34,810 --> 00:02:34,820
based on that it forms a plan and takes
 

101
00:02:34,820 --> 00:02:37,540
based on that it forms a plan and takes
action and because there operates in

102
00:02:37,540 --> 00:02:37,550
action and because there operates in
 

103
00:02:37,550 --> 00:02:39,310
action and because there operates in
many cases in the physical world

104
00:02:39,310 --> 00:02:39,320
many cases in the physical world
 

105
00:02:39,320 --> 00:02:43,120
many cases in the physical world
it must have tools effectors with which

106
00:02:43,120 --> 00:02:43,130
it must have tools effectors with which
 

107
00:02:43,130 --> 00:02:45,010
it must have tools effectors with which
it applies the actions to change

108
00:02:45,010 --> 00:02:45,020
it applies the actions to change
 

109
00:02:45,020 --> 00:02:47,500
it applies the actions to change
something about the world that's the

110
00:02:47,500 --> 00:02:47,510
something about the world that's the
 

111
00:02:47,510 --> 00:02:50,770
something about the world that's the
full stack of an artificial intelligence

112
00:02:50,770 --> 00:02:50,780
full stack of an artificial intelligence
 

113
00:02:50,780 --> 00:02:54,280
full stack of an artificial intelligence
system that acts in the world and the

114
00:02:54,280 --> 00:02:54,290
system that acts in the world and the
 

115
00:02:54,290 --> 00:02:58,780
system that acts in the world and the
question is what kind of task can such a

116
00:02:58,780 --> 00:02:58,790
question is what kind of task can such a
 

117
00:02:58,790 --> 00:03:02,440
question is what kind of task can such a
system take on what kind of task can an

118
00:03:02,440 --> 00:03:02,450
system take on what kind of task can an
 

119
00:03:02,450 --> 00:03:04,320
system take on what kind of task can an
artificial intelligence system learn as

120
00:03:04,320 --> 00:03:04,330
artificial intelligence system learn as
 

121
00:03:04,330 --> 00:03:08,230
artificial intelligence system learn as
we understand AI today we will talk

122
00:03:08,230 --> 00:03:08,240
we understand AI today we will talk
 

123
00:03:08,240 --> 00:03:11,170
we understand AI today we will talk
about the advancement of deeper

124
00:03:11,170 --> 00:03:11,180
about the advancement of deeper
 

125
00:03:11,180 --> 00:03:13,210
about the advancement of deeper
enforcement learning approaches and some

126
00:03:13,210 --> 00:03:13,220
enforcement learning approaches and some
 

127
00:03:13,220 --> 00:03:15,190
enforcement learning approaches and some
of the fascinating ways it's able to

128
00:03:15,190 --> 00:03:15,200
of the fascinating ways it's able to
 

129
00:03:15,200 --> 00:03:18,729
of the fascinating ways it's able to
take much of the stack and treat it as

130
00:03:18,729 --> 00:03:18,739
take much of the stack and treat it as
 

131
00:03:18,739 --> 00:03:21,880
take much of the stack and treat it as
an end-to-end learning problem but we

132
00:03:21,880 --> 00:03:21,890
an end-to-end learning problem but we
 

133
00:03:21,890 --> 00:03:24,370
an end-to-end learning problem but we
look at games we look at simple

134
00:03:24,370 --> 00:03:24,380
look at games we look at simple
 

135
00:03:24,380 --> 00:03:26,590
look at games we look at simple
formalized worlds while it's still

136
00:03:26,590 --> 00:03:26,600
formalized worlds while it's still
 

137
00:03:26,600 --> 00:03:28,800
formalized worlds while it's still
impressive beautiful and unprecedented

138
00:03:28,800 --> 00:03:28,810
impressive beautiful and unprecedented
 

139
00:03:28,810 --> 00:03:32,110
impressive beautiful and unprecedented
accomplishments it's nevertheless formal

140
00:03:32,110 --> 00:03:32,120
accomplishments it's nevertheless formal
 

141
00:03:32,120 --> 00:03:36,850
accomplishments it's nevertheless formal
tasks can we then move beyond games and

142
00:03:36,850 --> 00:03:36,860
tasks can we then move beyond games and
 

143
00:03:36,860 --> 00:03:40,449
tasks can we then move beyond games and
into expert tasks of medical diagnosis

144
00:03:40,449 --> 00:03:40,459
into expert tasks of medical diagnosis
 

145
00:03:40,459 --> 00:03:45,759
into expert tasks of medical diagnosis
of design and into natural language and

146
00:03:45,759 --> 00:03:45,769
of design and into natural language and
 

147
00:03:45,769 --> 00:03:49,199
of design and into natural language and
finally the human level tasks of emotion

148
00:03:49,199 --> 00:03:49,209
finally the human level tasks of emotion
 

149
00:03:49,209 --> 00:03:57,160
finally the human level tasks of emotion
imagination consciousness let's once

150
00:03:57,160 --> 00:03:57,170
imagination consciousness let's once
 

151
00:03:57,170 --> 00:03:59,740
imagination consciousness let's once
again review the stack in practicality

152
00:03:59,740 --> 00:03:59,750
again review the stack in practicality
 

153
00:03:59,750 --> 00:04:03,970
again review the stack in practicality
in the tools we have the input for

154
00:04:03,970 --> 00:04:03,980
in the tools we have the input for
 

155
00:04:03,980 --> 00:04:07,390
in the tools we have the input for
robots operating in the world from cars

156
00:04:07,390 --> 00:04:07,400
robots operating in the world from cars
 

157
00:04:07,400 --> 00:04:10,870
robots operating in the world from cars
to humanoid to drones as light our

158
00:04:10,870 --> 00:04:10,880
to humanoid to drones as light our
 

159
00:04:10,880 --> 00:04:14,560
to humanoid to drones as light our
camera radar GPS stereo cameras audio

160
00:04:14,560 --> 00:04:14,570
camera radar GPS stereo cameras audio
 

161
00:04:14,570 --> 00:04:17,590
camera radar GPS stereo cameras audio
microphone networking for communication

162
00:04:17,590 --> 00:04:17,600
microphone networking for communication
 

163
00:04:17,600 --> 00:04:19,090
microphone networking for communication
and the various ways to measure

164
00:04:19,090 --> 00:04:19,100
and the various ways to measure
 

165
00:04:19,100 --> 00:04:23,409
and the various ways to measure
kinematics with IMU

166
00:04:23,409 --> 00:04:23,419
kinematics with IMU
 

167
00:04:23,419 --> 00:04:27,399
kinematics with IMU
the raw sensory data is then processed

168
00:04:27,399 --> 00:04:27,409
the raw sensory data is then processed
 

169
00:04:27,409 --> 00:04:29,920
the raw sensory data is then processed
features of form to representations are

170
00:04:29,920 --> 00:04:29,930
features of form to representations are
 

171
00:04:29,930 --> 00:04:31,989
features of form to representations are
formed and multiple higher and higher

172
00:04:31,989 --> 00:04:31,999
formed and multiple higher and higher
 

173
00:04:31,999 --> 00:04:33,279
formed and multiple higher and higher
order representations

174
00:04:33,279 --> 00:04:33,289
order representations
 

175
00:04:33,289 --> 00:04:35,439
order representations
that's what deep learning gets us before

176
00:04:35,439 --> 00:04:35,449
that's what deep learning gets us before
 

177
00:04:35,449 --> 00:04:38,040
that's what deep learning gets us before
neural networks before the advent of

178
00:04:38,040 --> 00:04:38,050
neural networks before the advent of
 

179
00:04:38,050 --> 00:04:41,019
neural networks before the advent of
before the recent successes of neural

180
00:04:41,019 --> 00:04:41,029
before the recent successes of neural
 

181
00:04:41,029 --> 00:04:43,209
before the recent successes of neural
networks to go deeper and therefore be

182
00:04:43,209 --> 00:04:43,219
networks to go deeper and therefore be
 

183
00:04:43,219 --> 00:04:45,730
networks to go deeper and therefore be
able to form high order representations

184
00:04:45,730 --> 00:04:45,740
able to form high order representations
 

185
00:04:45,740 --> 00:04:48,550
able to form high order representations
of the data that was done by experts by

186
00:04:48,550 --> 00:04:48,560
of the data that was done by experts by
 

187
00:04:48,560 --> 00:04:51,369
of the data that was done by experts by
human experts today networks are able to

188
00:04:51,369 --> 00:04:51,379
human experts today networks are able to
 

189
00:04:51,379 --> 00:04:53,559
human experts today networks are able to
do that that's the representation piece

190
00:04:53,559 --> 00:04:53,569
do that that's the representation piece
 

191
00:04:53,569 --> 00:04:55,929
do that that's the representation piece
and on top of the representation piece

192
00:04:55,929 --> 00:04:55,939
and on top of the representation piece
 

193
00:04:55,939 --> 00:04:58,779
and on top of the representation piece
the final layers these networks are able

194
00:04:58,779 --> 00:04:58,789
the final layers these networks are able
 

195
00:04:58,789 --> 00:05:00,279
the final layers these networks are able
to accomplish the supervised learning

196
00:05:00,279 --> 00:05:00,289
to accomplish the supervised learning
 

197
00:05:00,289 --> 00:05:04,260
to accomplish the supervised learning
tasks the generative tasks and the

198
00:05:04,260 --> 00:05:04,270
tasks the generative tasks and the
 

199
00:05:04,270 --> 00:05:08,140
tasks the generative tasks and the
unsupervised clustering tasks through

200
00:05:08,140 --> 00:05:08,150
unsupervised clustering tasks through
 

201
00:05:08,150 --> 00:05:10,540
unsupervised clustering tasks through
machine learning that's what we talked

202
00:05:10,540 --> 00:05:10,550
machine learning that's what we talked
 

203
00:05:10,550 --> 00:05:13,300
machine learning that's what we talked
about a little in lecture one and we'll

204
00:05:13,300 --> 00:05:13,310
about a little in lecture one and we'll
 

205
00:05:13,310 --> 00:05:16,679
about a little in lecture one and we'll
continue tomorrow and Wednesday

206
00:05:16,679 --> 00:05:16,689
continue tomorrow and Wednesday
 

207
00:05:16,689 --> 00:05:20,019
continue tomorrow and Wednesday
that's supervised learning and you can

208
00:05:20,019 --> 00:05:20,029
that's supervised learning and you can
 

209
00:05:20,029 --> 00:05:21,909
that's supervised learning and you can
think about the output of those networks

210
00:05:21,909 --> 00:05:21,919
think about the output of those networks
 

211
00:05:21,919 --> 00:05:25,209
think about the output of those networks
as simple clean useful valuable

212
00:05:25,209 --> 00:05:25,219
as simple clean useful valuable
 

213
00:05:25,219 --> 00:05:27,869
as simple clean useful valuable
information that's the knowledge and

214
00:05:27,869 --> 00:05:27,879
information that's the knowledge and
 

215
00:05:27,879 --> 00:05:31,719
information that's the knowledge and
that knowledge can be in the form of

216
00:05:31,719 --> 00:05:31,729
that knowledge can be in the form of
 

217
00:05:31,729 --> 00:05:34,540
that knowledge can be in the form of
single numbers it could be regression

218
00:05:34,540 --> 00:05:34,550
single numbers it could be regression
 

219
00:05:34,550 --> 00:05:37,119
single numbers it could be regression
continuous variables it could be a

220
00:05:37,119 --> 00:05:37,129
continuous variables it could be a
 

221
00:05:37,129 --> 00:05:39,159
continuous variables it could be a
sequence of numbers it can be images

222
00:05:39,159 --> 00:05:39,169
sequence of numbers it can be images
 

223
00:05:39,169 --> 00:05:45,399
sequence of numbers it can be images
audio sentences text speech once that

224
00:05:45,399 --> 00:05:45,409
audio sentences text speech once that
 

225
00:05:45,409 --> 00:05:47,829
audio sentences text speech once that
knowledge is extracted and aggregated

226
00:05:47,829 --> 00:05:47,839
knowledge is extracted and aggregated
 

227
00:05:47,839 --> 00:05:51,429
knowledge is extracted and aggregated
how do we connect it in multi resolution

228
00:05:51,429 --> 00:05:51,439
how do we connect it in multi resolution
 

229
00:05:51,439 --> 00:05:54,429
how do we connect it in multi resolution
always form hierarchies of ideas connect

230
00:05:54,429 --> 00:05:54,439
always form hierarchies of ideas connect
 

231
00:05:54,439 --> 00:05:58,469
always form hierarchies of ideas connect
ideas the trivial silly example is

232
00:05:58,469 --> 00:05:58,479
ideas the trivial silly example is
 

233
00:05:58,479 --> 00:06:01,629
ideas the trivial silly example is
connecting images activity recognition

234
00:06:01,629 --> 00:06:01,639
connecting images activity recognition
 

235
00:06:01,639 --> 00:06:04,839
connecting images activity recognition
and audio for example if it looks like a

236
00:06:04,839 --> 00:06:04,849
and audio for example if it looks like a
 

237
00:06:04,849 --> 00:06:07,209
and audio for example if it looks like a
duck quacks like a duck and swims like a

238
00:06:07,209 --> 00:06:07,219
duck quacks like a duck and swims like a
 

239
00:06:07,219 --> 00:06:10,689
duck quacks like a duck and swims like a
duck we do not currently have approaches

240
00:06:10,689 --> 00:06:10,699
duck we do not currently have approaches
 

241
00:06:10,699 --> 00:06:11,860
duck we do not currently have approaches
that effectively integrate this

242
00:06:11,860 --> 00:06:11,870
that effectively integrate this
 

243
00:06:11,870 --> 00:06:14,950
that effectively integrate this
information to produce a higher

244
00:06:14,950 --> 00:06:14,960
information to produce a higher
 

245
00:06:14,960 --> 00:06:17,679
information to produce a higher
confidence estimate that is in fact the

246
00:06:17,679 --> 00:06:17,689
confidence estimate that is in fact the
 

247
00:06:17,689 --> 00:06:22,089
confidence estimate that is in fact the
duck and the planning piece the task of

248
00:06:22,089 --> 00:06:22,099
duck and the planning piece the task of
 

249
00:06:22,099 --> 00:06:25,420
duck and the planning piece the task of
taking the sensory information fusing

250
00:06:25,420 --> 00:06:25,430
taking the sensory information fusing
 

251
00:06:25,430 --> 00:06:27,339
taking the sensory information fusing
the sensory information and making

252
00:06:27,339 --> 00:06:27,349
the sensory information and making
 

253
00:06:27,349 --> 00:06:30,420
the sensory information and making
action control and longer-term plans

254
00:06:30,420 --> 00:06:30,430
action control and longer-term plans
 

255
00:06:30,430 --> 00:06:33,790
action control and longer-term plans
based on that information as we'll

256
00:06:33,790 --> 00:06:33,800
based on that information as we'll
 

257
00:06:33,800 --> 00:06:35,660
based on that information as we'll
discuss today

258
00:06:35,660 --> 00:06:35,670
discuss today
 

259
00:06:35,670 --> 00:06:38,150
discuss today
are more and more amenable to the

260
00:06:38,150 --> 00:06:38,160
are more and more amenable to the
 

261
00:06:38,160 --> 00:06:39,830
are more and more amenable to the
learning approach to the deep learning

262
00:06:39,830 --> 00:06:39,840
learning approach to the deep learning
 

263
00:06:39,840 --> 00:06:42,230
learning approach to the deep learning
approach but to date have been the most

264
00:06:42,230 --> 00:06:42,240
approach but to date have been the most
 

265
00:06:42,240 --> 00:06:44,450
approach but to date have been the most
successful and non learning optimization

266
00:06:44,450 --> 00:06:44,460
successful and non learning optimization
 

267
00:06:44,460 --> 00:06:46,520
successful and non learning optimization
based approaches like with the several

268
00:06:46,520 --> 00:06:46,530
based approaches like with the several
 

269
00:06:46,530 --> 00:06:48,350
based approaches like with the several
of the guest speakers we have including

270
00:06:48,350 --> 00:06:48,360
of the guest speakers we have including
 

271
00:06:48,360 --> 00:06:51,320
of the guest speakers we have including
the creator of this robot Atlas in

272
00:06:51,320 --> 00:06:51,330
the creator of this robot Atlas in
 

273
00:06:51,330 --> 00:06:56,450
the creator of this robot Atlas in
Boston Dynamics so the question how much

274
00:06:56,450 --> 00:06:56,460
Boston Dynamics so the question how much
 

275
00:06:56,460 --> 00:06:58,430
Boston Dynamics so the question how much
of the stack can be learned and to end

276
00:06:58,430 --> 00:06:58,440
of the stack can be learned and to end
 

277
00:06:58,440 --> 00:07:01,070
of the stack can be learned and to end
from the input to the output we know we

278
00:07:01,070 --> 00:07:01,080
from the input to the output we know we
 

279
00:07:01,080 --> 00:07:03,620
from the input to the output we know we
can learn the representation and the

280
00:07:03,620 --> 00:07:03,630
can learn the representation and the
 

281
00:07:03,630 --> 00:07:05,330
can learn the representation and the
knowledge from the representation and to

282
00:07:05,330 --> 00:07:05,340
knowledge from the representation and to
 

283
00:07:05,340 --> 00:07:08,360
knowledge from the representation and to
knowledge even with the kernel methods

284
00:07:08,360 --> 00:07:08,370
knowledge even with the kernel methods
 

285
00:07:08,370 --> 00:07:13,580
knowledge even with the kernel methods
of SVM and certainly with with neural

286
00:07:13,580 --> 00:07:13,590
of SVM and certainly with with neural
 

287
00:07:13,590 --> 00:07:17,000
of SVM and certainly with with neural
networks mapping from representation to

288
00:07:17,000 --> 00:07:17,010
networks mapping from representation to
 

289
00:07:17,010 --> 00:07:21,050
networks mapping from representation to
information has been where the primary

290
00:07:21,050 --> 00:07:21,060
information has been where the primary
 

291
00:07:21,060 --> 00:07:22,670
information has been where the primary
success of machine learning over the

292
00:07:22,670 --> 00:07:22,680
success of machine learning over the
 

293
00:07:22,680 --> 00:07:26,330
success of machine learning over the
past three decades has been mapping from

294
00:07:26,330 --> 00:07:26,340
past three decades has been mapping from
 

295
00:07:26,340 --> 00:07:29,300
past three decades has been mapping from
raw sensory data to knowledge that's

296
00:07:29,300 --> 00:07:29,310
raw sensory data to knowledge that's
 

297
00:07:29,310 --> 00:07:31,610
raw sensory data to knowledge that's
where the success the automated

298
00:07:31,610 --> 00:07:31,620
where the success the automated
 

299
00:07:31,620 --> 00:07:34,420
where the success the automated
representation learning of deep learning

300
00:07:34,420 --> 00:07:34,430
representation learning of deep learning
 

301
00:07:34,430 --> 00:07:37,340
representation learning of deep learning
has been a success going straight from

302
00:07:37,340 --> 00:07:37,350
has been a success going straight from
 

303
00:07:37,350 --> 00:07:40,460
has been a success going straight from
raw data to knowledge the open question

304
00:07:40,460 --> 00:07:40,470
raw data to knowledge the open question
 

305
00:07:40,470 --> 00:07:43,940
raw data to knowledge the open question
for us today and beyond is if we can

306
00:07:43,940 --> 00:07:43,950
for us today and beyond is if we can
 

307
00:07:43,950 --> 00:07:46,130
for us today and beyond is if we can
expand the red box there of what can be

308
00:07:46,130 --> 00:07:46,140
expand the red box there of what can be
 

309
00:07:46,140 --> 00:07:48,980
expand the red box there of what can be
learned and to end from sensory data to

310
00:07:48,980 --> 00:07:48,990
learned and to end from sensory data to
 

311
00:07:48,990 --> 00:07:51,340
learned and to end from sensory data to
reasoning so aggregating forming higher

312
00:07:51,340 --> 00:07:51,350
reasoning so aggregating forming higher
 

313
00:07:51,350 --> 00:07:53,540
reasoning so aggregating forming higher
representations of the extracted

314
00:07:53,540 --> 00:07:53,550
representations of the extracted
 

315
00:07:53,550 --> 00:07:57,500
representations of the extracted
knowledge and forming plans and acting

316
00:07:57,500 --> 00:07:57,510
knowledge and forming plans and acting
 

317
00:07:57,510 --> 00:07:59,230
knowledge and forming plans and acting
in this world from the raw sensory data

318
00:07:59,230 --> 00:07:59,240
in this world from the raw sensory data
 

319
00:07:59,240 --> 00:08:02,360
in this world from the raw sensory data
we will show the incredible fact that

320
00:08:02,360 --> 00:08:02,370
we will show the incredible fact that
 

321
00:08:02,370 --> 00:08:05,180
we will show the incredible fact that
we're able to do CERN exactly what's

322
00:08:05,180 --> 00:08:05,190
we're able to do CERN exactly what's
 

323
00:08:05,190 --> 00:08:07,070
we're able to do CERN exactly what's
shown here and to end with deeper

324
00:08:07,070 --> 00:08:07,080
shown here and to end with deeper
 

325
00:08:07,080 --> 00:08:09,460
shown here and to end with deeper
enforcement learning on trivial tasks in

326
00:08:09,460 --> 00:08:09,470
enforcement learning on trivial tasks in
 

327
00:08:09,470 --> 00:08:12,740
enforcement learning on trivial tasks in
a generalizable way the question is

328
00:08:12,740 --> 00:08:12,750
a generalizable way the question is
 

329
00:08:12,750 --> 00:08:14,990
a generalizable way the question is
whether that can then move on to

330
00:08:14,990 --> 00:08:15,000
whether that can then move on to
 

331
00:08:15,000 --> 00:08:17,780
whether that can then move on to
real-world tasks of autonomous vehicles

332
00:08:17,780 --> 00:08:17,790
real-world tasks of autonomous vehicles
 

333
00:08:17,790 --> 00:08:23,870
real-world tasks of autonomous vehicles
of humanoid robotics and so on that's

334
00:08:23,870 --> 00:08:23,880
of humanoid robotics and so on that's
 

335
00:08:23,880 --> 00:08:26,330
of humanoid robotics and so on that's
the open question so today let's talk

336
00:08:26,330 --> 00:08:26,340
the open question so today let's talk
 

337
00:08:26,340 --> 00:08:28,280
the open question so today let's talk
about reinforcement learning there's

338
00:08:28,280 --> 00:08:28,290
about reinforcement learning there's
 

339
00:08:28,290 --> 00:08:31,870
about reinforcement learning there's
three types of machine learning

340
00:08:31,870 --> 00:08:31,880

 

341
00:08:31,880 --> 00:08:34,420

supervised

342
00:08:34,420 --> 00:08:34,430
supervised
 

343
00:08:34,430 --> 00:08:37,430
supervised
unsupervised are the categories at the

344
00:08:37,430 --> 00:08:37,440
unsupervised are the categories at the
 

345
00:08:37,440 --> 00:08:40,519
unsupervised are the categories at the
extremes in relative to the amount of

346
00:08:40,519 --> 00:08:40,529
extremes in relative to the amount of
 

347
00:08:40,529 --> 00:08:42,620
extremes in relative to the amount of
human and human input that's required

348
00:08:42,620 --> 00:08:42,630
human and human input that's required
 

349
00:08:42,630 --> 00:08:44,930
human and human input that's required
for supervised learning every piece of

350
00:08:44,930 --> 00:08:44,940
for supervised learning every piece of
 

351
00:08:44,940 --> 00:08:47,090
for supervised learning every piece of
data that's used for teaching these

352
00:08:47,090 --> 00:08:47,100
data that's used for teaching these
 

353
00:08:47,100 --> 00:08:50,329
data that's used for teaching these
systems is first labeled by human beings

354
00:08:50,329 --> 00:08:50,339
systems is first labeled by human beings
 

355
00:08:50,339 --> 00:08:52,430
systems is first labeled by human beings
and unsupervised learning on the right

356
00:08:52,430 --> 00:08:52,440
and unsupervised learning on the right
 

357
00:08:52,440 --> 00:08:56,530
and unsupervised learning on the right
is no data is labeled by human beings in

358
00:08:56,530 --> 00:08:56,540
is no data is labeled by human beings in
 

359
00:08:56,540 --> 00:09:01,070
is no data is labeled by human beings in
between is some sparse input from humans

360
00:09:01,070 --> 00:09:01,080
between is some sparse input from humans
 

361
00:09:01,080 --> 00:09:04,730
between is some sparse input from humans
semi-supervised learning is when only

362
00:09:04,730 --> 00:09:04,740
semi-supervised learning is when only
 

363
00:09:04,740 --> 00:09:07,100
semi-supervised learning is when only
part of the data is provided by humans

364
00:09:07,100 --> 00:09:07,110
part of the data is provided by humans
 

365
00:09:07,110 --> 00:09:09,620
part of the data is provided by humans
ground truth and the rest must be

366
00:09:09,620 --> 00:09:09,630
ground truth and the rest must be
 

367
00:09:09,630 --> 00:09:11,690
ground truth and the rest must be
inferred generalized by the system and

368
00:09:11,690 --> 00:09:11,700
inferred generalized by the system and
 

369
00:09:11,700 --> 00:09:14,140
inferred generalized by the system and
that's what reinforcement learning Falls

370
00:09:14,140 --> 00:09:14,150
that's what reinforcement learning Falls
 

371
00:09:14,150 --> 00:09:17,210
that's what reinforcement learning Falls
reinforcement learning has shown there

372
00:09:17,210 --> 00:09:17,220
reinforcement learning has shown there
 

373
00:09:17,220 --> 00:09:20,690
reinforcement learning has shown there
with the cats as I said every successful

374
00:09:20,690 --> 00:09:20,700
with the cats as I said every successful
 

375
00:09:20,700 --> 00:09:24,170
with the cats as I said every successful
presentation must include cats they're

376
00:09:24,170 --> 00:09:24,180
presentation must include cats they're
 

377
00:09:24,180 --> 00:09:28,820
presentation must include cats they're
supposed to be Pavlov's cats and ringing

378
00:09:28,820 --> 00:09:28,830
supposed to be Pavlov's cats and ringing
 

379
00:09:28,830 --> 00:09:30,440
supposed to be Pavlov's cats and ringing
a bell and every time they ring a bell

380
00:09:30,440 --> 00:09:30,450
a bell and every time they ring a bell
 

381
00:09:30,450 --> 00:09:32,329
a bell and every time they ring a bell
they're given food and they learn this

382
00:09:32,329 --> 00:09:32,339
they're given food and they learn this
 

383
00:09:32,339 --> 00:09:36,410
they're given food and they learn this
process the goal of reinforcement

384
00:09:36,410 --> 00:09:36,420
process the goal of reinforcement
 

385
00:09:36,420 --> 00:09:41,480
process the goal of reinforcement
learning is to learn from sparse reward

386
00:09:41,480 --> 00:09:41,490
learning is to learn from sparse reward
 

387
00:09:41,490 --> 00:09:44,420
learning is to learn from sparse reward
data from learn from sparse supervised

388
00:09:44,420 --> 00:09:44,430
data from learn from sparse supervised
 

389
00:09:44,430 --> 00:09:47,120
data from learn from sparse supervised
data and take advantage of the fact that

390
00:09:47,120 --> 00:09:47,130
data and take advantage of the fact that
 

391
00:09:47,130 --> 00:09:49,910
data and take advantage of the fact that
in simulation or in the real world there

392
00:09:49,910 --> 00:09:49,920
in simulation or in the real world there
 

393
00:09:49,920 --> 00:09:51,829
in simulation or in the real world there
is a temporal consistency to the world

394
00:09:51,829 --> 00:09:51,839
is a temporal consistency to the world
 

395
00:09:51,839 --> 00:09:55,010
is a temporal consistency to the world
there is a temporal dynamics that

396
00:09:55,010 --> 00:09:55,020
there is a temporal dynamics that
 

397
00:09:55,020 --> 00:09:56,960
there is a temporal dynamics that
follows from state to state the state

398
00:09:56,960 --> 00:09:56,970
follows from state to state the state
 

399
00:09:56,970 --> 00:09:59,510
follows from state to state the state
through time and so you can propagate

400
00:09:59,510 --> 00:09:59,520
through time and so you can propagate
 

401
00:09:59,520 --> 00:10:02,030
through time and so you can propagate
information even if the information that

402
00:10:02,030 --> 00:10:02,040
information even if the information that
 

403
00:10:02,040 --> 00:10:04,100
information even if the information that
you're received about the the

404
00:10:04,100 --> 00:10:04,110
you're received about the the
 

405
00:10:04,110 --> 00:10:06,140
you're received about the the
supervision the ground truth is sparse

406
00:10:06,140 --> 00:10:06,150
supervision the ground truth is sparse
 

407
00:10:06,150 --> 00:10:08,600
supervision the ground truth is sparse
you can follow that information back

408
00:10:08,600 --> 00:10:08,610
you can follow that information back
 

409
00:10:08,610 --> 00:10:11,420
you can follow that information back
through time to infer something about

410
00:10:11,420 --> 00:10:11,430
through time to infer something about
 

411
00:10:11,430 --> 00:10:13,460
through time to infer something about
the reality of what happened before then

412
00:10:13,460 --> 00:10:13,470
the reality of what happened before then
 

413
00:10:13,470 --> 00:10:16,760
the reality of what happened before then
even if your reward signals were weak so

414
00:10:16,760 --> 00:10:16,770
even if your reward signals were weak so
 

415
00:10:16,770 --> 00:10:18,620
even if your reward signals were weak so
it's using the fact that the physical

416
00:10:18,620 --> 00:10:18,630
it's using the fact that the physical
 

417
00:10:18,630 --> 00:10:22,280
it's using the fact that the physical
world evolves through time and some some

418
00:10:22,280 --> 00:10:22,290
world evolves through time and some some
 

419
00:10:22,290 --> 00:10:25,220
world evolves through time and some some
sort of predictable way to take sparse

420
00:10:25,220 --> 00:10:25,230
sort of predictable way to take sparse
 

421
00:10:25,230 --> 00:10:29,150
sort of predictable way to take sparse
information and generalize it over the

422
00:10:29,150 --> 00:10:29,160
information and generalize it over the
 

423
00:10:29,160 --> 00:10:31,190
information and generalize it over the
entirety of the experience as being

424
00:10:31,190 --> 00:10:31,200
entirety of the experience as being
 

425
00:10:31,200 --> 00:10:33,769
entirety of the experience as being
learned so we apply this the two

426
00:10:33,769 --> 00:10:33,779
learned so we apply this the two
 

427
00:10:33,779 --> 00:10:36,110
learned so we apply this the two
problems today we'll talk about deep

428
00:10:36,110 --> 00:10:36,120
problems today we'll talk about deep
 

429
00:10:36,120 --> 00:10:40,790
problems today we'll talk about deep
traffic as a methodology deep

430
00:10:40,790 --> 00:10:40,800
traffic as a methodology deep
 

431
00:10:40,800 --> 00:10:42,980
traffic as a methodology deep
reinforcement learning so deep traffic

432
00:10:42,980 --> 00:10:42,990
reinforcement learning so deep traffic
 

433
00:10:42,990 --> 00:10:46,130
reinforcement learning so deep traffic
is a competition that we ran last year

434
00:10:46,130 --> 00:10:46,140
is a competition that we ran last year
 

435
00:10:46,140 --> 00:10:49,610
is a competition that we ran last year
and expanded significantly this year and

436
00:10:49,610 --> 00:10:49,620
and expanded significantly this year and
 

437
00:10:49,620 --> 00:10:51,290
and expanded significantly this year and
I'll talk about some of the details and

438
00:10:51,290 --> 00:10:51,300
I'll talk about some of the details and
 

439
00:10:51,300 --> 00:10:54,320
I'll talk about some of the details and
how the folks in this room can on your

440
00:10:54,320 --> 00:10:54,330
how the folks in this room can on your
 

441
00:10:54,330 --> 00:10:56,300
how the folks in this room can on your
smart phone today or if you have a

442
00:10:56,300 --> 00:10:56,310
smart phone today or if you have a
 

443
00:10:56,310 --> 00:10:59,890
smart phone today or if you have a
laptop training agent while I'm talking

444
00:10:59,890 --> 00:10:59,900
laptop training agent while I'm talking
 

445
00:10:59,900 --> 00:11:01,930
laptop training agent while I'm talking
training a neural network in the browser

446
00:11:01,930 --> 00:11:01,940
training a neural network in the browser
 

447
00:11:01,940 --> 00:11:04,910
training a neural network in the browser
some of the things we've added our we've

448
00:11:04,910 --> 00:11:04,920
some of the things we've added our we've
 

449
00:11:04,920 --> 00:11:07,700
some of the things we've added our we've
added the capability we've now turned it

450
00:11:07,700 --> 00:11:07,710
added the capability we've now turned it
 

451
00:11:07,710 --> 00:11:09,470
added the capability we've now turned it
into a multi agent deeper enforcement

452
00:11:09,470 --> 00:11:09,480
into a multi agent deeper enforcement
 

453
00:11:09,480 --> 00:11:11,360
into a multi agent deeper enforcement
learning problem where you can control

454
00:11:11,360 --> 00:11:11,370
learning problem where you can control
 

455
00:11:11,370 --> 00:11:14,560
learning problem where you can control
up to ten cars within your network

456
00:11:14,560 --> 00:11:14,570
up to ten cars within your network
 

457
00:11:14,570 --> 00:11:18,110
up to ten cars within your network
perhaps less significant but pretty cool

458
00:11:18,110 --> 00:11:18,120
perhaps less significant but pretty cool
 

459
00:11:18,120 --> 00:11:21,320
perhaps less significant but pretty cool
is the ability to customize the way the

460
00:11:21,320 --> 00:11:21,330
is the ability to customize the way the
 

461
00:11:21,330 --> 00:11:24,890
is the ability to customize the way the
agent looks so you can upload and people

462
00:11:24,890 --> 00:11:24,900
agent looks so you can upload and people
 

463
00:11:24,900 --> 00:11:27,230
agent looks so you can upload and people
have to an absurd degree have already

464
00:11:27,230 --> 00:11:27,240
have to an absurd degree have already
 

465
00:11:27,240 --> 00:11:29,480
have to an absurd degree have already
begun doing so uploading different

466
00:11:29,480 --> 00:11:29,490
begun doing so uploading different
 

467
00:11:29,490 --> 00:11:31,460
begun doing so uploading different
images instead of the car that's shown

468
00:11:31,460 --> 00:11:31,470
images instead of the car that's shown
 

469
00:11:31,470 --> 00:11:34,280
images instead of the car that's shown
there as long as it maintains the

470
00:11:34,280 --> 00:11:34,290
there as long as it maintains the
 

471
00:11:34,290 --> 00:11:38,020
there as long as it maintains the
dimensions shown here is a SpaceX rocket

472
00:11:38,020 --> 00:11:38,030
dimensions shown here is a SpaceX rocket
 

473
00:11:38,030 --> 00:11:41,740
dimensions shown here is a SpaceX rocket
the competition is hosted on the website

474
00:11:41,740 --> 00:11:41,750
the competition is hosted on the website
 

475
00:11:41,750 --> 00:11:44,420
the competition is hosted on the website
self-driving cars that MIT ID you slash

476
00:11:44,420 --> 00:11:44,430
self-driving cars that MIT ID you slash
 

477
00:11:44,430 --> 00:11:47,680
self-driving cars that MIT ID you slash
deep traffic will return to this later

478
00:11:47,680 --> 00:11:47,690
deep traffic will return to this later
 

479
00:11:47,690 --> 00:11:51,020
deep traffic will return to this later
the code is on github with some more

480
00:11:51,020 --> 00:11:51,030
the code is on github with some more
 

481
00:11:51,030 --> 00:11:54,440
the code is on github with some more
information a starter code and a paper

482
00:11:54,440 --> 00:11:54,450
information a starter code and a paper
 

483
00:11:54,450 --> 00:11:56,030
information a starter code and a paper
describing some of the fundamental

484
00:11:56,030 --> 00:11:56,040
describing some of the fundamental
 

485
00:11:56,040 --> 00:12:00,050
describing some of the fundamental
insights that will help you win at this

486
00:12:00,050 --> 00:12:00,060
insights that will help you win at this
 

487
00:12:00,060 --> 00:12:03,400
insights that will help you win at this
competition is an archive

488
00:12:03,400 --> 00:12:03,410
competition is an archive
 

489
00:12:03,410 --> 00:12:06,320
competition is an archive
so from supervised learning in lecture

490
00:12:06,320 --> 00:12:06,330
so from supervised learning in lecture
 

491
00:12:06,330 --> 00:12:10,460
so from supervised learning in lecture
one to today supervised learning we can

492
00:12:10,460 --> 00:12:10,470
one to today supervised learning we can
 

493
00:12:10,470 --> 00:12:15,050
one to today supervised learning we can
think of as memorization of ground truth

494
00:12:15,050 --> 00:12:15,060
think of as memorization of ground truth
 

495
00:12:15,060 --> 00:12:17,120
think of as memorization of ground truth
data in order to form representations

496
00:12:17,120 --> 00:12:17,130
data in order to form representations
 

497
00:12:17,130 --> 00:12:20,350
data in order to form representations
that generalizes from that ground truth

498
00:12:20,350 --> 00:12:20,360
that generalizes from that ground truth
 

499
00:12:20,360 --> 00:12:23,570
that generalizes from that ground truth
reinforcement learning is we can think

500
00:12:23,570 --> 00:12:23,580
reinforcement learning is we can think
 

501
00:12:23,580 --> 00:12:26,120
reinforcement learning is we can think
of as a way to brute force propagate

502
00:12:26,120 --> 00:12:26,130
of as a way to brute force propagate
 

503
00:12:26,130 --> 00:12:30,400
of as a way to brute force propagate
that information the sparse information

504
00:12:30,400 --> 00:12:30,410
that information the sparse information
 

505
00:12:30,410 --> 00:12:37,880
that information the sparse information
through time to to assign quality reward

506
00:12:37,880 --> 00:12:37,890
through time to to assign quality reward
 

507
00:12:37,890 --> 00:12:41,270
through time to to assign quality reward
to state that does not directly have a

508
00:12:41,270 --> 00:12:41,280
to state that does not directly have a
 

509
00:12:41,280 --> 00:12:45,080
to state that does not directly have a
reward to make sense of this world when

510
00:12:45,080 --> 00:12:45,090
reward to make sense of this world when
 

511
00:12:45,090 --> 00:12:47,930
reward to make sense of this world when
the rewards are sparse but are connected

512
00:12:47,930 --> 00:12:47,940
the rewards are sparse but are connected
 

513
00:12:47,940 --> 00:12:50,330
the rewards are sparse but are connected
through time you can think of that as

514
00:12:50,330 --> 00:12:50,340
through time you can think of that as
 

515
00:12:50,340 --> 00:12:55,070
through time you can think of that as
reasoning so the

516
00:12:55,070 --> 00:12:55,080
reasoning so the
 

517
00:12:55,080 --> 00:12:59,450
reasoning so the
through time is modeled in most

518
00:12:59,450 --> 00:12:59,460
through time is modeled in most
 

519
00:12:59,460 --> 00:13:02,270
through time is modeled in most
reinforcement learning approaches very

520
00:13:02,270 --> 00:13:02,280
reinforcement learning approaches very
 

521
00:13:02,280 --> 00:13:05,420
reinforcement learning approaches very
simply that there's an agent taking an

522
00:13:05,420 --> 00:13:05,430
simply that there's an agent taking an
 

523
00:13:05,430 --> 00:13:07,790
simply that there's an agent taking an
action in a state and receiving a little

524
00:13:07,790 --> 00:13:07,800
action in a state and receiving a little
 

525
00:13:07,800 --> 00:13:10,370
action in a state and receiving a little
reward and the agent operating in an

526
00:13:10,370 --> 00:13:10,380
reward and the agent operating in an
 

527
00:13:10,380 --> 00:13:13,310
reward and the agent operating in an
environment execute an action receives

528
00:13:13,310 --> 00:13:13,320
environment execute an action receives
 

529
00:13:13,320 --> 00:13:15,350
environment execute an action receives
an observed state and new state and

530
00:13:15,350 --> 00:13:15,360
an observed state and new state and
 

531
00:13:15,360 --> 00:13:17,630
an observed state and new state and
receives their reward this process

532
00:13:17,630 --> 00:13:17,640
receives their reward this process
 

533
00:13:17,640 --> 00:13:22,970
receives their reward this process
continues over and over and some

534
00:13:22,970 --> 00:13:22,980
continues over and over and some
 

535
00:13:22,980 --> 00:13:25,910
continues over and over and some
examples we can think of any of the

536
00:13:25,910 --> 00:13:25,920
examples we can think of any of the
 

537
00:13:25,920 --> 00:13:27,530
examples we can think of any of the
video games some of which we'll talk

538
00:13:27,530 --> 00:13:27,540
video games some of which we'll talk
 

539
00:13:27,540 --> 00:13:31,520
video games some of which we'll talk
about today like Atari breakout as the

540
00:13:31,520 --> 00:13:31,530
about today like Atari breakout as the
 

541
00:13:31,530 --> 00:13:36,800
about today like Atari breakout as the
environment the agent is the paddle each

542
00:13:36,800 --> 00:13:36,810
environment the agent is the paddle each
 

543
00:13:36,810 --> 00:13:40,070
environment the agent is the paddle each
action that the agent takes has an

544
00:13:40,070 --> 00:13:40,080
action that the agent takes has an
 

545
00:13:40,080 --> 00:13:43,040
action that the agent takes has an
influence on the evolution of the

546
00:13:43,040 --> 00:13:43,050
influence on the evolution of the
 

547
00:13:43,050 --> 00:13:45,710
influence on the evolution of the
environment and the success is measured

548
00:13:45,710 --> 00:13:45,720
environment and the success is measured
 

549
00:13:45,720 --> 00:13:48,920
environment and the success is measured
by some reward mechanism in this case

550
00:13:48,920 --> 00:13:48,930
by some reward mechanism in this case
 

551
00:13:48,930 --> 00:13:51,770
by some reward mechanism in this case
points are given by the game and every

552
00:13:51,770 --> 00:13:51,780
points are given by the game and every
 

553
00:13:51,780 --> 00:13:55,730
points are given by the game and every
game has a different point scheme that

554
00:13:55,730 --> 00:13:55,740
game has a different point scheme that
 

555
00:13:55,740 --> 00:13:58,490
game has a different point scheme that
must be converted normalized into a way

556
00:13:58,490 --> 00:13:58,500
must be converted normalized into a way
 

557
00:13:58,500 --> 00:14:01,220
must be converted normalized into a way
that's interpreted by the system and the

558
00:14:01,220 --> 00:14:01,230
that's interpreted by the system and the
 

559
00:14:01,230 --> 00:14:03,080
that's interpreted by the system and the
goal is to maximize those points

560
00:14:03,080 --> 00:14:03,090
goal is to maximize those points
 

561
00:14:03,090 --> 00:14:09,020
goal is to maximize those points
maximize the reward the continuous

562
00:14:09,020 --> 00:14:09,030
maximize the reward the continuous
 

563
00:14:09,030 --> 00:14:11,030
maximize the reward the continuous
problem of card pole by balancing the

564
00:14:11,030 --> 00:14:11,040
problem of card pole by balancing the
 

565
00:14:11,040 --> 00:14:12,650
problem of card pole by balancing the
goal is to balance the pole on top of a

566
00:14:12,650 --> 00:14:12,660
goal is to balance the pole on top of a
 

567
00:14:12,660 --> 00:14:15,980
goal is to balance the pole on top of a
moving cart the state is the angle the

568
00:14:15,980 --> 00:14:15,990
moving cart the state is the angle the
 

569
00:14:15,990 --> 00:14:18,320
moving cart the state is the angle the
angular speed the position of horizontal

570
00:14:18,320 --> 00:14:18,330
angular speed the position of horizontal
 

571
00:14:18,330 --> 00:14:21,200
angular speed the position of horizontal
velocity the actions are the horizontal

572
00:14:21,200 --> 00:14:21,210
velocity the actions are the horizontal
 

573
00:14:21,210 --> 00:14:23,750
velocity the actions are the horizontal
force applied to the cart and the reward

574
00:14:23,750 --> 00:14:23,760
force applied to the cart and the reward
 

575
00:14:23,760 --> 00:14:26,000
force applied to the cart and the reward
is one at each time step if the pole is

576
00:14:26,000 --> 00:14:26,010
is one at each time step if the pole is
 

577
00:14:26,010 --> 00:14:28,690
is one at each time step if the pole is
still upright

578
00:14:28,690 --> 00:14:28,700
still upright
 

579
00:14:28,700 --> 00:14:31,150
still upright
all the

580
00:14:31,150 --> 00:14:31,160
all the
 

581
00:14:31,160 --> 00:14:33,310
all the
first-person shooters the video games is

582
00:14:33,310 --> 00:14:33,320
first-person shooters the video games is
 

583
00:14:33,320 --> 00:14:40,690
first-person shooters the video games is
now Starcraft the strategy games in case

584
00:14:40,690 --> 00:14:40,700
now Starcraft the strategy games in case
 

585
00:14:40,700 --> 00:14:43,390
now Starcraft the strategy games in case
of first-person shooter and doom what is

586
00:14:43,390 --> 00:14:43,400
of first-person shooter and doom what is
 

587
00:14:43,400 --> 00:14:45,790
of first-person shooter and doom what is
the goal the environment is the game the

588
00:14:45,790 --> 00:14:45,800
the goal the environment is the game the
 

589
00:14:45,800 --> 00:14:47,950
the goal the environment is the game the
goal is to eliminate all opponents the

590
00:14:47,950 --> 00:14:47,960
goal is to eliminate all opponents the
 

591
00:14:47,960 --> 00:14:50,110
goal is to eliminate all opponents the
state is the raw game pixels coming in

592
00:14:50,110 --> 00:14:50,120
state is the raw game pixels coming in
 

593
00:14:50,120 --> 00:14:53,230
state is the raw game pixels coming in
the actions is moving up down left right

594
00:14:53,230 --> 00:14:53,240
the actions is moving up down left right
 

595
00:14:53,240 --> 00:14:57,310
the actions is moving up down left right
and so on and the reward is positive

596
00:14:57,310 --> 00:14:57,320
and so on and the reward is positive
 

597
00:14:57,320 --> 00:15:00,180
and so on and the reward is positive
when eliminating an opponent and

598
00:15:00,180 --> 00:15:00,190
when eliminating an opponent and
 

599
00:15:00,190 --> 00:15:05,370
when eliminating an opponent and
negative when the agent is eliminated

600
00:15:05,370 --> 00:15:05,380

 

601
00:15:05,380 --> 00:15:09,010

industrial robotics been packin with a

602
00:15:09,010 --> 00:15:09,020
industrial robotics been packin with a
 

603
00:15:09,020 --> 00:15:10,900
industrial robotics been packin with a
robotic arm the goal is to pick up a

604
00:15:10,900 --> 00:15:10,910
robotic arm the goal is to pick up a
 

605
00:15:10,910 --> 00:15:12,940
robotic arm the goal is to pick up a
device from a box and put it into a

606
00:15:12,940 --> 00:15:12,950
device from a box and put it into a
 

607
00:15:12,950 --> 00:15:15,610
device from a box and put it into a
container the state is the raw pixels of

608
00:15:15,610 --> 00:15:15,620
container the state is the raw pixels of
 

609
00:15:15,620 --> 00:15:17,650
container the state is the raw pixels of
the real world that the robot observes

610
00:15:17,650 --> 00:15:17,660
the real world that the robot observes
 

611
00:15:17,660 --> 00:15:21,040
the real world that the robot observes
the actions are the possible actions of

612
00:15:21,040 --> 00:15:21,050
the actions are the possible actions of
 

613
00:15:21,050 --> 00:15:22,510
the actions are the possible actions of
the robot the different degrees of

614
00:15:22,510 --> 00:15:22,520
the robot the different degrees of
 

615
00:15:22,520 --> 00:15:23,890
the robot the different degrees of
freedom are moving through those degrees

616
00:15:23,890 --> 00:15:23,900
freedom are moving through those degrees
 

617
00:15:23,900 --> 00:15:25,900
freedom are moving through those degrees
moving the different actuators to

618
00:15:25,900 --> 00:15:25,910
moving the different actuators to
 

619
00:15:25,910 --> 00:15:28,810
moving the different actuators to
realize of the position of the arm and

620
00:15:28,810 --> 00:15:28,820
realize of the position of the arm and
 

621
00:15:28,820 --> 00:15:30,970
realize of the position of the arm and
the reward is positive when placing a

622
00:15:30,970 --> 00:15:30,980
the reward is positive when placing a
 

623
00:15:30,980 --> 00:15:32,530
the reward is positive when placing a
device successfully and negative

624
00:15:32,530 --> 00:15:32,540
device successfully and negative
 

625
00:15:32,540 --> 00:15:36,370
device successfully and negative
otherwise everything could be modeled in

626
00:15:36,370 --> 00:15:36,380
otherwise everything could be modeled in
 

627
00:15:36,380 --> 00:15:39,880
otherwise everything could be modeled in
this way Markov decision process there's

628
00:15:39,880 --> 00:15:39,890
this way Markov decision process there's
 

629
00:15:39,890 --> 00:15:43,600
this way Markov decision process there's
a state as zero action a zero and reward

630
00:15:43,600 --> 00:15:43,610
a state as zero action a zero and reward
 

631
00:15:43,610 --> 00:15:46,630
a state as zero action a zero and reward
received a new state is achieved again

632
00:15:46,630 --> 00:15:46,640
received a new state is achieved again
 

633
00:15:46,640 --> 00:15:48,880
received a new state is achieved again
action rewards state action rewards

634
00:15:48,880 --> 00:15:48,890
action rewards state action rewards
 

635
00:15:48,890 --> 00:15:52,060
action rewards state action rewards
state until a terminal state is reached

636
00:15:52,060 --> 00:15:52,070
state until a terminal state is reached
 

637
00:15:52,070 --> 00:15:55,500
state until a terminal state is reached
and the major components of

638
00:15:55,500 --> 00:15:55,510
and the major components of
 

639
00:15:55,510 --> 00:15:59,380
and the major components of
reinforcement learning is a policy some

640
00:15:59,380 --> 00:15:59,390
reinforcement learning is a policy some
 

641
00:15:59,390 --> 00:16:01,000
reinforcement learning is a policy some
kind of plan of what to do in every

642
00:16:01,000 --> 00:16:01,010
kind of plan of what to do in every
 

643
00:16:01,010 --> 00:16:02,980
kind of plan of what to do in every
single state what kind of action to

644
00:16:02,980 --> 00:16:02,990
single state what kind of action to
 

645
00:16:02,990 --> 00:16:08,530
single state what kind of action to
perform a value function a some kind of

646
00:16:08,530 --> 00:16:08,540
perform a value function a some kind of
 

647
00:16:08,540 --> 00:16:11,200
perform a value function a some kind of
sense of what is a good state to be in

648
00:16:11,200 --> 00:16:11,210
sense of what is a good state to be in
 

649
00:16:11,210 --> 00:16:13,390
sense of what is a good state to be in
of what is a good action to take in a

650
00:16:13,390 --> 00:16:13,400
of what is a good action to take in a
 

651
00:16:13,400 --> 00:16:19,630
of what is a good action to take in a
state and sometimes a model that the

652
00:16:19,630 --> 00:16:19,640
state and sometimes a model that the
 

653
00:16:19,640 --> 00:16:21,370
state and sometimes a model that the
agent represents the environment with

654
00:16:21,370 --> 00:16:21,380
agent represents the environment with
 

655
00:16:21,380 --> 00:16:24,550
agent represents the environment with
some kind of sense of the environment

656
00:16:24,550 --> 00:16:24,560
some kind of sense of the environment
 

657
00:16:24,560 --> 00:16:26,050
some kind of sense of the environment
its operating in the dynamics of that

658
00:16:26,050 --> 00:16:26,060
its operating in the dynamics of that
 

659
00:16:26,060 --> 00:16:28,510
its operating in the dynamics of that
environment that's useful for making

660
00:16:28,510 --> 00:16:28,520
environment that's useful for making
 

661
00:16:28,520 --> 00:16:31,300
environment that's useful for making
decisions about actions let's take a

662
00:16:31,300 --> 00:16:31,310
decisions about actions let's take a
 

663
00:16:31,310 --> 00:16:35,400
decisions about actions let's take a
trivial example

664
00:16:35,400 --> 00:16:35,410

 

665
00:16:35,410 --> 00:16:38,340

a grid world of three by four twelve

666
00:16:38,340 --> 00:16:38,350
a grid world of three by four twelve
 

667
00:16:38,350 --> 00:16:41,749
a grid world of three by four twelve
squares we start at the bottom left and

668
00:16:41,749 --> 00:16:41,759
squares we start at the bottom left and
 

669
00:16:41,759 --> 00:16:44,879
squares we start at the bottom left and
their task with walking about this world

670
00:16:44,879 --> 00:16:44,889
their task with walking about this world
 

671
00:16:44,889 --> 00:16:48,569
their task with walking about this world
to maximize reward they're awarded at

672
00:16:48,569 --> 00:16:48,579
to maximize reward they're awarded at
 

673
00:16:48,579 --> 00:16:51,119
to maximize reward they're awarded at
the top right is a plus 1 and a 1 square

674
00:16:51,119 --> 00:16:51,129
the top right is a plus 1 and a 1 square
 

675
00:16:51,129 --> 00:16:53,100
the top right is a plus 1 and a 1 square
below that is a negative 1 and every

676
00:16:53,100 --> 00:16:53,110
below that is a negative 1 and every
 

677
00:16:53,110 --> 00:16:56,340
below that is a negative 1 and every
step you take is a punishment or is a

678
00:16:56,340 --> 00:16:56,350
step you take is a punishment or is a
 

679
00:16:56,350 --> 00:17:00,600
step you take is a punishment or is a
negative reward of 0.04 so what is the

680
00:17:00,600 --> 00:17:00,610
negative reward of 0.04 so what is the
 

681
00:17:00,610 --> 00:17:04,710
negative reward of 0.04 so what is the
optimal policy in this world now when

682
00:17:04,710 --> 00:17:04,720
optimal policy in this world now when
 

683
00:17:04,720 --> 00:17:07,829
optimal policy in this world now when
everything is deterministic perhaps this

684
00:17:07,829 --> 00:17:07,839
everything is deterministic perhaps this
 

685
00:17:07,839 --> 00:17:11,069
everything is deterministic perhaps this
is the policy when you start the bottom

686
00:17:11,069 --> 00:17:11,079
is the policy when you start the bottom
 

687
00:17:11,079 --> 00:17:14,520
is the policy when you start the bottom
left well because every step hurts every

688
00:17:14,520 --> 00:17:14,530
left well because every step hurts every
 

689
00:17:14,530 --> 00:17:15,929
left well because every step hurts every
step has a negative reward

690
00:17:15,929 --> 00:17:15,939
step has a negative reward
 

691
00:17:15,939 --> 00:17:17,909
step has a negative reward
then you want to take the shortest path

692
00:17:17,909 --> 00:17:17,919
then you want to take the shortest path
 

693
00:17:17,919 --> 00:17:20,159
then you want to take the shortest path
to the maximum square with a maximum

694
00:17:20,159 --> 00:17:20,169
to the maximum square with a maximum
 

695
00:17:20,169 --> 00:17:22,949
to the maximum square with a maximum
reward when the state space is

696
00:17:22,949 --> 00:17:22,959
reward when the state space is
 

697
00:17:22,959 --> 00:17:27,449
reward when the state space is
non-deterministic as presented before

698
00:17:27,449 --> 00:17:27,459
non-deterministic as presented before
 

699
00:17:27,459 --> 00:17:31,049
non-deterministic as presented before
with a probability of 0.8 when you

700
00:17:31,049 --> 00:17:31,059
with a probability of 0.8 when you
 

701
00:17:31,059 --> 00:17:33,360
with a probability of 0.8 when you
choose to go up you go up but with

702
00:17:33,360 --> 00:17:33,370
choose to go up you go up but with
 

703
00:17:33,370 --> 00:17:37,200
choose to go up you go up but with
probability 0.1 you go left and point 1

704
00:17:37,200 --> 00:17:37,210
probability 0.1 you go left and point 1
 

705
00:17:37,210 --> 00:17:41,659
probability 0.1 you go left and point 1
you go right unfair again much like life

706
00:17:41,659 --> 00:17:41,669
you go right unfair again much like life
 

707
00:17:41,669 --> 00:17:46,260
you go right unfair again much like life
that would be the optimal policy what is

708
00:17:46,260 --> 00:17:46,270
that would be the optimal policy what is
 

709
00:17:46,270 --> 00:17:48,779
that would be the optimal policy what is
the Keith observation here that every

710
00:17:48,779 --> 00:17:48,789
the Keith observation here that every
 

711
00:17:48,789 --> 00:17:50,669
the Keith observation here that every
single state in the space must have a

712
00:17:50,669 --> 00:17:50,679
single state in the space must have a
 

713
00:17:50,679 --> 00:17:54,750
single state in the space must have a
plan because you can't because then a

714
00:17:54,750 --> 00:17:54,760
plan because you can't because then a
 

715
00:17:54,760 --> 00:17:57,779
plan because you can't because then a
non-deterministic aspect of the control

716
00:17:57,779 --> 00:17:57,789
non-deterministic aspect of the control
 

717
00:17:57,789 --> 00:18:00,029
non-deterministic aspect of the control
you can't control where you're going to

718
00:18:00,029 --> 00:18:00,039
you can't control where you're going to
 

719
00:18:00,039 --> 00:18:01,710
you can't control where you're going to
end up so you must have a plan for every

720
00:18:01,710 --> 00:18:01,720
end up so you must have a plan for every
 

721
00:18:01,720 --> 00:18:04,860
end up so you must have a plan for every
place that's the policy having an action

722
00:18:04,860 --> 00:18:04,870
place that's the policy having an action
 

723
00:18:04,870 --> 00:18:06,600
place that's the policy having an action
an optimal action to take in every

724
00:18:06,600 --> 00:18:06,610
an optimal action to take in every
 

725
00:18:06,610 --> 00:18:09,480
an optimal action to take in every
single state now suppose we change the

726
00:18:09,480 --> 00:18:09,490
single state now suppose we change the
 

727
00:18:09,490 --> 00:18:11,700
single state now suppose we change the
reward structure and for every step we

728
00:18:11,700 --> 00:18:11,710
reward structure and for every step we
 

729
00:18:11,710 --> 00:18:14,640
reward structure and for every step we
take there's a negative reward is a

730
00:18:14,640 --> 00:18:14,650
take there's a negative reward is a
 

731
00:18:14,650 --> 00:18:17,220
take there's a negative reward is a
negative 2 so it really hurts there's a

732
00:18:17,220 --> 00:18:17,230
negative 2 so it really hurts there's a
 

733
00:18:17,230 --> 00:18:18,990
negative 2 so it really hurts there's a
high punishment for every single step we

734
00:18:18,990 --> 00:18:19,000
high punishment for every single step we
 

735
00:18:19,000 --> 00:18:22,620
high punishment for every single step we
take so no matter what we always take

736
00:18:22,620 --> 00:18:22,630
take so no matter what we always take
 

737
00:18:22,630 --> 00:18:24,510
take so no matter what we always take
the shortest path the optimal policy is

738
00:18:24,510 --> 00:18:24,520
the shortest path the optimal policy is
 

739
00:18:24,520 --> 00:18:27,270
the shortest path the optimal policy is
to take the shortest path to the to the

740
00:18:27,270 --> 00:18:27,280
to take the shortest path to the to the
 

741
00:18:27,280 --> 00:18:30,360
to take the shortest path to the to the
only spot on the board that doesn't

742
00:18:30,360 --> 00:18:30,370
only spot on the board that doesn't
 

743
00:18:30,370 --> 00:18:35,700
only spot on the board that doesn't
result in punishment if we decrease the

744
00:18:35,700 --> 00:18:35,710
result in punishment if we decrease the
 

745
00:18:35,710 --> 00:18:39,960
result in punishment if we decrease the
reward of each step to negative 0.1 the

746
00:18:39,960 --> 00:18:39,970
reward of each step to negative 0.1 the
 

747
00:18:39,970 --> 00:18:43,420
reward of each step to negative 0.1 the
policy changes whether

748
00:18:43,420 --> 00:18:43,430
policy changes whether
 

749
00:18:43,430 --> 00:18:46,020
policy changes whether
some extra degree of wandering

750
00:18:46,020 --> 00:18:46,030
some extra degree of wandering
 

751
00:18:46,030 --> 00:18:49,990
some extra degree of wandering
encouraged and as we go further and

752
00:18:49,990 --> 00:18:50,000
encouraged and as we go further and
 

753
00:18:50,000 --> 00:18:52,270
encouraged and as we go further and
further in lowering the punishment as

754
00:18:52,270 --> 00:18:52,280
further in lowering the punishment as
 

755
00:18:52,280 --> 00:18:56,200
further in lowering the punishment as
before to negative 0.04 more wandering

756
00:18:56,200 --> 00:18:56,210
before to negative 0.04 more wandering
 

757
00:18:56,210 --> 00:19:00,550
before to negative 0.04 more wandering
and more wandering is allowed and when

758
00:19:00,550 --> 00:19:00,560
and more wandering is allowed and when
 

759
00:19:00,560 --> 00:19:06,220
and more wandering is allowed and when
we finally turn the reward into positive

760
00:19:06,220 --> 00:19:06,230
we finally turn the reward into positive
 

761
00:19:06,230 --> 00:19:12,640
we finally turn the reward into positive
so every step it every step is increases

762
00:19:12,640 --> 00:19:12,650
so every step it every step is increases
 

763
00:19:12,650 --> 00:19:15,160
so every step it every step is increases
the reward then there's a significant

764
00:19:15,160 --> 00:19:15,170
the reward then there's a significant
 

765
00:19:15,170 --> 00:19:18,160
the reward then there's a significant
incentive to to stay on the board

766
00:19:18,160 --> 00:19:18,170
incentive to to stay on the board
 

767
00:19:18,170 --> 00:19:21,780
incentive to to stay on the board
without ever reaching the destination

768
00:19:21,780 --> 00:19:21,790

 

769
00:19:21,790 --> 00:19:27,750

kind of like college for a lot of people

770
00:19:27,750 --> 00:19:27,760

 

771
00:19:27,760 --> 00:19:30,730

so the value function the way we think

772
00:19:30,730 --> 00:19:30,740
so the value function the way we think
 

773
00:19:30,740 --> 00:19:34,330
so the value function the way we think
about the value of a state or the value

774
00:19:34,330 --> 00:19:34,340
about the value of a state or the value
 

775
00:19:34,340 --> 00:19:38,620
about the value of a state or the value
of anything in the environment is the

776
00:19:38,620 --> 00:19:38,630
of anything in the environment is the
 

777
00:19:38,630 --> 00:19:41,350
of anything in the environment is the
reward were likely to receive in the

778
00:19:41,350 --> 00:19:41,360
reward were likely to receive in the
 

779
00:19:41,360 --> 00:19:44,500
reward were likely to receive in the
future and the way we see the reward

780
00:19:44,500 --> 00:19:44,510
future and the way we see the reward
 

781
00:19:44,510 --> 00:19:47,710
future and the way we see the reward
were likely to receive as we discount

782
00:19:47,710 --> 00:19:47,720
were likely to receive as we discount
 

783
00:19:47,720 --> 00:19:51,130
were likely to receive as we discount
the future award because we can't always

784
00:19:51,130 --> 00:19:51,140
the future award because we can't always
 

785
00:19:51,140 --> 00:19:52,920
the future award because we can't always
count on it

786
00:19:52,920 --> 00:19:52,930
count on it
 

787
00:19:52,930 --> 00:19:56,590
count on it
here Gama further and further out into

788
00:19:56,590 --> 00:19:56,600
here Gama further and further out into
 

789
00:19:56,600 --> 00:19:59,170
here Gama further and further out into
the future more and more discounts

790
00:19:59,170 --> 00:19:59,180
the future more and more discounts
 

791
00:19:59,180 --> 00:20:02,440
the future more and more discounts
decreases the reward the importance of

792
00:20:02,440 --> 00:20:02,450
decreases the reward the importance of
 

793
00:20:02,450 --> 00:20:05,050
decreases the reward the importance of
the reward received and the good

794
00:20:05,050 --> 00:20:05,060
the reward received and the good
 

795
00:20:05,060 --> 00:20:07,210
the reward received and the good
strategy is taking the sum of these

796
00:20:07,210 --> 00:20:07,220
strategy is taking the sum of these
 

797
00:20:07,220 --> 00:20:09,910
strategy is taking the sum of these
rewards and maximizing it maximizing the

798
00:20:09,910 --> 00:20:09,920
rewards and maximizing it maximizing the
 

799
00:20:09,920 --> 00:20:11,230
rewards and maximizing it maximizing the
scoundrel ward

800
00:20:11,230 --> 00:20:11,240
scoundrel ward
 

801
00:20:11,240 --> 00:20:13,750
scoundrel ward
that's what reinforcement learning hopes

802
00:20:13,750 --> 00:20:13,760
that's what reinforcement learning hopes
 

803
00:20:13,760 --> 00:20:19,500
that's what reinforcement learning hopes
to achieve and with cue learning we use

804
00:20:19,500 --> 00:20:19,510
to achieve and with cue learning we use
 

805
00:20:19,510 --> 00:20:23,650
to achieve and with cue learning we use
any policy to estimate the value of

806
00:20:23,650 --> 00:20:23,660
any policy to estimate the value of
 

807
00:20:23,660 --> 00:20:28,510
any policy to estimate the value of
taking an action in a state so off

808
00:20:28,510 --> 00:20:28,520
taking an action in a state so off
 

809
00:20:28,520 --> 00:20:32,290
taking an action in a state so off
policy forget policy we move about the

810
00:20:32,290 --> 00:20:32,300
policy forget policy we move about the
 

811
00:20:32,300 --> 00:20:34,540
policy forget policy we move about the
world and use the bellman equation here

812
00:20:34,540 --> 00:20:34,550
world and use the bellman equation here
 

813
00:20:34,550 --> 00:20:37,240
world and use the bellman equation here
on the bottom to continuously update our

814
00:20:37,240 --> 00:20:37,250
on the bottom to continuously update our
 

815
00:20:37,250 --> 00:20:39,640
on the bottom to continuously update our
estimate of how good a certain action is

816
00:20:39,640 --> 00:20:39,650
estimate of how good a certain action is
 

817
00:20:39,650 --> 00:20:45,010
estimate of how good a certain action is
in a certain state so we don't need this

818
00:20:45,010 --> 00:20:45,020
in a certain state so we don't need this
 

819
00:20:45,020 --> 00:20:46,510
in a certain state so we don't need this
this allows us to operate in a much

820
00:20:46,510 --> 00:20:46,520
this allows us to operate in a much
 

821
00:20:46,520 --> 00:20:48,010
this allows us to operate in a much
larger state space in a much larger

822
00:20:48,010 --> 00:20:48,020
larger state space in a much larger
 

823
00:20:48,020 --> 00:20:50,590
larger state space in a much larger
action space we move about this world

824
00:20:50,590 --> 00:20:50,600
action space we move about this world
 

825
00:20:50,600 --> 00:20:52,210
action space we move about this world
through simulation or in the real world

826
00:20:52,210 --> 00:20:52,220
through simulation or in the real world
 

827
00:20:52,220 --> 00:20:55,000
through simulation or in the real world
taking actions and updating our estimate

828
00:20:55,000 --> 00:20:55,010
taking actions and updating our estimate
 

829
00:20:55,010 --> 00:20:57,070
taking actions and updating our estimate
of how good certain actions are over

830
00:20:57,070 --> 00:20:57,080
of how good certain actions are over
 

831
00:20:57,080 --> 00:21:01,870
of how good certain actions are over
I'm the new state at the left is the is

832
00:21:01,870 --> 00:21:01,880
I'm the new state at the left is the is
 

833
00:21:01,880 --> 00:21:04,450
I'm the new state at the left is the is
the updated value the old state is the

834
00:21:04,450 --> 00:21:04,460
the updated value the old state is the
 

835
00:21:04,460 --> 00:21:06,130
the updated value the old state is the
starting value for the equation and we

836
00:21:06,130 --> 00:21:06,140
starting value for the equation and we
 

837
00:21:06,140 --> 00:21:08,980
starting value for the equation and we
update that old state estimation with

838
00:21:08,980 --> 00:21:08,990
update that old state estimation with
 

839
00:21:08,990 --> 00:21:13,120
update that old state estimation with
the sum of the reward received by taking

840
00:21:13,120 --> 00:21:13,130
the sum of the reward received by taking
 

841
00:21:13,130 --> 00:21:18,330
the sum of the reward received by taking
action s tax action a and state us and

842
00:21:18,330 --> 00:21:18,340
action s tax action a and state us and
 

843
00:21:18,340 --> 00:21:22,570
action s tax action a and state us and
the maximum reward that's possible to be

844
00:21:22,570 --> 00:21:22,580
the maximum reward that's possible to be
 

845
00:21:22,580 --> 00:21:25,380
the maximum reward that's possible to be
received in the following states

846
00:21:25,380 --> 00:21:25,390
received in the following states
 

847
00:21:25,390 --> 00:21:30,580
received in the following states
discounted that update is decreased with

848
00:21:30,580 --> 00:21:30,590
discounted that update is decreased with
 

849
00:21:30,590 --> 00:21:32,650
discounted that update is decreased with
a learning rate the higher the learning

850
00:21:32,650 --> 00:21:32,660
a learning rate the higher the learning
 

851
00:21:32,660 --> 00:21:35,920
a learning rate the higher the learning
rate the more value we the the faster

852
00:21:35,920 --> 00:21:35,930
rate the more value we the the faster
 

853
00:21:35,930 --> 00:21:38,260
rate the more value we the the faster
will learn the more value we assigned to

854
00:21:38,260 --> 00:21:38,270
will learn the more value we assigned to
 

855
00:21:38,270 --> 00:21:40,990
will learn the more value we assigned to
new information that's simple that's it

856
00:21:40,990 --> 00:21:41,000
new information that's simple that's it
 

857
00:21:41,000 --> 00:21:43,510
new information that's simple that's it
that's Q learning the simple update rule

858
00:21:43,510 --> 00:21:43,520
that's Q learning the simple update rule
 

859
00:21:43,520 --> 00:21:47,560
that's Q learning the simple update rule
allows us to to explore the world and as

860
00:21:47,560 --> 00:21:47,570
allows us to to explore the world and as
 

861
00:21:47,570 --> 00:21:52,000
allows us to to explore the world and as
we explore get more and more information

862
00:21:52,000 --> 00:21:52,010
we explore get more and more information
 

863
00:21:52,010 --> 00:21:53,590
we explore get more and more information
about what's good to do in this world

864
00:21:53,590 --> 00:21:53,600
about what's good to do in this world
 

865
00:21:53,600 --> 00:21:56,140
about what's good to do in this world
and there's always a balance in the

866
00:21:56,140 --> 00:21:56,150
and there's always a balance in the
 

867
00:21:56,150 --> 00:21:58,120
and there's always a balance in the
various problem spaces we'll discuss

868
00:21:58,120 --> 00:21:58,130
various problem spaces we'll discuss
 

869
00:21:58,130 --> 00:21:59,820
various problem spaces we'll discuss
there's always a balance between

870
00:21:59,820 --> 00:21:59,830
there's always a balance between
 

871
00:21:59,830 --> 00:22:05,410
there's always a balance between
exploration and exploitation as you form

872
00:22:05,410 --> 00:22:05,420
exploration and exploitation as you form
 

873
00:22:05,420 --> 00:22:06,910
exploration and exploitation as you form
a better and better estimate of the Q

874
00:22:06,910 --> 00:22:06,920
a better and better estimate of the Q
 

875
00:22:06,920 --> 00:22:08,350
a better and better estimate of the Q
function of what actions are good to

876
00:22:08,350 --> 00:22:08,360
function of what actions are good to
 

877
00:22:08,360 --> 00:22:11,380
function of what actions are good to
take you start to get a sense of what is

878
00:22:11,380 --> 00:22:11,390
take you start to get a sense of what is
 

879
00:22:11,390 --> 00:22:14,140
take you start to get a sense of what is
the best action to take but it's not a

880
00:22:14,140 --> 00:22:14,150
the best action to take but it's not a
 

881
00:22:14,150 --> 00:22:15,520
the best action to take but it's not a
perfect sense it's still an

882
00:22:15,520 --> 00:22:15,530
perfect sense it's still an
 

883
00:22:15,530 --> 00:22:17,530
perfect sense it's still an
approximation and so there's value of

884
00:22:17,530 --> 00:22:17,540
approximation and so there's value of
 

885
00:22:17,540 --> 00:22:19,690
approximation and so there's value of
exploration but the better and better

886
00:22:19,690 --> 00:22:19,700
exploration but the better and better
 

887
00:22:19,700 --> 00:22:21,610
exploration but the better and better
your estimate becomes the less and less

888
00:22:21,610 --> 00:22:21,620
your estimate becomes the less and less
 

889
00:22:21,620 --> 00:22:25,270
your estimate becomes the less and less
exploration has a benefit so usually we

890
00:22:25,270 --> 00:22:25,280
exploration has a benefit so usually we
 

891
00:22:25,280 --> 00:22:26,530
exploration has a benefit so usually we
want to explore a lot in the beginning

892
00:22:26,530 --> 00:22:26,540
want to explore a lot in the beginning
 

893
00:22:26,540 --> 00:22:29,590
want to explore a lot in the beginning
and less and less so towards the end and

894
00:22:29,590 --> 00:22:29,600
and less and less so towards the end and
 

895
00:22:29,600 --> 00:22:31,900
and less and less so towards the end and
when we finally release the system out

896
00:22:31,900 --> 00:22:31,910
when we finally release the system out
 

897
00:22:31,910 --> 00:22:34,270
when we finally release the system out
into the world and wish it to operate

898
00:22:34,270 --> 00:22:34,280
into the world and wish it to operate
 

899
00:22:34,280 --> 00:22:38,140
into the world and wish it to operate
its best then we have it operate as a

900
00:22:38,140 --> 00:22:38,150
its best then we have it operate as a
 

901
00:22:38,150 --> 00:22:40,180
its best then we have it operate as a
greedy system always taking the optimal

902
00:22:40,180 --> 00:22:40,190
greedy system always taking the optimal
 

903
00:22:40,190 --> 00:22:42,460
greedy system always taking the optimal
action according to the q2 key value

904
00:22:42,460 --> 00:22:42,470
action according to the q2 key value
 

905
00:22:42,470 --> 00:22:46,720
action according to the q2 key value
function and everything I'm talking

906
00:22:46,720 --> 00:22:46,730
function and everything I'm talking
 

907
00:22:46,730 --> 00:22:49,690
function and everything I'm talking
about now is permit rised and our

908
00:22:49,690 --> 00:22:49,700
about now is permit rised and our
 

909
00:22:49,700 --> 00:22:53,560
about now is permit rised and our
parameters that are very important for

910
00:22:53,560 --> 00:22:53,570
parameters that are very important for
 

911
00:22:53,570 --> 00:22:56,490
parameters that are very important for
winning the deep traffic competition

912
00:22:56,490 --> 00:22:56,500
winning the deep traffic competition
 

913
00:22:56,500 --> 00:22:59,410
winning the deep traffic competition
which is using this very algorithm with

914
00:22:59,410 --> 00:22:59,420
which is using this very algorithm with
 

915
00:22:59,420 --> 00:23:04,590
which is using this very algorithm with
a neural network at its core so for sin

916
00:23:04,590 --> 00:23:04,600
a neural network at its core so for sin
 

917
00:23:04,600 --> 00:23:07,080
a neural network at its core so for sin
table representation of a cue function

918
00:23:07,080 --> 00:23:07,090
table representation of a cue function
 

919
00:23:07,090 --> 00:23:10,830
table representation of a cue function
where the y-axis is state four states s

920
00:23:10,830 --> 00:23:10,840
where the y-axis is state four states s
 

921
00:23:10,840 --> 00:23:14,420
where the y-axis is state four states s
one two three four and the x-axis is

922
00:23:14,420 --> 00:23:14,430
one two three four and the x-axis is
 

923
00:23:14,430 --> 00:23:18,450
one two three four and the x-axis is
actions a one two three four we can

924
00:23:18,450 --> 00:23:18,460
actions a one two three four we can
 

925
00:23:18,460 --> 00:23:20,760
actions a one two three four we can
think of this table as randomly

926
00:23:20,760 --> 00:23:20,770
think of this table as randomly
 

927
00:23:20,770 --> 00:23:23,430
think of this table as randomly
initiated or initiated initialized in

928
00:23:23,430 --> 00:23:23,440
initiated or initiated initialized in
 

929
00:23:23,440 --> 00:23:25,740
initiated or initiated initialized in
any kind of way that's not

930
00:23:25,740 --> 00:23:25,750
any kind of way that's not
 

931
00:23:25,750 --> 00:23:28,230
any kind of way that's not
representative of actual reality and as

932
00:23:28,230 --> 00:23:28,240
representative of actual reality and as
 

933
00:23:28,240 --> 00:23:29,730
representative of actual reality and as
we move about this world and we take

934
00:23:29,730 --> 00:23:29,740
we move about this world and we take
 

935
00:23:29,740 --> 00:23:31,950
we move about this world and we take
actions we update this table with the

936
00:23:31,950 --> 00:23:31,960
actions we update this table with the
 

937
00:23:31,960 --> 00:23:34,860
actions we update this table with the
bellman equation shown up top and here

938
00:23:34,860 --> 00:23:34,870
bellman equation shown up top and here
 

939
00:23:34,870 --> 00:23:37,260
bellman equation shown up top and here
slides now are online you can see a

940
00:23:37,260 --> 00:23:37,270
slides now are online you can see a
 

941
00:23:37,270 --> 00:23:40,110
slides now are online you can see a
simple pseudocode algorithm of how to

942
00:23:40,110 --> 00:23:40,120
simple pseudocode algorithm of how to
 

943
00:23:40,120 --> 00:23:42,570
simple pseudocode algorithm of how to
update it how to run this bellman

944
00:23:42,570 --> 00:23:42,580
update it how to run this bellman
 

945
00:23:42,580 --> 00:23:46,680
update it how to run this bellman
equation and over time the approximation

946
00:23:46,680 --> 00:23:46,690
equation and over time the approximation
 

947
00:23:46,690 --> 00:23:48,990
equation and over time the approximation
becomes the optimal cue table

948
00:23:48,990 --> 00:23:49,000
becomes the optimal cue table
 

949
00:23:49,000 --> 00:23:51,870
becomes the optimal cue table
the problem is when that cue table it

950
00:23:51,870 --> 00:23:51,880
the problem is when that cue table it
 

951
00:23:51,880 --> 00:23:55,920
the problem is when that cue table it
becomes exponential in size when we take

952
00:23:55,920 --> 00:23:55,930
becomes exponential in size when we take
 

953
00:23:55,930 --> 00:23:58,200
becomes exponential in size when we take
in raw sensory information as we do with

954
00:23:58,200 --> 00:23:58,210
in raw sensory information as we do with
 

955
00:23:58,210 --> 00:24:01,770
in raw sensory information as we do with
cameras with deep crash or with deep

956
00:24:01,770 --> 00:24:01,780
cameras with deep crash or with deep
 

957
00:24:01,780 --> 00:24:04,200
cameras with deep crash or with deep
traffic it's taking the full grid space

958
00:24:04,200 --> 00:24:04,210
traffic it's taking the full grid space
 

959
00:24:04,210 --> 00:24:07,650
traffic it's taking the full grid space
and taking that information the raw the

960
00:24:07,650 --> 00:24:07,660
and taking that information the raw the
 

961
00:24:07,660 --> 00:24:10,920
and taking that information the raw the
raw grid pixels of deep traffic and when

962
00:24:10,920 --> 00:24:10,930
raw grid pixels of deep traffic and when
 

963
00:24:10,930 --> 00:24:13,260
raw grid pixels of deep traffic and when
you take the arcade games here they're

964
00:24:13,260 --> 00:24:13,270
you take the arcade games here they're
 

965
00:24:13,270 --> 00:24:17,340
you take the arcade games here they're
taking the raw pixels of the game or

966
00:24:17,340 --> 00:24:17,350
taking the raw pixels of the game or
 

967
00:24:17,350 --> 00:24:19,890
taking the raw pixels of the game or
when we take go the game of go when it's

968
00:24:19,890 --> 00:24:19,900
when we take go the game of go when it's
 

969
00:24:19,900 --> 00:24:23,610
when we take go the game of go when it's
taking the units the the board the raw

970
00:24:23,610 --> 00:24:23,620
taking the units the the board the raw
 

971
00:24:23,620 --> 00:24:27,980
taking the units the the board the raw
state of the board as the input the

972
00:24:27,980 --> 00:24:27,990
state of the board as the input the
 

973
00:24:27,990 --> 00:24:31,500
state of the board as the input the
potential state space the number of

974
00:24:31,500 --> 00:24:31,510
potential state space the number of
 

975
00:24:31,510 --> 00:24:35,040
potential state space the number of
possible combinations of what states it

976
00:24:35,040 --> 00:24:35,050
possible combinations of what states it
 

977
00:24:35,050 --> 00:24:38,940
possible combinations of what states it
possible is extremely large larger than

978
00:24:38,940 --> 00:24:38,950
possible is extremely large larger than
 

979
00:24:38,950 --> 00:24:40,980
possible is extremely large larger than
we can certainly hold the memory and

980
00:24:40,980 --> 00:24:40,990
we can certainly hold the memory and
 

981
00:24:40,990 --> 00:24:43,950
we can certainly hold the memory and
larger that we can ever be able to

982
00:24:43,950 --> 00:24:43,960
larger that we can ever be able to
 

983
00:24:43,960 --> 00:24:45,660
larger that we can ever be able to
accurately approximate through the

984
00:24:45,660 --> 00:24:45,670
accurately approximate through the
 

985
00:24:45,670 --> 00:24:48,210
accurately approximate through the
bellman equation over time through

986
00:24:48,210 --> 00:24:48,220
bellman equation over time through
 

987
00:24:48,220 --> 00:24:51,990
bellman equation over time through
simulation through the simple update of

988
00:24:51,990 --> 00:24:52,000
simulation through the simple update of
 

989
00:24:52,000 --> 00:24:54,600
simulation through the simple update of
the bellman equation so this is where

990
00:24:54,600 --> 00:24:54,610
the bellman equation so this is where
 

991
00:24:54,610 --> 00:24:57,050
the bellman equation so this is where
deep reinforcement learning comes in

992
00:24:57,050 --> 00:24:57,060
deep reinforcement learning comes in
 

993
00:24:57,060 --> 00:24:59,550
deep reinforcement learning comes in
neural networks are really good

994
00:24:59,550 --> 00:24:59,560
neural networks are really good
 

995
00:24:59,560 --> 00:25:01,590
neural networks are really good
approximate errs they're really good at

996
00:25:01,590 --> 00:25:01,600
approximate errs they're really good at
 

997
00:25:01,600 --> 00:25:04,410
approximate errs they're really good at
exactly this task of learning this kind

998
00:25:04,410 --> 00:25:04,420
exactly this task of learning this kind
 

999
00:25:04,420 --> 00:25:09,030
exactly this task of learning this kind
of cue table

1000
00:25:09,030 --> 00:25:09,040

 

1001
00:25:09,040 --> 00:25:11,200

so as we started with supervised

1002
00:25:11,200 --> 00:25:11,210
so as we started with supervised
 

1003
00:25:11,210 --> 00:25:13,360
so as we started with supervised
learning or neural networks helped us

1004
00:25:13,360 --> 00:25:13,370
learning or neural networks helped us
 

1005
00:25:13,370 --> 00:25:15,450
learning or neural networks helped us
memorize patterns using supervised

1006
00:25:15,450 --> 00:25:15,460
memorize patterns using supervised
 

1007
00:25:15,460 --> 00:25:17,980
memorize patterns using supervised
ground true data and we'll move to

1008
00:25:17,980 --> 00:25:17,990
ground true data and we'll move to
 

1009
00:25:17,990 --> 00:25:19,630
ground true data and we'll move to
reinforcement learning that hopes to

1010
00:25:19,630 --> 00:25:19,640
reinforcement learning that hopes to
 

1011
00:25:19,640 --> 00:25:25,360
reinforcement learning that hopes to
propagate outcomes to knowledge deep

1012
00:25:25,360 --> 00:25:25,370
propagate outcomes to knowledge deep
 

1013
00:25:25,370 --> 00:25:28,299
propagate outcomes to knowledge deep
learning allows us to do so on much

1014
00:25:28,299 --> 00:25:28,309
learning allows us to do so on much
 

1015
00:25:28,309 --> 00:25:30,690
learning allows us to do so on much
larger state spaces are much larger

1016
00:25:30,690 --> 00:25:30,700
larger state spaces are much larger
 

1017
00:25:30,700 --> 00:25:34,780
larger state spaces are much larger
action spaces which means it's

1018
00:25:34,780 --> 00:25:34,790
action spaces which means it's
 

1019
00:25:34,790 --> 00:25:38,380
action spaces which means it's
generalizable it's much more capable to

1020
00:25:38,380 --> 00:25:38,390
generalizable it's much more capable to
 

1021
00:25:38,390 --> 00:25:43,120
generalizable it's much more capable to
deal with the raw stuff of sensory data

1022
00:25:43,120 --> 00:25:43,130
deal with the raw stuff of sensory data
 

1023
00:25:43,130 --> 00:25:46,180
deal with the raw stuff of sensory data
which means it's much more capable to

1024
00:25:46,180 --> 00:25:46,190
which means it's much more capable to
 

1025
00:25:46,190 --> 00:25:47,950
which means it's much more capable to
deal with the broad variation of real

1026
00:25:47,950 --> 00:25:47,960
deal with the broad variation of real
 

1027
00:25:47,960 --> 00:25:55,120
deal with the broad variation of real
world applications and it does so

1028
00:25:55,120 --> 00:25:55,130
world applications and it does so
 

1029
00:25:55,130 --> 00:25:58,030
world applications and it does so
because it's able to learn the

1030
00:25:58,030 --> 00:25:58,040
because it's able to learn the
 

1031
00:25:58,040 --> 00:26:01,090
because it's able to learn the
representations as we discussed on

1032
00:26:01,090 --> 00:26:01,100
representations as we discussed on
 

1033
00:26:01,100 --> 00:26:07,020
representations as we discussed on
Monday the understanding comes from

1034
00:26:07,020 --> 00:26:07,030
Monday the understanding comes from
 

1035
00:26:07,030 --> 00:26:09,390
Monday the understanding comes from
converting the raw sensory information

1036
00:26:09,390 --> 00:26:09,400
converting the raw sensory information
 

1037
00:26:09,400 --> 00:26:12,820
converting the raw sensory information
into into simple useful information

1038
00:26:12,820 --> 00:26:12,830
into into simple useful information
 

1039
00:26:12,830 --> 00:26:15,610
into into simple useful information
based on which the action in this

1040
00:26:15,610 --> 00:26:15,620
based on which the action in this
 

1041
00:26:15,620 --> 00:26:17,440
based on which the action in this
particular state can be taken in the

1042
00:26:17,440 --> 00:26:17,450
particular state can be taken in the
 

1043
00:26:17,450 --> 00:26:19,960
particular state can be taken in the
same exact way so instead of the cue

1044
00:26:19,960 --> 00:26:19,970
same exact way so instead of the cue
 

1045
00:26:19,970 --> 00:26:22,450
same exact way so instead of the cue
table instead of this cue function we

1046
00:26:22,450 --> 00:26:22,460
table instead of this cue function we
 

1047
00:26:22,460 --> 00:26:24,580
table instead of this cue function we
plug in a neural network where the input

1048
00:26:24,580 --> 00:26:24,590
plug in a neural network where the input
 

1049
00:26:24,590 --> 00:26:26,980
plug in a neural network where the input
is the state space no matter how complex

1050
00:26:26,980 --> 00:26:26,990
is the state space no matter how complex
 

1051
00:26:26,990 --> 00:26:30,760
is the state space no matter how complex
and the output is a value for each of

1052
00:26:30,760 --> 00:26:30,770
and the output is a value for each of
 

1053
00:26:30,770 --> 00:26:35,500
and the output is a value for each of
the actions that you could take input is

1054
00:26:35,500 --> 00:26:35,510
the actions that you could take input is
 

1055
00:26:35,510 --> 00:26:38,290
the actions that you could take input is
the state output is the value of the

1056
00:26:38,290 --> 00:26:38,300
the state output is the value of the
 

1057
00:26:38,300 --> 00:26:43,210
the state output is the value of the
function it's simple this is deep Q

1058
00:26:43,210 --> 00:26:43,220
function it's simple this is deep Q
 

1059
00:26:43,220 --> 00:26:47,440
function it's simple this is deep Q
Network DQ one at the core of the

1060
00:26:47,440 --> 00:26:47,450
Network DQ one at the core of the
 

1061
00:26:47,450 --> 00:26:49,630
Network DQ one at the core of the
success of deep mind a lot of the cool

1062
00:26:49,630 --> 00:26:49,640
success of deep mind a lot of the cool
 

1063
00:26:49,640 --> 00:26:51,790
success of deep mind a lot of the cool
stuff you see about video games D

1064
00:26:51,790 --> 00:26:51,800
stuff you see about video games D
 

1065
00:26:51,800 --> 00:26:54,660
stuff you see about video games D
queuing or variants of DQ and our play

1066
00:26:54,660 --> 00:26:54,670
queuing or variants of DQ and our play
 

1067
00:26:54,670 --> 00:26:57,520
queuing or variants of DQ and our play
this is water first with a nature paper

1068
00:26:57,520 --> 00:26:57,530
this is water first with a nature paper
 

1069
00:26:57,530 --> 00:27:03,040
this is water first with a nature paper
a deep mind the success came of playing

1070
00:27:03,040 --> 00:27:03,050
a deep mind the success came of playing
 

1071
00:27:03,050 --> 00:27:05,340
a deep mind the success came of playing
the different games including Atari

1072
00:27:05,340 --> 00:27:05,350
the different games including Atari
 

1073
00:27:05,350 --> 00:27:10,120
the different games including Atari
games

1074
00:27:10,120 --> 00:27:10,130

 

1075
00:27:10,130 --> 00:27:12,760

how are these things trained very

1076
00:27:12,760 --> 00:27:12,770
how are these things trained very
 

1077
00:27:12,770 --> 00:27:18,850
how are these things trained very
similar to supervised learning the

1078
00:27:18,850 --> 00:27:18,860
similar to supervised learning the
 

1079
00:27:18,860 --> 00:27:20,200
similar to supervised learning the
bellman equation up top

1080
00:27:20,200 --> 00:27:20,210
bellman equation up top
 

1081
00:27:20,210 --> 00:27:25,870
bellman equation up top
it takes the reward and the discounted

1082
00:27:25,870 --> 00:27:25,880
it takes the reward and the discounted
 

1083
00:27:25,880 --> 00:27:32,470
it takes the reward and the discounted
expected reward from future states the

1084
00:27:32,470 --> 00:27:32,480
expected reward from future states the
 

1085
00:27:32,480 --> 00:27:34,510
expected reward from future states the
loss function here for neural network

1086
00:27:34,510 --> 00:27:34,520
loss function here for neural network
 

1087
00:27:34,520 --> 00:27:36,220
loss function here for neural network
and you'll now work learners with a loss

1088
00:27:36,220 --> 00:27:36,230
and you'll now work learners with a loss
 

1089
00:27:36,230 --> 00:27:40,510
and you'll now work learners with a loss
function it takes the reward received at

1090
00:27:40,510 --> 00:27:40,520
function it takes the reward received at
 

1091
00:27:40,520 --> 00:27:43,480
function it takes the reward received at
the current state does a forward pass

1092
00:27:43,480 --> 00:27:43,490
the current state does a forward pass
 

1093
00:27:43,490 --> 00:27:45,340
the current state does a forward pass
through a neural network to estimate the

1094
00:27:45,340 --> 00:27:45,350
through a neural network to estimate the
 

1095
00:27:45,350 --> 00:27:49,410
through a neural network to estimate the
value of the future state of the best

1096
00:27:49,410 --> 00:27:49,420
value of the future state of the best
 

1097
00:27:49,420 --> 00:27:52,410
value of the future state of the best
action to take in the future state and

1098
00:27:52,410 --> 00:27:52,420
action to take in the future state and
 

1099
00:27:52,420 --> 00:27:57,780
action to take in the future state and
then subtract that from the forward pass

1100
00:27:57,780 --> 00:27:57,790
then subtract that from the forward pass
 

1101
00:27:57,790 --> 00:27:59,830
then subtract that from the forward pass
through the network for the current

1102
00:27:59,830 --> 00:27:59,840
through the network for the current
 

1103
00:27:59,840 --> 00:28:02,680
through the network for the current
state in action so you take the

1104
00:28:02,680 --> 00:28:02,690
state in action so you take the
 

1105
00:28:02,690 --> 00:28:05,610
state in action so you take the
difference between what your a Q

1106
00:28:05,610 --> 00:28:05,620
difference between what your a Q
 

1107
00:28:05,620 --> 00:28:06,730
difference between what your a Q
estimator

1108
00:28:06,730 --> 00:28:06,740
estimator
 

1109
00:28:06,740 --> 00:28:09,520
estimator
then you'll network believes the value

1110
00:28:09,520 --> 00:28:09,530
then you'll network believes the value
 

1111
00:28:09,530 --> 00:28:14,050
then you'll network believes the value
of the current state is and what it more

1112
00:28:14,050 --> 00:28:14,060
of the current state is and what it more
 

1113
00:28:14,060 --> 00:28:17,950
of the current state is and what it more
likely is to be based on the value of

1114
00:28:17,950 --> 00:28:17,960
likely is to be based on the value of
 

1115
00:28:17,960 --> 00:28:19,870
likely is to be based on the value of
the future states that are reachable

1116
00:28:19,870 --> 00:28:19,880
the future states that are reachable
 

1117
00:28:19,880 --> 00:28:26,860
the future states that are reachable
based on the actions you can take here's

1118
00:28:26,860 --> 00:28:26,870
based on the actions you can take here's
 

1119
00:28:26,870 --> 00:28:31,630
based on the actions you can take here's
the algorithm input is the state output

1120
00:28:31,630 --> 00:28:31,640
the algorithm input is the state output
 

1121
00:28:31,640 --> 00:28:33,700
the algorithm input is the state output
is the Q value for each action or in

1122
00:28:33,700 --> 00:28:33,710
is the Q value for each action or in
 

1123
00:28:33,710 --> 00:28:35,800
is the Q value for each action or in
this diagram input is the state in

1124
00:28:35,800 --> 00:28:35,810
this diagram input is the state in
 

1125
00:28:35,810 --> 00:28:38,070
this diagram input is the state in
action and the output is the Q value

1126
00:28:38,070 --> 00:28:38,080
action and the output is the Q value
 

1127
00:28:38,080 --> 00:28:41,530
action and the output is the Q value
it's very similar architectures so given

1128
00:28:41,530 --> 00:28:41,540
it's very similar architectures so given
 

1129
00:28:41,540 --> 00:28:47,580
it's very similar architectures so given
a transition of s a are s prime s

1130
00:28:47,580 --> 00:28:47,590
a transition of s a are s prime s
 

1131
00:28:47,590 --> 00:28:50,170
a transition of s a are s prime s
current state taking an action receiving

1132
00:28:50,170 --> 00:28:50,180
current state taking an action receiving
 

1133
00:28:50,180 --> 00:28:56,360
current state taking an action receiving
reward and achieving US prime state the

1134
00:28:56,360 --> 00:28:56,370
reward and achieving US prime state the
 

1135
00:28:56,370 --> 00:29:00,230
reward and achieving US prime state the
the update is to a feed-forward pass

1136
00:29:00,230 --> 00:29:00,240
the update is to a feed-forward pass
 

1137
00:29:00,240 --> 00:29:01,640
the update is to a feed-forward pass
through the network for the current

1138
00:29:01,640 --> 00:29:01,650
through the network for the current
 

1139
00:29:01,650 --> 00:29:04,790
through the network for the current
state do a feed-forward pass for each of

1140
00:29:04,790 --> 00:29:04,800
state do a feed-forward pass for each of
 

1141
00:29:04,800 --> 00:29:07,730
state do a feed-forward pass for each of
the possible actions taken in the next

1142
00:29:07,730 --> 00:29:07,740
the possible actions taken in the next
 

1143
00:29:07,740 --> 00:29:09,890
the possible actions taken in the next
state and that's how we compute the two

1144
00:29:09,890 --> 00:29:09,900
state and that's how we compute the two
 

1145
00:29:09,900 --> 00:29:13,880
state and that's how we compute the two
parts of the loss function and update

1146
00:29:13,880 --> 00:29:13,890
parts of the loss function and update
 

1147
00:29:13,890 --> 00:29:17,240
parts of the loss function and update
the weights using back propagation again

1148
00:29:17,240 --> 00:29:17,250
the weights using back propagation again
 

1149
00:29:17,250 --> 00:29:19,130
the weights using back propagation again
loss function back propagation is how

1150
00:29:19,130 --> 00:29:19,140
loss function back propagation is how
 

1151
00:29:19,140 --> 00:29:22,910
loss function back propagation is how
the network is trained this has actually

1152
00:29:22,910 --> 00:29:22,920
the network is trained this has actually
 

1153
00:29:22,920 --> 00:29:25,220
the network is trained this has actually
been around for much longer than the

1154
00:29:25,220 --> 00:29:25,230
been around for much longer than the
 

1155
00:29:25,230 --> 00:29:30,260
been around for much longer than the
deep mind a few tricks made it made it

1156
00:29:30,260 --> 00:29:30,270
deep mind a few tricks made it made it
 

1157
00:29:30,270 --> 00:29:35,570
deep mind a few tricks made it made it
really work experience replays the

1158
00:29:35,570 --> 00:29:35,580
really work experience replays the
 

1159
00:29:35,580 --> 00:29:40,310
really work experience replays the
biggest one so as the games are played

1160
00:29:40,310 --> 00:29:40,320
biggest one so as the games are played
 

1161
00:29:40,320 --> 00:29:41,960
biggest one so as the games are played
through simulation or if it's a physical

1162
00:29:41,960 --> 00:29:41,970
through simulation or if it's a physical
 

1163
00:29:41,970 --> 00:29:45,680
through simulation or if it's a physical
system as it acts in the world it's

1164
00:29:45,680 --> 00:29:45,690
system as it acts in the world it's
 

1165
00:29:45,690 --> 00:29:49,190
system as it acts in the world it's
actually collecting the observations

1166
00:29:49,190 --> 00:29:49,200
actually collecting the observations
 

1167
00:29:49,200 --> 00:29:52,490
actually collecting the observations
into a library of experiences and that

1168
00:29:52,490 --> 00:29:52,500
into a library of experiences and that
 

1169
00:29:52,500 --> 00:29:54,920
into a library of experiences and that
training is performed by randomly

1170
00:29:54,920 --> 00:29:54,930
training is performed by randomly
 

1171
00:29:54,930 --> 00:29:57,920
training is performed by randomly
sampling the library in the past by

1172
00:29:57,920 --> 00:29:57,930
sampling the library in the past by
 

1173
00:29:57,930 --> 00:30:00,410
sampling the library in the past by
randomly sampling the previous

1174
00:30:00,410 --> 00:30:00,420
randomly sampling the previous
 

1175
00:30:00,420 --> 00:30:03,710
randomly sampling the previous
experiences and batches so you're not

1176
00:30:03,710 --> 00:30:03,720
experiences and batches so you're not
 

1177
00:30:03,720 --> 00:30:05,990
experiences and batches so you're not
always training on the natural

1178
00:30:05,990 --> 00:30:06,000
always training on the natural
 

1179
00:30:06,000 --> 00:30:07,520
always training on the natural
continuous evolution of the system

1180
00:30:07,520 --> 00:30:07,530
continuous evolution of the system
 

1181
00:30:07,530 --> 00:30:09,770
continuous evolution of the system
you're training on randomly picked

1182
00:30:09,770 --> 00:30:09,780
you're training on randomly picked
 

1183
00:30:09,780 --> 00:30:12,320
you're training on randomly picked
batches of those experiences that's like

1184
00:30:12,320 --> 00:30:12,330
batches of those experiences that's like
 

1185
00:30:12,330 --> 00:30:15,950
batches of those experiences that's like
huge it's a it's a seems like a subtle

1186
00:30:15,950 --> 00:30:15,960
huge it's a it's a seems like a subtle
 

1187
00:30:15,960 --> 00:30:18,460
huge it's a it's a seems like a subtle
trick but it's a really important one so

1188
00:30:18,460 --> 00:30:18,470
trick but it's a really important one so
 

1189
00:30:18,470 --> 00:30:23,150
trick but it's a really important one so
the system doesn't over fit a particular

1190
00:30:23,150 --> 00:30:23,160
the system doesn't over fit a particular
 

1191
00:30:23,160 --> 00:30:27,230
the system doesn't over fit a particular
evolution of this of the game of the

1192
00:30:27,230 --> 00:30:27,240
evolution of this of the game of the
 

1193
00:30:27,240 --> 00:30:33,380
evolution of this of the game of the
simulation another important again

1194
00:30:33,380 --> 00:30:33,390
simulation another important again
 

1195
00:30:33,390 --> 00:30:35,420
simulation another important again
subtle trick as in a lot of deep

1196
00:30:35,420 --> 00:30:35,430
subtle trick as in a lot of deep
 

1197
00:30:35,430 --> 00:30:37,550
subtle trick as in a lot of deep
learning approaches the subtle tricks

1198
00:30:37,550 --> 00:30:37,560
learning approaches the subtle tricks
 

1199
00:30:37,560 --> 00:30:41,000
learning approaches the subtle tricks
make all the difference is fixing the

1200
00:30:41,000 --> 00:30:41,010
make all the difference is fixing the
 

1201
00:30:41,010 --> 00:30:44,540
make all the difference is fixing the
target network for the loss function if

1202
00:30:44,540 --> 00:30:44,550
target network for the loss function if
 

1203
00:30:44,550 --> 00:30:47,600
target network for the loss function if
you notice you have to use the neural

1204
00:30:47,600 --> 00:30:47,610
you notice you have to use the neural
 

1205
00:30:47,610 --> 00:30:49,580
you notice you have to use the neural
network thick the singly neural network

1206
00:30:49,580 --> 00:30:49,590
network thick the singly neural network
 

1207
00:30:49,590 --> 00:30:52,400
network thick the singly neural network
the gqi network to estimate the value of

1208
00:30:52,400 --> 00:30:52,410
the gqi network to estimate the value of
 

1209
00:30:52,410 --> 00:30:57,860
the gqi network to estimate the value of
the current state and action pair and

1210
00:30:57,860 --> 00:30:57,870
the current state and action pair and
 

1211
00:30:57,870 --> 00:31:03,350
the current state and action pair and
next so using it multiple times and as

1212
00:31:03,350 --> 00:31:03,360
next so using it multiple times and as
 

1213
00:31:03,360 --> 00:31:06,470
next so using it multiple times and as
you perform that operation you're

1214
00:31:06,470 --> 00:31:06,480
you perform that operation you're
 

1215
00:31:06,480 --> 00:31:08,810
you perform that operation you're
updating the network which means the

1216
00:31:08,810 --> 00:31:08,820
updating the network which means the
 

1217
00:31:08,820 --> 00:31:10,730
updating the network which means the
target function inside that loss

1218
00:31:10,730 --> 00:31:10,740
target function inside that loss
 

1219
00:31:10,740 --> 00:31:13,400
target function inside that loss
function is always changing so you're

1220
00:31:13,400 --> 00:31:13,410
function is always changing so you're
 

1221
00:31:13,410 --> 00:31:15,890
function is always changing so you're
the very nature your loss function is

1222
00:31:15,890 --> 00:31:15,900
the very nature your loss function is
 

1223
00:31:15,900 --> 00:31:17,930
the very nature your loss function is
changing all the time as you're learning

1224
00:31:17,930 --> 00:31:17,940
changing all the time as you're learning
 

1225
00:31:17,940 --> 00:31:20,380
changing all the time as you're learning
and that's a big problem for stability

1226
00:31:20,380 --> 00:31:20,390
and that's a big problem for stability
 

1227
00:31:20,390 --> 00:31:22,670
and that's a big problem for stability
that can create big problems for the

1228
00:31:22,670 --> 00:31:22,680
that can create big problems for the
 

1229
00:31:22,680 --> 00:31:25,340
that can create big problems for the
learning process so this little trick is

1230
00:31:25,340 --> 00:31:25,350
learning process so this little trick is
 

1231
00:31:25,350 --> 00:31:28,400
learning process so this little trick is
to fix the network and only update it

1232
00:31:28,400 --> 00:31:28,410
to fix the network and only update it
 

1233
00:31:28,410 --> 00:31:33,919
to fix the network and only update it
every safe thousand steps so as you

1234
00:31:33,919 --> 00:31:33,929
every safe thousand steps so as you
 

1235
00:31:33,929 --> 00:31:37,280
every safe thousand steps so as you
train the network the the network that's

1236
00:31:37,280 --> 00:31:37,290
train the network the the network that's
 

1237
00:31:37,290 --> 00:31:39,680
train the network the the network that's
used to compute the target function

1238
00:31:39,680 --> 00:31:39,690
used to compute the target function
 

1239
00:31:39,690 --> 00:31:43,460
used to compute the target function
inside the loss function is fixed it

1240
00:31:43,460 --> 00:31:43,470
inside the loss function is fixed it
 

1241
00:31:43,470 --> 00:31:45,830
inside the loss function is fixed it
produces a more stable computation on a

1242
00:31:45,830 --> 00:31:45,840
produces a more stable computation on a
 

1243
00:31:45,840 --> 00:31:48,700
produces a more stable computation on a
loss function so the ground doesn't

1244
00:31:48,700 --> 00:31:48,710
loss function so the ground doesn't
 

1245
00:31:48,710 --> 00:31:51,620
loss function so the ground doesn't
shift under you as you're trying to find

1246
00:31:51,620 --> 00:31:51,630
shift under you as you're trying to find
 

1247
00:31:51,630 --> 00:31:55,010
shift under you as you're trying to find
a minimal for the loss function the loss

1248
00:31:55,010 --> 00:31:55,020
a minimal for the loss function the loss
 

1249
00:31:55,020 --> 00:31:57,440
a minimal for the loss function the loss
function doesn't change in unpredictable

1250
00:31:57,440 --> 00:31:57,450
function doesn't change in unpredictable
 

1251
00:31:57,450 --> 00:32:01,400
function doesn't change in unpredictable
difficult to understand ways and reward

1252
00:32:01,400 --> 00:32:01,410
difficult to understand ways and reward
 

1253
00:32:01,410 --> 00:32:05,210
difficult to understand ways and reward
clipping which is always true with

1254
00:32:05,210 --> 00:32:05,220
clipping which is always true with
 

1255
00:32:05,220 --> 00:32:08,990
clipping which is always true with
general systems that are operating it's

1256
00:32:08,990 --> 00:32:09,000
general systems that are operating it's
 

1257
00:32:09,000 --> 00:32:10,760
general systems that are operating it's
seeking to operate in the generalized

1258
00:32:10,760 --> 00:32:10,770
seeking to operate in the generalized
 

1259
00:32:10,770 --> 00:32:14,590
seeking to operate in the generalized
way is for very for these various games

1260
00:32:14,590 --> 00:32:14,600
way is for very for these various games
 

1261
00:32:14,600 --> 00:32:17,600
way is for very for these various games
the points are different some some

1262
00:32:17,600 --> 00:32:17,610
the points are different some some
 

1263
00:32:17,610 --> 00:32:19,700
the points are different some some
points are low some points are high some

1264
00:32:19,700 --> 00:32:19,710
points are low some points are high some
 

1265
00:32:19,710 --> 00:32:21,380
points are low some points are high some
go positive and negative and they're all

1266
00:32:21,380 --> 00:32:21,390
go positive and negative and they're all
 

1267
00:32:21,390 --> 00:32:23,720
go positive and negative and they're all
normalized to a point where the good

1268
00:32:23,720 --> 00:32:23,730
normalized to a point where the good
 

1269
00:32:23,730 --> 00:32:26,840
normalized to a point where the good
points or the positive points are a 1

1270
00:32:26,840 --> 00:32:26,850
points or the positive points are a 1
 

1271
00:32:26,850 --> 00:32:29,720
points or the positive points are a 1
and negative points are a negative 1

1272
00:32:29,720 --> 00:32:29,730
and negative points are a negative 1
 

1273
00:32:29,730 --> 00:32:32,299
and negative points are a negative 1
that's reward clipping simplify the

1274
00:32:32,299 --> 00:32:32,309
that's reward clipping simplify the
 

1275
00:32:32,309 --> 00:32:35,540
that's reward clipping simplify the
reward structure and because a lot of

1276
00:32:35,540 --> 00:32:35,550
reward structure and because a lot of
 

1277
00:32:35,550 --> 00:32:38,720
reward structure and because a lot of
the games are 30 FPS or 60 FPS and the

1278
00:32:38,720 --> 00:32:38,730
the games are 30 FPS or 60 FPS and the
 

1279
00:32:38,730 --> 00:32:43,130
the games are 30 FPS or 60 FPS and the
actions are not it's not valuable to

1280
00:32:43,130 --> 00:32:43,140
actions are not it's not valuable to
 

1281
00:32:43,140 --> 00:32:45,650
actions are not it's not valuable to
take actions at such a high rate inside

1282
00:32:45,650 --> 00:32:45,660
take actions at such a high rate inside
 

1283
00:32:45,660 --> 00:32:47,450
take actions at such a high rate inside
of these as particularly Atari games

1284
00:32:47,450 --> 00:32:47,460
of these as particularly Atari games
 

1285
00:32:47,460 --> 00:32:49,730
of these as particularly Atari games
then you only take an action every four

1286
00:32:49,730 --> 00:32:49,740
then you only take an action every four
 

1287
00:32:49,740 --> 00:32:52,190
then you only take an action every four
steps while still taking in the frames

1288
00:32:52,190 --> 00:32:52,200
steps while still taking in the frames
 

1289
00:32:52,200 --> 00:32:54,080
steps while still taking in the frames
as part of the temporal window to make

1290
00:32:54,080 --> 00:32:54,090
as part of the temporal window to make
 

1291
00:32:54,090 --> 00:32:57,169
as part of the temporal window to make
decisions tricks but hopefully gives you

1292
00:32:57,169 --> 00:32:57,179
decisions tricks but hopefully gives you
 

1293
00:32:57,179 --> 00:33:01,370
decisions tricks but hopefully gives you
a sense of the kind of things necessary

1294
00:33:01,370 --> 00:33:01,380
a sense of the kind of things necessary
 

1295
00:33:01,380 --> 00:33:05,570
a sense of the kind of things necessary
for both seminal papers like this one

1296
00:33:05,570 --> 00:33:05,580
for both seminal papers like this one
 

1297
00:33:05,580 --> 00:33:07,610
for both seminal papers like this one
and for the more important

1298
00:33:07,610 --> 00:33:07,620
and for the more important
 

1299
00:33:07,620 --> 00:33:09,380
and for the more important
accomplishment of winning deep traffic

1300
00:33:09,380 --> 00:33:09,390
accomplishment of winning deep traffic
 

1301
00:33:09,390 --> 00:33:11,270
accomplishment of winning deep traffic
is that

1302
00:33:11,270 --> 00:33:11,280
is that
 

1303
00:33:11,280 --> 00:33:14,120
is that
the tricks make all the difference here

1304
00:33:14,120 --> 00:33:14,130
the tricks make all the difference here
 

1305
00:33:14,130 --> 00:33:19,400
the tricks make all the difference here
on the bottom is the circle is when the

1306
00:33:19,400 --> 00:33:19,410
on the bottom is the circle is when the
 

1307
00:33:19,410 --> 00:33:21,500
on the bottom is the circle is when the
technique is used in the x1 it's not

1308
00:33:21,500 --> 00:33:21,510
technique is used in the x1 it's not
 

1309
00:33:21,510 --> 00:33:24,290
technique is used in the x1 it's not
looking at replay and target takes

1310
00:33:24,290 --> 00:33:24,300
looking at replay and target takes
 

1311
00:33:24,300 --> 00:33:26,180
looking at replay and target takes
target network and experience replay

1312
00:33:26,180 --> 00:33:26,190
target network and experience replay
 

1313
00:33:26,190 --> 00:33:28,550
target network and experience replay
when both are used for the game of

1314
00:33:28,550 --> 00:33:28,560
when both are used for the game of
 

1315
00:33:28,560 --> 00:33:31,820
when both are used for the game of
breakout River raid sea quests and Space

1316
00:33:31,820 --> 00:33:31,830
breakout River raid sea quests and Space
 

1317
00:33:31,830 --> 00:33:32,420
breakout River raid sea quests and Space
Invaders

1318
00:33:32,420 --> 00:33:32,430
Invaders
 

1319
00:33:32,430 --> 00:33:34,700
Invaders
the higher the number the better it is

1320
00:33:34,700 --> 00:33:34,710
the higher the number the better it is
 

1321
00:33:34,710 --> 00:33:38,750
the higher the number the better it is
the more points achieved so when it

1322
00:33:38,750 --> 00:33:38,760
the more points achieved so when it
 

1323
00:33:38,760 --> 00:33:40,670
the more points achieved so when it
gives you a sense that when replay and

1324
00:33:40,670 --> 00:33:40,680
gives you a sense that when replay and
 

1325
00:33:40,680 --> 00:33:42,590
gives you a sense that when replay and
target both gives significant

1326
00:33:42,590 --> 00:33:42,600
target both gives significant
 

1327
00:33:42,600 --> 00:33:44,750
target both gives significant
improvements in the performance of the

1328
00:33:44,750 --> 00:33:44,760
improvements in the performance of the
 

1329
00:33:44,760 --> 00:33:49,190
improvements in the performance of the
system order of magnitude improvements

1330
00:33:49,190 --> 00:33:49,200
system order of magnitude improvements
 

1331
00:33:49,200 --> 00:33:54,100
system order of magnitude improvements
two orders of magnitude for breakup and

1332
00:33:54,100 --> 00:33:54,110
two orders of magnitude for breakup and
 

1333
00:33:54,110 --> 00:33:59,270
two orders of magnitude for breakup and
here is pseudocode of implementing dq1

1334
00:33:59,270 --> 00:33:59,280
here is pseudocode of implementing dq1
 

1335
00:33:59,280 --> 00:34:03,890
here is pseudocode of implementing dq1
the learning the key thing to notice and

1336
00:34:03,890 --> 00:34:03,900
the learning the key thing to notice and
 

1337
00:34:03,900 --> 00:34:08,750
the learning the key thing to notice and
you can look to the slides is the the

1338
00:34:08,750 --> 00:34:08,760
you can look to the slides is the the
 

1339
00:34:08,760 --> 00:34:11,780
you can look to the slides is the the
loop the while loop of playing through

1340
00:34:11,780 --> 00:34:11,790
loop the while loop of playing through
 

1341
00:34:11,790 --> 00:34:14,120
loop the while loop of playing through
the games and selecting the actions to

1342
00:34:14,120 --> 00:34:14,130
the games and selecting the actions to
 

1343
00:34:14,130 --> 00:34:17,870
the games and selecting the actions to
play is not part of the training it's

1344
00:34:17,870 --> 00:34:17,880
play is not part of the training it's
 

1345
00:34:17,880 --> 00:34:22,040
play is not part of the training it's
it's part of the saving the observations

1346
00:34:22,040 --> 00:34:22,050
it's part of the saving the observations
 

1347
00:34:22,050 --> 00:34:25,240
it's part of the saving the observations
the state action reward next state

1348
00:34:25,240 --> 00:34:25,250
the state action reward next state
 

1349
00:34:25,250 --> 00:34:27,710
the state action reward next state
observation is saving them into replay

1350
00:34:27,710 --> 00:34:27,720
observation is saving them into replay
 

1351
00:34:27,720 --> 00:34:30,080
observation is saving them into replay
memory into that library and then you

1352
00:34:30,080 --> 00:34:30,090
memory into that library and then you
 

1353
00:34:30,090 --> 00:34:32,899
memory into that library and then you
sample randomly from that replay memory

1354
00:34:32,899 --> 00:34:32,909
sample randomly from that replay memory
 

1355
00:34:32,909 --> 00:34:36,169
sample randomly from that replay memory
to then train the network based on the

1356
00:34:36,169 --> 00:34:36,179
to then train the network based on the
 

1357
00:34:36,179 --> 00:34:40,550
to then train the network based on the
loss function and with probability up up

1358
00:34:40,550 --> 00:34:40,560
loss function and with probability up up
 

1359
00:34:40,560 --> 00:34:43,159
loss function and with probability up up
top with the probability epsilon select

1360
00:34:43,159 --> 00:34:43,169
top with the probability epsilon select
 

1361
00:34:43,169 --> 00:34:46,389
top with the probability epsilon select
a random action that epsilon is the

1362
00:34:46,389 --> 00:34:46,399
a random action that epsilon is the
 

1363
00:34:46,399 --> 00:34:49,280
a random action that epsilon is the
probability of exploration that

1364
00:34:49,280 --> 00:34:49,290
probability of exploration that
 

1365
00:34:49,290 --> 00:34:51,800
probability of exploration that
decreases that's something you'll see in

1366
00:34:51,800 --> 00:34:51,810
decreases that's something you'll see in
 

1367
00:34:51,810 --> 00:34:55,460
decreases that's something you'll see in
deep traffic as well is the rate at

1368
00:34:55,460 --> 00:34:55,470
deep traffic as well is the rate at
 

1369
00:34:55,470 --> 00:34:57,620
deep traffic as well is the rate at
which that exploration decreases over

1370
00:34:57,620 --> 00:34:57,630
which that exploration decreases over
 

1371
00:34:57,630 --> 00:34:59,510
which that exploration decreases over
time through the training process you

1372
00:34:59,510 --> 00:34:59,520
time through the training process you
 

1373
00:34:59,520 --> 00:35:02,300
time through the training process you
want to explore a lot first and less and

1374
00:35:02,300 --> 00:35:02,310
want to explore a lot first and less and
 

1375
00:35:02,310 --> 00:35:05,180
want to explore a lot first and less and
less over time so this algorithm is

1376
00:35:05,180 --> 00:35:05,190
less over time so this algorithm is
 

1377
00:35:05,190 --> 00:35:08,350
less over time so this algorithm is
being able to accomplish in 2015 and

1378
00:35:08,350 --> 00:35:08,360
being able to accomplish in 2015 and
 

1379
00:35:08,360 --> 00:35:12,440
being able to accomplish in 2015 and
since a lot of incredible things things

1380
00:35:12,440 --> 00:35:12,450
since a lot of incredible things things
 

1381
00:35:12,450 --> 00:35:18,760
since a lot of incredible things things
that made the AI world think that we

1382
00:35:18,760 --> 00:35:18,770
that made the AI world think that we
 

1383
00:35:18,770 --> 00:35:22,819
that made the AI world think that we
were onto something that

1384
00:35:22,819 --> 00:35:22,829
were onto something that
 

1385
00:35:22,829 --> 00:35:26,630
were onto something that
general AI is within reach for the first

1386
00:35:26,630 --> 00:35:26,640
general AI is within reach for the first
 

1387
00:35:26,640 --> 00:35:28,969
general AI is within reach for the first
time that raw sensor information was

1388
00:35:28,969 --> 00:35:28,979
time that raw sensor information was
 

1389
00:35:28,979 --> 00:35:31,519
time that raw sensor information was
used to create a system that acts and

1390
00:35:31,519 --> 00:35:31,529
used to create a system that acts and
 

1391
00:35:31,529 --> 00:35:33,589
used to create a system that acts and
makes sense of the world make sense of

1392
00:35:33,589 --> 00:35:33,599
makes sense of the world make sense of
 

1393
00:35:33,599 --> 00:35:35,420
makes sense of the world make sense of
the physics of the world enough to be

1394
00:35:35,420 --> 00:35:35,430
the physics of the world enough to be
 

1395
00:35:35,430 --> 00:35:37,519
the physics of the world enough to be
able to succeed in it from very little

1396
00:35:37,519 --> 00:35:37,529
able to succeed in it from very little
 

1397
00:35:37,529 --> 00:35:42,819
able to succeed in it from very little
information but these games are trivial

1398
00:35:42,819 --> 00:35:42,829

 

1399
00:35:42,829 --> 00:35:48,529

even though there is a lot of them this

1400
00:35:48,529 --> 00:35:48,539
even though there is a lot of them this
 

1401
00:35:48,539 --> 00:35:50,449
even though there is a lot of them this
dqn approach has been able to outperform

1402
00:35:50,449 --> 00:35:50,459
dqn approach has been able to outperform
 

1403
00:35:50,459 --> 00:35:52,640
dqn approach has been able to outperform
a lot of the Atari games

1404
00:35:52,640 --> 00:35:52,650
a lot of the Atari games
 

1405
00:35:52,650 --> 00:35:54,309
a lot of the Atari games
that's what's been reported on

1406
00:35:54,309 --> 00:35:54,319
that's what's been reported on
 

1407
00:35:54,319 --> 00:35:56,589
that's what's been reported on
outperform the human level performance

1408
00:35:56,589 --> 00:35:56,599
outperform the human level performance
 

1409
00:35:56,599 --> 00:36:00,979
outperform the human level performance
but again these games are trivial what I

1410
00:36:00,979 --> 00:36:00,989
but again these games are trivial what I
 

1411
00:36:00,989 --> 00:36:05,209
but again these games are trivial what I
think and perhaps biased I'm biased but

1412
00:36:05,209 --> 00:36:05,219
think and perhaps biased I'm biased but
 

1413
00:36:05,219 --> 00:36:06,859
think and perhaps biased I'm biased but
one of the greatest accomplishments of

1414
00:36:06,859 --> 00:36:06,869
one of the greatest accomplishments of
 

1415
00:36:06,869 --> 00:36:08,420
one of the greatest accomplishments of
artificial intelligence in the last

1416
00:36:08,420 --> 00:36:08,430
artificial intelligence in the last
 

1417
00:36:08,430 --> 00:36:12,499
artificial intelligence in the last
decade at least from the philosophical

1418
00:36:12,499 --> 00:36:12,509
decade at least from the philosophical
 

1419
00:36:12,509 --> 00:36:19,059
decade at least from the philosophical
or the research perspective is alphago 0

1420
00:36:19,059 --> 00:36:19,069
or the research perspective is alphago 0
 

1421
00:36:19,069 --> 00:36:24,380
or the research perspective is alphago 0
first alphago and then alphago 0 its

1422
00:36:24,380 --> 00:36:24,390
first alphago and then alphago 0 its
 

1423
00:36:24,390 --> 00:36:27,469
first alphago and then alphago 0 its
deepmind system that beat the best in

1424
00:36:27,469 --> 00:36:27,479
deepmind system that beat the best in
 

1425
00:36:27,479 --> 00:36:30,380
deepmind system that beat the best in
the world in a game of go so what's the

1426
00:36:30,380 --> 00:36:30,390
the world in a game of go so what's the
 

1427
00:36:30,390 --> 00:36:35,390
the world in a game of go so what's the
game of go it's simple I won't get into

1428
00:36:35,390 --> 00:36:35,400
game of go it's simple I won't get into
 

1429
00:36:35,400 --> 00:36:38,209
game of go it's simple I won't get into
the rules but basically it's a 19 by 19

1430
00:36:38,209 --> 00:36:38,219
the rules but basically it's a 19 by 19
 

1431
00:36:38,219 --> 00:36:42,039
the rules but basically it's a 19 by 19
board shown on the bottom of the slide

1432
00:36:42,039 --> 00:36:42,049
board shown on the bottom of the slide
 

1433
00:36:42,049 --> 00:36:45,650
board shown on the bottom of the slide
for the bottom row of the table for a

1434
00:36:45,650 --> 00:36:45,660
for the bottom row of the table for a
 

1435
00:36:45,660 --> 00:36:50,359
for the bottom row of the table for a
board of 19 by 19 the number of legal

1436
00:36:50,359 --> 00:36:50,369
board of 19 by 19 the number of legal
 

1437
00:36:50,369 --> 00:36:53,539
board of 19 by 19 the number of legal
game positions is 2 times 10 to the

1438
00:36:53,539 --> 00:36:53,549
game positions is 2 times 10 to the
 

1439
00:36:53,549 --> 00:36:57,709
game positions is 2 times 10 to the
power of 170 it's a very large number of

1440
00:36:57,709 --> 00:36:57,719
power of 170 it's a very large number of
 

1441
00:36:57,719 --> 00:37:00,349
power of 170 it's a very large number of
possible positions to consider any one

1442
00:37:00,349 --> 00:37:00,359
possible positions to consider any one
 

1443
00:37:00,359 --> 00:37:03,140
possible positions to consider any one
time especially the game evolves the

1444
00:37:03,140 --> 00:37:03,150
time especially the game evolves the
 

1445
00:37:03,150 --> 00:37:07,339
time especially the game evolves the
number of possible moves is huge much

1446
00:37:07,339 --> 00:37:07,349
number of possible moves is huge much
 

1447
00:37:07,349 --> 00:37:12,189
number of possible moves is huge much
larger than in chess so that's why AI

1448
00:37:12,189 --> 00:37:12,199
larger than in chess so that's why AI
 

1449
00:37:12,199 --> 00:37:14,630
larger than in chess so that's why AI
the community thought that this game is

1450
00:37:14,630 --> 00:37:14,640
the community thought that this game is
 

1451
00:37:14,640 --> 00:37:21,789
the community thought that this game is
not solvable until 2016 when alphago

1452
00:37:21,789 --> 00:37:21,799
not solvable until 2016 when alphago
 

1453
00:37:21,799 --> 00:37:26,739
not solvable until 2016 when alphago
used this use human expert position play

1454
00:37:26,739 --> 00:37:26,749
used this use human expert position play
 

1455
00:37:26,749 --> 00:37:30,009
used this use human expert position play
to seed in a supervised way

1456
00:37:30,009 --> 00:37:30,019
to seed in a supervised way
 

1457
00:37:30,019 --> 00:37:32,930
to seed in a supervised way
reinforcement learning approach and I'll

1458
00:37:32,930 --> 00:37:32,940
reinforcement learning approach and I'll
 

1459
00:37:32,940 --> 00:37:35,479
reinforcement learning approach and I'll
describe in a little bit of detail and a

1460
00:37:35,479 --> 00:37:35,489
describe in a little bit of detail and a
 

1461
00:37:35,489 --> 00:37:36,850
describe in a little bit of detail and a
couple of slides here

1462
00:37:36,850 --> 00:37:36,860
couple of slides here
 

1463
00:37:36,860 --> 00:37:42,650
couple of slides here
to beat the best in the world and then

1464
00:37:42,650 --> 00:37:42,660
to beat the best in the world and then
 

1465
00:37:42,660 --> 00:37:47,930
to beat the best in the world and then
alphago 0 that is the accomplishment of

1466
00:37:47,930 --> 00:37:47,940
alphago 0 that is the accomplishment of
 

1467
00:37:47,940 --> 00:37:52,850
alphago 0 that is the accomplishment of
the decade for me in AI is being able to

1468
00:37:52,850 --> 00:37:52,860
the decade for me in AI is being able to
 

1469
00:37:52,860 --> 00:37:59,360
the decade for me in AI is being able to
play with no training data on human

1470
00:37:59,360 --> 00:37:59,370
play with no training data on human
 

1471
00:37:59,370 --> 00:38:04,700
play with no training data on human
expert games and beat the best in the

1472
00:38:04,700 --> 00:38:04,710
expert games and beat the best in the
 

1473
00:38:04,710 --> 00:38:07,100
expert games and beat the best in the
world in an extremely complex game this

1474
00:38:07,100 --> 00:38:07,110
world in an extremely complex game this
 

1475
00:38:07,110 --> 00:38:12,130
world in an extremely complex game this
is not Atari this is and this is a much

1476
00:38:12,130 --> 00:38:12,140
is not Atari this is and this is a much
 

1477
00:38:12,140 --> 00:38:17,030
is not Atari this is and this is a much
higher order difficulty game and that

1478
00:38:17,030 --> 00:38:17,040
higher order difficulty game and that
 

1479
00:38:17,040 --> 00:38:18,680
higher order difficulty game and that
and the quality of players that is

1480
00:38:18,680 --> 00:38:18,690
and the quality of players that is
 

1481
00:38:18,690 --> 00:38:21,230
and the quality of players that is
competing in is much higher and it's

1482
00:38:21,230 --> 00:38:21,240
competing in is much higher and it's
 

1483
00:38:21,240 --> 00:38:24,260
competing in is much higher and it's
able to extremely quickly here to

1484
00:38:24,260 --> 00:38:24,270
able to extremely quickly here to
 

1485
00:38:24,270 --> 00:38:26,000
able to extremely quickly here to
achieve a rating that's better than

1486
00:38:26,000 --> 00:38:26,010
achieve a rating that's better than
 

1487
00:38:26,010 --> 00:38:29,690
achieve a rating that's better than
alphago and better than the different

1488
00:38:29,690 --> 00:38:29,700
alphago and better than the different
 

1489
00:38:29,700 --> 00:38:32,150
alphago and better than the different
variants of alphago and certainly better

1490
00:38:32,150 --> 00:38:32,160
variants of alphago and certainly better
 

1491
00:38:32,160 --> 00:38:35,240
variants of alphago and certainly better
than the best of the human players in 21

1492
00:38:35,240 --> 00:38:35,250
than the best of the human players in 21
 

1493
00:38:35,250 --> 00:38:39,790
than the best of the human players in 21
days of self play so how does it work

1494
00:38:39,790 --> 00:38:39,800
days of self play so how does it work
 

1495
00:38:39,800 --> 00:38:43,640
days of self play so how does it work
all of these approaches much much like

1496
00:38:43,640 --> 00:38:43,650
all of these approaches much much like
 

1497
00:38:43,650 --> 00:38:45,620
all of these approaches much much like
the previous ones the traditional ones

1498
00:38:45,620 --> 00:38:45,630
the previous ones the traditional ones
 

1499
00:38:45,630 --> 00:38:49,480
the previous ones the traditional ones
that are not based on deep learning are

1500
00:38:49,480 --> 00:38:49,490
that are not based on deep learning are
 

1501
00:38:49,490 --> 00:38:53,590
that are not based on deep learning are
using Monte Carlo tree search MCTS

1502
00:38:53,590 --> 00:38:53,600
using Monte Carlo tree search MCTS
 

1503
00:38:53,600 --> 00:38:57,260
using Monte Carlo tree search MCTS
which is when you have such a large

1504
00:38:57,260 --> 00:38:57,270
which is when you have such a large
 

1505
00:38:57,270 --> 00:39:00,500
which is when you have such a large
state space you start at a board and you

1506
00:39:00,500 --> 00:39:00,510
state space you start at a board and you
 

1507
00:39:00,510 --> 00:39:06,400
state space you start at a board and you
play and you choose moves with some

1508
00:39:06,400 --> 00:39:06,410
play and you choose moves with some
 

1509
00:39:06,410 --> 00:39:10,540
play and you choose moves with some
exploitation exploration balancing

1510
00:39:10,540 --> 00:39:10,550
exploitation exploration balancing
 

1511
00:39:10,550 --> 00:39:12,710
exploitation exploration balancing
choosing to explore totally new

1512
00:39:12,710 --> 00:39:12,720
choosing to explore totally new
 

1513
00:39:12,720 --> 00:39:15,050
choosing to explore totally new
positions or to go deep in the positions

1514
00:39:15,050 --> 00:39:15,060
positions or to go deep in the positions
 

1515
00:39:15,060 --> 00:39:17,000
positions or to go deep in the positions
you know are good until the bottom of

1516
00:39:17,000 --> 00:39:17,010
you know are good until the bottom of
 

1517
00:39:17,010 --> 00:39:19,130
you know are good until the bottom of
the game is reached until the final

1518
00:39:19,130 --> 00:39:19,140
the game is reached until the final
 

1519
00:39:19,140 --> 00:39:20,870
the game is reached until the final
state is reached and then you back

1520
00:39:20,870 --> 00:39:20,880
state is reached and then you back
 

1521
00:39:20,880 --> 00:39:24,710
state is reached and then you back
propagate the quality of the choices you

1522
00:39:24,710 --> 00:39:24,720
propagate the quality of the choices you
 

1523
00:39:24,720 --> 00:39:26,240
propagate the quality of the choices you
made leading to that position

1524
00:39:26,240 --> 00:39:26,250
made leading to that position
 

1525
00:39:26,250 --> 00:39:28,840
made leading to that position
and in that way you learn the value of

1526
00:39:28,840 --> 00:39:28,850
and in that way you learn the value of
 

1527
00:39:28,850 --> 00:39:34,310
and in that way you learn the value of
of board positions and play that's been

1528
00:39:34,310 --> 00:39:34,320
of board positions and play that's been
 

1529
00:39:34,320 --> 00:39:37,120
of board positions and play that's been
used by the most successful go playing

1530
00:39:37,120 --> 00:39:37,130
used by the most successful go playing
 

1531
00:39:37,130 --> 00:39:41,990
used by the most successful go playing
engines before and alphago since but you

1532
00:39:41,990 --> 00:39:42,000
engines before and alphago since but you
 

1533
00:39:42,000 --> 00:39:43,760
engines before and alphago since but you
might be able to guess what's the

1534
00:39:43,760 --> 00:39:43,770
might be able to guess what's the
 

1535
00:39:43,770 --> 00:39:45,680
might be able to guess what's the
difference with alphago verse to the

1536
00:39:45,680 --> 00:39:45,690
difference with alphago verse to the
 

1537
00:39:45,690 --> 00:39:48,860
difference with alphago verse to the
previous approaches they use the neural

1538
00:39:48,860 --> 00:39:48,870
previous approaches they use the neural
 

1539
00:39:48,870 --> 00:39:50,240
previous approaches they use the neural
network

1540
00:39:50,240 --> 00:39:50,250
network
 

1541
00:39:50,250 --> 00:39:55,220
network
as the intuition quote-unquote - what

1542
00:39:55,220 --> 00:39:55,230
as the intuition quote-unquote - what
 

1543
00:39:55,230 --> 00:39:57,620
as the intuition quote-unquote - what
are the good states what are the good

1544
00:39:57,620 --> 00:39:57,630
are the good states what are the good
 

1545
00:39:57,630 --> 00:40:06,230
are the good states what are the good
next board positions to explore and the

1546
00:40:06,230 --> 00:40:06,240
next board positions to explore and the
 

1547
00:40:06,240 --> 00:40:09,320
next board positions to explore and the
key things again the tricks make all the

1548
00:40:09,320 --> 00:40:09,330
key things again the tricks make all the
 

1549
00:40:09,330 --> 00:40:13,760
key things again the tricks make all the
difference that made alphago zero work

1550
00:40:13,760 --> 00:40:13,770
difference that made alphago zero work
 

1551
00:40:13,770 --> 00:40:16,460
difference that made alphago zero work
and work much better than alphago is

1552
00:40:16,460 --> 00:40:16,470
and work much better than alphago is
 

1553
00:40:16,470 --> 00:40:19,360
and work much better than alphago is
first because there was no expert play

1554
00:40:19,360 --> 00:40:19,370
first because there was no expert play
 

1555
00:40:19,370 --> 00:40:23,230
first because there was no expert play
instead of human games

1556
00:40:23,230 --> 00:40:23,240

 

1557
00:40:23,240 --> 00:40:28,970

alphago used that very same Monte Carlo

1558
00:40:28,970 --> 00:40:28,980
alphago used that very same Monte Carlo
 

1559
00:40:28,980 --> 00:40:32,150
alphago used that very same Monte Carlo
tree search algorithm MCTS to do an

1560
00:40:32,150 --> 00:40:32,160
tree search algorithm MCTS to do an
 

1561
00:40:32,160 --> 00:40:34,790
tree search algorithm MCTS to do an
intelligent look ahead based on the

1562
00:40:34,790 --> 00:40:34,800
intelligent look ahead based on the
 

1563
00:40:34,800 --> 00:40:36,710
intelligent look ahead based on the
neural network prediction of where dove

1564
00:40:36,710 --> 00:40:36,720
neural network prediction of where dove
 

1565
00:40:36,720 --> 00:40:40,250
neural network prediction of where dove
the good States to take it checked that

1566
00:40:40,250 --> 00:40:40,260
the good States to take it checked that
 

1567
00:40:40,260 --> 00:40:42,680
the good States to take it checked that
instead of human expert play it checked

1568
00:40:42,680 --> 00:40:42,690
instead of human expert play it checked
 

1569
00:40:42,690 --> 00:40:47,300
instead of human expert play it checked
how good indeed are those states it's a

1570
00:40:47,300 --> 00:40:47,310
how good indeed are those states it's a
 

1571
00:40:47,310 --> 00:40:50,540
how good indeed are those states it's a
simple look ahead action that does the

1572
00:40:50,540 --> 00:40:50,550
simple look ahead action that does the
 

1573
00:40:50,550 --> 00:40:52,780
simple look ahead action that does the
ground truth that does the target

1574
00:40:52,780 --> 00:40:52,790
ground truth that does the target
 

1575
00:40:52,790 --> 00:40:54,860
ground truth that does the target
correction that produces the loss

1576
00:40:54,860 --> 00:40:54,870
correction that produces the loss
 

1577
00:40:54,870 --> 00:40:56,750
correction that produces the loss
function the second part is the

1578
00:40:56,750 --> 00:40:56,760
function the second part is the
 

1579
00:40:56,760 --> 00:40:59,090
function the second part is the
multitask learning what's now called

1580
00:40:59,090 --> 00:40:59,100
multitask learning what's now called
 

1581
00:40:59,100 --> 00:41:01,900
multitask learning what's now called
multitask learning is the networkers is

1582
00:41:01,900 --> 00:41:01,910
multitask learning is the networkers is
 

1583
00:41:01,910 --> 00:41:04,820
multitask learning is the networkers is
quote-unquote two-headed in the sense

1584
00:41:04,820 --> 00:41:04,830
quote-unquote two-headed in the sense
 

1585
00:41:04,830 --> 00:41:06,800
quote-unquote two-headed in the sense
that first it outputs the probability of

1586
00:41:06,800 --> 00:41:06,810
that first it outputs the probability of
 

1587
00:41:06,810 --> 00:41:09,020
that first it outputs the probability of
which move to take the obvious thing and

1588
00:41:09,020 --> 00:41:09,030
which move to take the obvious thing and
 

1589
00:41:09,030 --> 00:41:11,060
which move to take the obvious thing and
it's also producing a probability of

1590
00:41:11,060 --> 00:41:11,070
it's also producing a probability of
 

1591
00:41:11,070 --> 00:41:13,520
it's also producing a probability of
winning and there's a few ways to

1592
00:41:13,520 --> 00:41:13,530
winning and there's a few ways to
 

1593
00:41:13,530 --> 00:41:15,160
winning and there's a few ways to
combine that information and

1594
00:41:15,160 --> 00:41:15,170
combine that information and
 

1595
00:41:15,170 --> 00:41:18,440
combine that information and
continuously train both parts of the

1596
00:41:18,440 --> 00:41:18,450
continuously train both parts of the
 

1597
00:41:18,450 --> 00:41:21,440
continuously train both parts of the
network depending on the choice taken so

1598
00:41:21,440 --> 00:41:21,450
network depending on the choice taken so
 

1599
00:41:21,450 --> 00:41:23,270
network depending on the choice taken so
you want to take the best choice in the

1600
00:41:23,270 --> 00:41:23,280
you want to take the best choice in the
 

1601
00:41:23,280 --> 00:41:26,030
you want to take the best choice in the
short term and achieve the positions

1602
00:41:26,030 --> 00:41:26,040
short term and achieve the positions
 

1603
00:41:26,040 --> 00:41:28,250
short term and achieve the positions
that are highly a slightly hood of

1604
00:41:28,250 --> 00:41:28,260
that are highly a slightly hood of
 

1605
00:41:28,260 --> 00:41:30,710
that are highly a slightly hood of
winning for the player that's whose turn

1606
00:41:30,710 --> 00:41:30,720
winning for the player that's whose turn
 

1607
00:41:30,720 --> 00:41:37,220
winning for the player that's whose turn
it is and another big step is that they

1608
00:41:37,220 --> 00:41:37,230
it is and another big step is that they
 

1609
00:41:37,230 --> 00:41:40,250
it is and another big step is that they
updated from 2015 the updated of the

1610
00:41:40,250 --> 00:41:40,260
updated from 2015 the updated of the
 

1611
00:41:40,260 --> 00:41:42,230
updated from 2015 the updated of the
state-of-the-art architecture which are

1612
00:41:42,230 --> 00:41:42,240
state-of-the-art architecture which are
 

1613
00:41:42,240 --> 00:41:44,720
state-of-the-art architecture which are
now the architecture that one imagenet

1614
00:41:44,720 --> 00:41:44,730
now the architecture that one imagenet
 

1615
00:41:44,730 --> 00:41:48,170
now the architecture that one imagenet
as the residual networks ResNet for

1616
00:41:48,170 --> 00:41:48,180
as the residual networks ResNet for
 

1617
00:41:48,180 --> 00:41:50,750
as the residual networks ResNet for
imagenet those that's it

1618
00:41:50,750 --> 00:41:50,760
imagenet those that's it
 

1619
00:41:50,760 --> 00:41:53,660
imagenet those that's it
and those little changes made all the

1620
00:41:53,660 --> 00:41:53,670
and those little changes made all the
 

1621
00:41:53,670 --> 00:41:56,690
and those little changes made all the
difference so that takes us to deep

1622
00:41:56,690 --> 00:41:56,700
difference so that takes us to deep
 

1623
00:41:56,700 --> 00:41:58,850
difference so that takes us to deep
traffic and the eight billion hours

1624
00:41:58,850 --> 00:41:58,860
traffic and the eight billion hours
 

1625
00:41:58,860 --> 00:42:02,210
traffic and the eight billion hours
stuck in traffic

1626
00:42:02,210 --> 00:42:02,220

 

1627
00:42:02,220 --> 00:42:04,190

America's pastime so we tried to

1628
00:42:04,190 --> 00:42:04,200
America's pastime so we tried to
 

1629
00:42:04,200 --> 00:42:08,780
America's pastime so we tried to
simulate driving that behavior layer of

1630
00:42:08,780 --> 00:42:08,790
simulate driving that behavior layer of
 

1631
00:42:08,790 --> 00:42:12,349
simulate driving that behavior layer of
driving so not the immediate control not

1632
00:42:12,349 --> 00:42:12,359
driving so not the immediate control not
 

1633
00:42:12,359 --> 00:42:14,870
driving so not the immediate control not
the motion planning but beyond that on

1634
00:42:14,870 --> 00:42:14,880
the motion planning but beyond that on
 

1635
00:42:14,880 --> 00:42:17,960
the motion planning but beyond that on
top on top of those control decisions

1636
00:42:17,960 --> 00:42:17,970
top on top of those control decisions
 

1637
00:42:17,970 --> 00:42:21,320
top on top of those control decisions
the human interpretable decisions of

1638
00:42:21,320 --> 00:42:21,330
the human interpretable decisions of
 

1639
00:42:21,330 --> 00:42:22,730
the human interpretable decisions of
changing lane of speeding up slowing

1640
00:42:22,730 --> 00:42:22,740
changing lane of speeding up slowing
 

1641
00:42:22,740 --> 00:42:26,540
changing lane of speeding up slowing
down modeling that in a micro traffic

1642
00:42:26,540 --> 00:42:26,550
down modeling that in a micro traffic
 

1643
00:42:26,550 --> 00:42:28,160
down modeling that in a micro traffic
simulation framework that's popular in

1644
00:42:28,160 --> 00:42:28,170
simulation framework that's popular in
 

1645
00:42:28,170 --> 00:42:30,470
simulation framework that's popular in
traffic engineering the kind of shown

1646
00:42:30,470 --> 00:42:30,480
traffic engineering the kind of shown
 

1647
00:42:30,480 --> 00:42:34,339
traffic engineering the kind of shown
here we apply deep reinforcement

1648
00:42:34,339 --> 00:42:34,349
here we apply deep reinforcement
 

1649
00:42:34,349 --> 00:42:36,470
here we apply deep reinforcement
learning to that I'll call it deep

1650
00:42:36,470 --> 00:42:36,480
learning to that I'll call it deep
 

1651
00:42:36,480 --> 00:42:39,140
learning to that I'll call it deep
traffic the goal is to achieve the

1652
00:42:39,140 --> 00:42:39,150
traffic the goal is to achieve the
 

1653
00:42:39,150 --> 00:42:41,450
traffic the goal is to achieve the
highest average speed over a long period

1654
00:42:41,450 --> 00:42:41,460
highest average speed over a long period
 

1655
00:42:41,460 --> 00:42:44,170
highest average speed over a long period
of time weaving in and out of traffic

1656
00:42:44,170 --> 00:42:44,180
of time weaving in and out of traffic
 

1657
00:42:44,180 --> 00:42:47,660
of time weaving in and out of traffic
for students here the requirement is to

1658
00:42:47,660 --> 00:42:47,670
for students here the requirement is to
 

1659
00:42:47,670 --> 00:42:49,640
for students here the requirement is to
follow the tutorial and achieve a speed

1660
00:42:49,640 --> 00:42:49,650
follow the tutorial and achieve a speed
 

1661
00:42:49,650 --> 00:42:55,670
follow the tutorial and achieve a speed
of 65 miles an hour and if you really

1662
00:42:55,670 --> 00:42:55,680
of 65 miles an hour and if you really
 

1663
00:42:55,680 --> 00:42:58,880
of 65 miles an hour and if you really
want to achieve a speed over 70 miles an

1664
00:42:58,880 --> 00:42:58,890
want to achieve a speed over 70 miles an
 

1665
00:42:58,890 --> 00:43:02,380
want to achieve a speed over 70 miles an
hour which is what's acquired to win and

1666
00:43:02,380 --> 00:43:02,390
hour which is what's acquired to win and
 

1667
00:43:02,390 --> 00:43:06,380
hour which is what's acquired to win and
perhaps upload your own image to make

1668
00:43:06,380 --> 00:43:06,390
perhaps upload your own image to make
 

1669
00:43:06,390 --> 00:43:10,220
perhaps upload your own image to make
sure you look good doing it what you

1670
00:43:10,220 --> 00:43:10,230
sure you look good doing it what you
 

1671
00:43:10,230 --> 00:43:13,160
sure you look good doing it what you
should do clear instructions to compete

1672
00:43:13,160 --> 00:43:13,170
should do clear instructions to compete
 

1673
00:43:13,170 --> 00:43:17,540
should do clear instructions to compete
read the tutorial you can change

1674
00:43:17,540 --> 00:43:17,550
read the tutorial you can change
 

1675
00:43:17,550 --> 00:43:19,220
read the tutorial you can change
parameters in the code box on that

1676
00:43:19,220 --> 00:43:19,230
parameters in the code box on that
 

1677
00:43:19,230 --> 00:43:21,890
parameters in the code box on that
website cars done on mighty dad you size

1678
00:43:21,890 --> 00:43:21,900
website cars done on mighty dad you size
 

1679
00:43:21,900 --> 00:43:24,440
website cars done on mighty dad you size
deep traffic click the white button that

1680
00:43:24,440 --> 00:43:24,450
deep traffic click the white button that
 

1681
00:43:24,450 --> 00:43:26,960
deep traffic click the white button that
says apply code which applies the code

1682
00:43:26,960 --> 00:43:26,970
says apply code which applies the code
 

1683
00:43:26,970 --> 00:43:28,400
says apply code which applies the code
that you write these are the parameters

1684
00:43:28,400 --> 00:43:28,410
that you write these are the parameters
 

1685
00:43:28,410 --> 00:43:31,460
that you write these are the parameters
that you specify then you'll network it

1686
00:43:31,460 --> 00:43:31,470
that you specify then you'll network it
 

1687
00:43:31,470 --> 00:43:34,130
that you specify then you'll network it
applies those parameters creates the

1688
00:43:34,130 --> 00:43:34,140
applies those parameters creates the
 

1689
00:43:34,140 --> 00:43:36,290
applies those parameters creates the
architecture do you specify and now you

1690
00:43:36,290 --> 00:43:36,300
architecture do you specify and now you
 

1691
00:43:36,300 --> 00:43:38,990
architecture do you specify and now you
have a network written in JavaScript

1692
00:43:38,990 --> 00:43:39,000
have a network written in JavaScript
 

1693
00:43:39,000 --> 00:43:40,370
have a network written in JavaScript
living in the browser ready to be

1694
00:43:40,370 --> 00:43:40,380
living in the browser ready to be
 

1695
00:43:40,380 --> 00:43:43,609
living in the browser ready to be
trained then you click the blue button

1696
00:43:43,609 --> 00:43:43,619
trained then you click the blue button
 

1697
00:43:43,619 --> 00:43:46,760
trained then you click the blue button
that says run training and that trains

1698
00:43:46,760 --> 00:43:46,770
that says run training and that trains
 

1699
00:43:46,770 --> 00:43:50,300
that says run training and that trains
the network much faster than one's

1700
00:43:50,300 --> 00:43:50,310
the network much faster than one's
 

1701
00:43:50,310 --> 00:43:52,280
the network much faster than one's
actually being visualized in the browser

1702
00:43:52,280 --> 00:43:52,290
actually being visualized in the browser
 

1703
00:43:52,290 --> 00:43:56,359
actually being visualized in the browser
a thousand times faster by evolving the

1704
00:43:56,359 --> 00:43:56,369
a thousand times faster by evolving the
 

1705
00:43:56,369 --> 00:43:58,460
a thousand times faster by evolving the
game making decisions taking in the grid

1706
00:43:58,460 --> 00:43:58,470
game making decisions taking in the grid
 

1707
00:43:58,470 --> 00:44:00,170
game making decisions taking in the grid
space as I'll talk about here in a

1708
00:44:00,170 --> 00:44:00,180
space as I'll talk about here in a
 

1709
00:44:00,180 --> 00:44:02,540
space as I'll talk about here in a
second the speed limit is 80 miles an

1710
00:44:02,540 --> 00:44:02,550
second the speed limit is 80 miles an
 

1711
00:44:02,550 --> 00:44:05,630
second the speed limit is 80 miles an
hour based on the various adjustments

1712
00:44:05,630 --> 00:44:05,640
hour based on the various adjustments
 

1713
00:44:05,640 --> 00:44:08,210
hour based on the various adjustments
were made to the game reaching 80 miles

1714
00:44:08,210 --> 00:44:08,220
were made to the game reaching 80 miles
 

1715
00:44:08,220 --> 00:44:09,920
were made to the game reaching 80 miles
an hour is certainly impossible an

1716
00:44:09,920 --> 00:44:09,930
an hour is certainly impossible an
 

1717
00:44:09,930 --> 00:44:13,099
an hour is certainly impossible an
average and reaching some of the speeds

1718
00:44:13,099 --> 00:44:13,109
average and reaching some of the speeds
 

1719
00:44:13,109 --> 00:44:15,200
average and reaching some of the speeds
that we've achieved last year

1720
00:44:15,200 --> 00:44:15,210
that we've achieved last year
 

1721
00:44:15,210 --> 00:44:18,040
that we've achieved last year
it's much much much more difficult

1722
00:44:18,040 --> 00:44:18,050
it's much much much more difficult
 

1723
00:44:18,050 --> 00:44:20,450
it's much much much more difficult
finally when you're happy and the

1724
00:44:20,450 --> 00:44:20,460
finally when you're happy and the
 

1725
00:44:20,460 --> 00:44:24,050
finally when you're happy and the
training is done submit the model to

1726
00:44:24,050 --> 00:44:24,060
training is done submit the model to
 

1727
00:44:24,060 --> 00:44:28,099
training is done submit the model to
competition for those super eager

1728
00:44:28,099 --> 00:44:28,109
competition for those super eager
 

1729
00:44:28,109 --> 00:44:30,380
competition for those super eager
dedicated students you can do so every

1730
00:44:30,380 --> 00:44:30,390
dedicated students you can do so every
 

1731
00:44:30,390 --> 00:44:34,400
dedicated students you can do so every
five minutes and to visualize your

1732
00:44:34,400 --> 00:44:34,410
five minutes and to visualize your
 

1733
00:44:34,410 --> 00:44:39,079
five minutes and to visualize your
submission you can click the request

1734
00:44:39,079 --> 00:44:39,089
submission you can click the request
 

1735
00:44:39,089 --> 00:44:41,300
submission you can click the request
visualization specifying the custom

1736
00:44:41,300 --> 00:44:41,310
visualization specifying the custom
 

1737
00:44:41,310 --> 00:44:45,530
visualization specifying the custom
image and the color okay

1738
00:44:45,530 --> 00:44:45,540
image and the color okay
 

1739
00:44:45,540 --> 00:44:47,870
image and the color okay
so here's the simulation speed limit 80

1740
00:44:47,870 --> 00:44:47,880
so here's the simulation speed limit 80
 

1741
00:44:47,880 --> 00:44:48,880
so here's the simulation speed limit 80
miles an hour

1742
00:44:48,880 --> 00:44:48,890
miles an hour
 

1743
00:44:48,890 --> 00:44:52,579
miles an hour
cars 20 on the screen one of them is a

1744
00:44:52,579 --> 00:44:52,589
cars 20 on the screen one of them is a
 

1745
00:44:52,589 --> 00:44:54,890
cars 20 on the screen one of them is a
red one in this case that's that one is

1746
00:44:54,890 --> 00:44:54,900
red one in this case that's that one is
 

1747
00:44:54,900 --> 00:44:57,349
red one in this case that's that one is
controlled by a neural network its speed

1748
00:44:57,349 --> 00:44:57,359
controlled by a neural network its speed
 

1749
00:44:57,359 --> 00:44:59,210
controlled by a neural network its speed
it's allowed the actions of speed up

1750
00:44:59,210 --> 00:44:59,220
it's allowed the actions of speed up
 

1751
00:44:59,220 --> 00:45:03,230
it's allowed the actions of speed up
slow down change lanes left-right or

1752
00:45:03,230 --> 00:45:03,240
slow down change lanes left-right or
 

1753
00:45:03,240 --> 00:45:10,180
slow down change lanes left-right or
stay exactly the same the other cars are

1754
00:45:10,180 --> 00:45:10,190
stay exactly the same the other cars are
 

1755
00:45:10,190 --> 00:45:13,940
stay exactly the same the other cars are
pretty dumb they speed up slow down turn

1756
00:45:13,940 --> 00:45:13,950
pretty dumb they speed up slow down turn
 

1757
00:45:13,950 --> 00:45:16,190
pretty dumb they speed up slow down turn
left right but they don't have a purpose

1758
00:45:16,190 --> 00:45:16,200
left right but they don't have a purpose
 

1759
00:45:16,200 --> 00:45:19,630
left right but they don't have a purpose
in their existence they do so randomly

1760
00:45:19,630 --> 00:45:19,640
in their existence they do so randomly
 

1761
00:45:19,640 --> 00:45:21,859
in their existence they do so randomly
or at least purpose has not been

1762
00:45:21,859 --> 00:45:21,869
or at least purpose has not been
 

1763
00:45:21,869 --> 00:45:25,070
or at least purpose has not been
discovered the road the car the speed

1764
00:45:25,070 --> 00:45:25,080
discovered the road the car the speed
 

1765
00:45:25,080 --> 00:45:28,940
discovered the road the car the speed
the road is a grid space an occupancy

1766
00:45:28,940 --> 00:45:28,950
the road is a grid space an occupancy
 

1767
00:45:28,950 --> 00:45:32,510
the road is a grid space an occupancy
grid that specifies when it's empty

1768
00:45:32,510 --> 00:45:32,520
grid that specifies when it's empty
 

1769
00:45:32,520 --> 00:45:40,400
grid that specifies when it's empty
it's set to a B meaning that the the

1770
00:45:40,400 --> 00:45:40,410
it's set to a B meaning that the the
 

1771
00:45:40,410 --> 00:45:43,609
it's set to a B meaning that the the
grid value is whatever speed is

1772
00:45:43,609 --> 00:45:43,619
grid value is whatever speed is
 

1773
00:45:43,619 --> 00:45:45,920
grid value is whatever speed is
achievable if you were inside that grid

1774
00:45:45,920 --> 00:45:45,930
achievable if you were inside that grid
 

1775
00:45:45,930 --> 00:45:48,260
achievable if you were inside that grid
and when there's other cars that are

1776
00:45:48,260 --> 00:45:48,270
and when there's other cars that are
 

1777
00:45:48,270 --> 00:45:51,020
and when there's other cars that are
going slow the value in that grid is the

1778
00:45:51,020 --> 00:45:51,030
going slow the value in that grid is the
 

1779
00:45:51,030 --> 00:45:53,420
going slow the value in that grid is the
speed of that car that's the state space

1780
00:45:53,420 --> 00:45:53,430
speed of that car that's the state space
 

1781
00:45:53,430 --> 00:45:55,550
speed of that car that's the state space
that's the state representation and you

1782
00:45:55,550 --> 00:45:55,560
that's the state representation and you
 

1783
00:45:55,560 --> 00:45:57,980
that's the state representation and you
can choose how much what slice that

1784
00:45:57,980 --> 00:45:57,990
can choose how much what slice that
 

1785
00:45:57,990 --> 00:46:00,050
can choose how much what slice that
state space you take in that's the input

1786
00:46:00,050 --> 00:46:00,060
state space you take in that's the input
 

1787
00:46:00,060 --> 00:46:06,470
state space you take in that's the input
to the neural network for a visual

1788
00:46:06,470 --> 00:46:06,480
to the neural network for a visual
 

1789
00:46:06,480 --> 00:46:08,690
to the neural network for a visual
Asian purposes you can choose normal

1790
00:46:08,690 --> 00:46:08,700
Asian purposes you can choose normal
 

1791
00:46:08,700 --> 00:46:11,690
Asian purposes you can choose normal
speed or fast speed for watching the

1792
00:46:11,690 --> 00:46:11,700
speed or fast speed for watching the
 

1793
00:46:11,700 --> 00:46:15,830
speed or fast speed for watching the
network operate and there's display

1794
00:46:15,830 --> 00:46:15,840
network operate and there's display
 

1795
00:46:15,840 --> 00:46:18,050
network operate and there's display
options to help you build intuition

1796
00:46:18,050 --> 00:46:18,060
options to help you build intuition
 

1797
00:46:18,060 --> 00:46:19,940
options to help you build intuition
about the network takes in and what

1798
00:46:19,940 --> 00:46:19,950
about the network takes in and what
 

1799
00:46:19,950 --> 00:46:22,160
about the network takes in and what
space that car is operating in the

1800
00:46:22,160 --> 00:46:22,170
space that car is operating in the
 

1801
00:46:22,170 --> 00:46:25,040
space that car is operating in the
default is no extra information is added

1802
00:46:25,040 --> 00:46:25,050
default is no extra information is added
 

1803
00:46:25,050 --> 00:46:27,800
default is no extra information is added
then there's the learning input which

1804
00:46:27,800 --> 00:46:27,810
then there's the learning input which
 

1805
00:46:27,810 --> 00:46:30,320
then there's the learning input which
visualizes exactly which part of the

1806
00:46:30,320 --> 00:46:30,330
visualizes exactly which part of the
 

1807
00:46:30,330 --> 00:46:32,960
visualizes exactly which part of the
road the is serves as the input to the

1808
00:46:32,960 --> 00:46:32,970
road the is serves as the input to the
 

1809
00:46:32,970 --> 00:46:36,080
road the is serves as the input to the
network then there is the safety system

1810
00:46:36,080 --> 00:46:36,090
network then there is the safety system
 

1811
00:46:36,090 --> 00:46:37,700
network then there is the safety system
which I'll describe in a little bit

1812
00:46:37,700 --> 00:46:37,710
which I'll describe in a little bit
 

1813
00:46:37,710 --> 00:46:39,980
which I'll describe in a little bit
which is all the parts of the road the

1814
00:46:39,980 --> 00:46:39,990
which is all the parts of the road the
 

1815
00:46:39,990 --> 00:46:42,140
which is all the parts of the road the
car is not allowed to go into because it

1816
00:46:42,140 --> 00:46:42,150
car is not allowed to go into because it
 

1817
00:46:42,150 --> 00:46:44,030
car is not allowed to go into because it
would result in a collision and that

1818
00:46:44,030 --> 00:46:44,040
would result in a collision and that
 

1819
00:46:44,040 --> 00:46:45,380
would result in a collision and that
with JavaScript would be very difficult

1820
00:46:45,380 --> 00:46:45,390
with JavaScript would be very difficult
 

1821
00:46:45,390 --> 00:46:50,420
with JavaScript would be very difficult
to animate and the full map here's a

1822
00:46:50,420 --> 00:46:50,430
to animate and the full map here's a
 

1823
00:46:50,430 --> 00:46:52,250
to animate and the full map here's a
safety system you could think of this

1824
00:46:52,250 --> 00:46:52,260
safety system you could think of this
 

1825
00:46:52,260 --> 00:46:57,020
safety system you could think of this
system as a CC basic radar ultrasonic

1826
00:46:57,020 --> 00:46:57,030
system as a CC basic radar ultrasonic
 

1827
00:46:57,030 --> 00:46:59,530
system as a CC basic radar ultrasonic
sensors helping you avoid the obvious

1828
00:46:59,530 --> 00:46:59,540
sensors helping you avoid the obvious
 

1829
00:46:59,540 --> 00:47:02,210
sensors helping you avoid the obvious
collisions to obviously detectable

1830
00:47:02,210 --> 00:47:02,220
collisions to obviously detectable
 

1831
00:47:02,220 --> 00:47:04,520
collisions to obviously detectable
objects around you and the task for this

1832
00:47:04,520 --> 00:47:04,530
objects around you and the task for this
 

1833
00:47:04,530 --> 00:47:06,770
objects around you and the task for this
red car for the steel Network is to move

1834
00:47:06,770 --> 00:47:06,780
red car for the steel Network is to move
 

1835
00:47:06,780 --> 00:47:11,359
red car for the steel Network is to move
about this space is to move about the

1836
00:47:11,359 --> 00:47:11,369
about this space is to move about the
 

1837
00:47:11,369 --> 00:47:13,520
about this space is to move about the
space under the constraints of the

1838
00:47:13,520 --> 00:47:13,530
space under the constraints of the
 

1839
00:47:13,530 --> 00:47:17,750
space under the constraints of the
safety system the red shows all the

1840
00:47:17,750 --> 00:47:17,760
safety system the red shows all the
 

1841
00:47:17,760 --> 00:47:19,460
safety system the red shows all the
parts of the grid it's not able to move

1842
00:47:19,460 --> 00:47:19,470
parts of the grid it's not able to move
 

1843
00:47:19,470 --> 00:47:23,359
parts of the grid it's not able to move
into so the goal for the car is to not

1844
00:47:23,359 --> 00:47:23,369
into so the goal for the car is to not
 

1845
00:47:23,369 --> 00:47:26,660
into so the goal for the car is to not
get stuck in traffic it's make big

1846
00:47:26,660 --> 00:47:26,670
get stuck in traffic it's make big
 

1847
00:47:26,670 --> 00:47:32,109
get stuck in traffic it's make big
sweeping motions to avoid crowds of cars

1848
00:47:32,109 --> 00:47:32,119

 

1849
00:47:32,119 --> 00:47:35,930

the input like DQ n is the state space

1850
00:47:35,930 --> 00:47:35,940
the input like DQ n is the state space
 

1851
00:47:35,940 --> 00:47:38,210
the input like DQ n is the state space
the output is the value of the different

1852
00:47:38,210 --> 00:47:38,220
the output is the value of the different
 

1853
00:47:38,220 --> 00:47:41,180
the output is the value of the different
actions and based on the epsilon

1854
00:47:41,180 --> 00:47:41,190
actions and based on the epsilon
 

1855
00:47:41,190 --> 00:47:44,410
actions and based on the epsilon
parameter through training and through

1856
00:47:44,410 --> 00:47:44,420
parameter through training and through
 

1857
00:47:44,420 --> 00:47:47,960
parameter through training and through
inference evaluation process you choose

1858
00:47:47,960 --> 00:47:47,970
inference evaluation process you choose
 

1859
00:47:47,970 --> 00:47:50,120
inference evaluation process you choose
how much exploration you want to do

1860
00:47:50,120 --> 00:47:50,130
how much exploration you want to do
 

1861
00:47:50,130 --> 00:47:53,030
how much exploration you want to do
these are all parameters the learning is

1862
00:47:53,030 --> 00:47:53,040
these are all parameters the learning is
 

1863
00:47:53,040 --> 00:47:58,660
these are all parameters the learning is
done in the browser on your own computer

1864
00:47:58,660 --> 00:47:58,670

 

1865
00:47:58,670 --> 00:48:03,470

utilizing only the CPU the action space

1866
00:48:03,470 --> 00:48:03,480
utilizing only the CPU the action space
 

1867
00:48:03,480 --> 00:48:06,170
utilizing only the CPU the action space
there's five giving you some of the

1868
00:48:06,170 --> 00:48:06,180
there's five giving you some of the
 

1869
00:48:06,180 --> 00:48:07,940
there's five giving you some of the
variables here perhaps you go back to

1870
00:48:07,940 --> 00:48:07,950
variables here perhaps you go back to
 

1871
00:48:07,950 --> 00:48:10,970
variables here perhaps you go back to
the slides to look at it the brain quote

1872
00:48:10,970 --> 00:48:10,980
the slides to look at it the brain quote
 

1873
00:48:10,980 --> 00:48:14,390
the slides to look at it the brain quote
unquote is the thing that takes in the

1874
00:48:14,390 --> 00:48:14,400
unquote is the thing that takes in the
 

1875
00:48:14,400 --> 00:48:18,160
unquote is the thing that takes in the
state and the reward takes a four

1876
00:48:18,160 --> 00:48:18,170
state and the reward takes a four
 

1877
00:48:18,170 --> 00:48:19,720
state and the reward takes a four
passed through the state and produce to

1878
00:48:19,720 --> 00:48:19,730
passed through the state and produce to
 

1879
00:48:19,730 --> 00:48:22,720
passed through the state and produce to
the next action the brain is where the

1880
00:48:22,720 --> 00:48:22,730
the next action the brain is where the
 

1881
00:48:22,730 --> 00:48:25,060
the next action the brain is where the
neural network is contained both of the

1882
00:48:25,060 --> 00:48:25,070
neural network is contained both of the
 

1883
00:48:25,070 --> 00:48:28,240
neural network is contained both of the
training and the evaluation the learning

1884
00:48:28,240 --> 00:48:28,250
training and the evaluation the learning
 

1885
00:48:28,250 --> 00:48:32,590
training and the evaluation the learning
input can be controlled in width forward

1886
00:48:32,590 --> 00:48:32,600
input can be controlled in width forward
 

1887
00:48:32,600 --> 00:48:35,080
input can be controlled in width forward
length and backward length lane side

1888
00:48:35,080 --> 00:48:35,090
length and backward length lane side
 

1889
00:48:35,090 --> 00:48:37,210
length and backward length lane side
number of lanes to the side that you see

1890
00:48:37,210 --> 00:48:37,220
number of lanes to the side that you see
 

1891
00:48:37,220 --> 00:48:39,970
number of lanes to the side that you see
patches ahead as the patches ahead that

1892
00:48:39,970 --> 00:48:39,980
patches ahead as the patches ahead that
 

1893
00:48:39,980 --> 00:48:42,430
patches ahead as the patches ahead that
you see patches behind as patches behind

1894
00:48:42,430 --> 00:48:42,440
you see patches behind as patches behind
 

1895
00:48:42,440 --> 00:48:47,170
you see patches behind as patches behind
the you see mu this year can control the

1896
00:48:47,170 --> 00:48:47,180
the you see mu this year can control the
 

1897
00:48:47,180 --> 00:48:49,990
the you see mu this year can control the
number of agents that are controlled by

1898
00:48:49,990 --> 00:48:50,000
number of agents that are controlled by
 

1899
00:48:50,000 --> 00:48:53,880
number of agents that are controlled by
the neural network anywhere from one to

1900
00:48:53,880 --> 00:48:53,890
the neural network anywhere from one to
 

1901
00:48:53,890 --> 00:49:00,070
the neural network anywhere from one to
ten and the evaluation is performed

1902
00:49:00,070 --> 00:49:00,080
ten and the evaluation is performed
 

1903
00:49:00,080 --> 00:49:02,200
ten and the evaluation is performed
exactly the same way you have to achieve

1904
00:49:02,200 --> 00:49:02,210
exactly the same way you have to achieve
 

1905
00:49:02,210 --> 00:49:05,100
exactly the same way you have to achieve
the highest average speed for the agents

1906
00:49:05,100 --> 00:49:05,110
the highest average speed for the agents
 

1907
00:49:05,110 --> 00:49:08,440
the highest average speed for the agents
the very critical thing here is the

1908
00:49:08,440 --> 00:49:08,450
the very critical thing here is the
 

1909
00:49:08,450 --> 00:49:12,460
the very critical thing here is the
agents are not aware of each other so

1910
00:49:12,460 --> 00:49:12,470
agents are not aware of each other so
 

1911
00:49:12,470 --> 00:49:17,110
agents are not aware of each other so
they're not jointly jointly planning the

1912
00:49:17,110 --> 00:49:17,120
they're not jointly jointly planning the
 

1913
00:49:17,120 --> 00:49:20,520
they're not jointly jointly planning the
network is trained under the joint

1914
00:49:20,520 --> 00:49:20,530
network is trained under the joint
 

1915
00:49:20,530 --> 00:49:23,590
network is trained under the joint
objective of achieving the average speed

1916
00:49:23,590 --> 00:49:23,600
objective of achieving the average speed
 

1917
00:49:23,600 --> 00:49:24,520
objective of achieving the average speed
for all of them

1918
00:49:24,520 --> 00:49:24,530
for all of them
 

1919
00:49:24,530 --> 00:49:27,340
for all of them
but the actions are taking in a greedy

1920
00:49:27,340 --> 00:49:27,350
but the actions are taking in a greedy
 

1921
00:49:27,350 --> 00:49:30,790
but the actions are taking in a greedy
way for each it's very interesting what

1922
00:49:30,790 --> 00:49:30,800
way for each it's very interesting what
 

1923
00:49:30,800 --> 00:49:33,310
way for each it's very interesting what
can be learned in this way because this

1924
00:49:33,310 --> 00:49:33,320
can be learned in this way because this
 

1925
00:49:33,320 --> 00:49:35,800
can be learned in this way because this
kinds of approaches are scalable to an

1926
00:49:35,800 --> 00:49:35,810
kinds of approaches are scalable to an
 

1927
00:49:35,810 --> 00:49:38,170
kinds of approaches are scalable to an
arbitrary number of cars and you could

1928
00:49:38,170 --> 00:49:38,180
arbitrary number of cars and you could
 

1929
00:49:38,180 --> 00:49:41,230
arbitrary number of cars and you could
imagine us plopping down the best cars

1930
00:49:41,230 --> 00:49:41,240
imagine us plopping down the best cars
 

1931
00:49:41,240 --> 00:49:44,830
imagine us plopping down the best cars
from this class together and having them

1932
00:49:44,830 --> 00:49:44,840
from this class together and having them
 

1933
00:49:44,840 --> 00:49:47,950
from this class together and having them
compete in this way the best neural

1934
00:49:47,950 --> 00:49:47,960
compete in this way the best neural
 

1935
00:49:47,960 --> 00:49:52,180
compete in this way the best neural
networks because they're full in their

1936
00:49:52,180 --> 00:49:52,190
networks because they're full in their
 

1937
00:49:52,190 --> 00:49:55,540
networks because they're full in their
greedy operation the number of networks

1938
00:49:55,540 --> 00:49:55,550
greedy operation the number of networks
 

1939
00:49:55,550 --> 00:49:57,910
greedy operation the number of networks
that can concurrently operate is fully

1940
00:49:57,910 --> 00:49:57,920
that can concurrently operate is fully
 

1941
00:49:57,920 --> 00:50:01,060
that can concurrently operate is fully
scaleable there's a lot of parameters

1942
00:50:01,060 --> 00:50:01,070
scaleable there's a lot of parameters
 

1943
00:50:01,070 --> 00:50:07,900
scaleable there's a lot of parameters
the temporal window the layers the many

1944
00:50:07,900 --> 00:50:07,910
the temporal window the layers the many
 

1945
00:50:07,910 --> 00:50:10,240
the temporal window the layers the many
layers types that can be added here's a

1946
00:50:10,240 --> 00:50:10,250
layers types that can be added here's a
 

1947
00:50:10,250 --> 00:50:11,950
layers types that can be added here's a
fully connected layer with tenure ons

1948
00:50:11,950 --> 00:50:11,960
fully connected layer with tenure ons
 

1949
00:50:11,960 --> 00:50:14,230
fully connected layer with tenure ons
the activation functions all of these

1950
00:50:14,230 --> 00:50:14,240
the activation functions all of these
 

1951
00:50:14,240 --> 00:50:16,810
the activation functions all of these
things can be customized as specified in

1952
00:50:16,810 --> 00:50:16,820
things can be customized as specified in
 

1953
00:50:16,820 --> 00:50:20,350
things can be customized as specified in
the tutorial the final layer a fully

1954
00:50:20,350 --> 00:50:20,360
the tutorial the final layer a fully
 

1955
00:50:20,360 --> 00:50:23,790
the tutorial the final layer a fully
connected layer with output a five

1956
00:50:23,790 --> 00:50:23,800
connected layer with output a five
 

1957
00:50:23,800 --> 00:50:26,560
connected layer with output a five
regression giving the value of each of

1958
00:50:26,560 --> 00:50:26,570
regression giving the value of each of
 

1959
00:50:26,570 --> 00:50:29,710
regression giving the value of each of
the five actions and there's a lot of

1960
00:50:29,710 --> 00:50:29,720
the five actions and there's a lot of
 

1961
00:50:29,720 --> 00:50:31,750
the five actions and there's a lot of
more specific parameters some of which

1962
00:50:31,750 --> 00:50:31,760
more specific parameters some of which
 

1963
00:50:31,760 --> 00:50:32,250
more specific parameters some of which
have this

1964
00:50:32,250 --> 00:50:32,260
have this
 

1965
00:50:32,260 --> 00:50:39,390
have this
just from gamma to epsilon to experience

1966
00:50:39,390 --> 00:50:39,400
just from gamma to epsilon to experience
 

1967
00:50:39,400 --> 00:50:43,530
just from gamma to epsilon to experience
replay size to learning rate in temporal

1968
00:50:43,530 --> 00:50:43,540
replay size to learning rate in temporal
 

1969
00:50:43,540 --> 00:50:47,940
replay size to learning rate in temporal
window the optimizer the learning rate

1970
00:50:47,940 --> 00:50:47,950
window the optimizer the learning rate
 

1971
00:50:47,950 --> 00:50:51,420
window the optimizer the learning rate
momentum batch size l2 l1 to K for

1972
00:50:51,420 --> 00:50:51,430
momentum batch size l2 l1 to K for
 

1973
00:50:51,430 --> 00:50:54,120
momentum batch size l2 l1 to K for
regularization and so on there's a big

1974
00:50:54,120 --> 00:50:54,130
regularization and so on there's a big
 

1975
00:50:54,130 --> 00:50:56,130
regularization and so on there's a big
white button that says apply code that

1976
00:50:56,130 --> 00:50:56,140
white button that says apply code that
 

1977
00:50:56,140 --> 00:50:58,620
white button that says apply code that
you press that kills all the work you've

1978
00:50:58,620 --> 00:50:58,630
you press that kills all the work you've
 

1979
00:50:58,630 --> 00:51:00,360
you press that kills all the work you've
done up to this point so be careful

1980
00:51:00,360 --> 00:51:00,370
done up to this point so be careful
 

1981
00:51:00,370 --> 00:51:02,670
done up to this point so be careful
doing it it should be doing it only at

1982
00:51:02,670 --> 00:51:02,680
doing it it should be doing it only at
 

1983
00:51:02,680 --> 00:51:06,210
doing it it should be doing it only at
the very beginning if you happen to

1984
00:51:06,210 --> 00:51:06,220
the very beginning if you happen to
 

1985
00:51:06,220 --> 00:51:08,220
the very beginning if you happen to
leave your computer running in training

1986
00:51:08,220 --> 00:51:08,230
leave your computer running in training
 

1987
00:51:08,230 --> 00:51:10,350
leave your computer running in training
for several days as as folks have done

1988
00:51:10,350 --> 00:51:10,360
for several days as as folks have done
 

1989
00:51:10,360 --> 00:51:13,800
for several days as as folks have done
the blue training button you press and

1990
00:51:13,800 --> 00:51:13,810
the blue training button you press and
 

1991
00:51:13,810 --> 00:51:15,480
the blue training button you press and
it trains based on the parameters you

1992
00:51:15,480 --> 00:51:15,490
it trains based on the parameters you
 

1993
00:51:15,490 --> 00:51:18,540
it trains based on the parameters you
specify and the network state gets

1994
00:51:18,540 --> 00:51:18,550
specify and the network state gets
 

1995
00:51:18,550 --> 00:51:20,280
specify and the network state gets
shipped to the main simulation from time

1996
00:51:20,280 --> 00:51:20,290
shipped to the main simulation from time
 

1997
00:51:20,290 --> 00:51:22,260
shipped to the main simulation from time
to time so the thing you see in the

1998
00:51:22,260 --> 00:51:22,270
to time so the thing you see in the
 

1999
00:51:22,270 --> 00:51:24,270
to time so the thing you see in the
browser as you open up the web site is

2000
00:51:24,270 --> 00:51:24,280
browser as you open up the web site is
 

2001
00:51:24,280 --> 00:51:26,400
browser as you open up the web site is
running then the same network that's

2002
00:51:26,400 --> 00:51:26,410
running then the same network that's
 

2003
00:51:26,410 --> 00:51:29,070
running then the same network that's
being trained and regularly it updates

2004
00:51:29,070 --> 00:51:29,080
being trained and regularly it updates
 

2005
00:51:29,080 --> 00:51:30,600
being trained and regularly it updates
that network so it's getting better and

2006
00:51:30,600 --> 00:51:30,610
that network so it's getting better and
 

2007
00:51:30,610 --> 00:51:32,580
that network so it's getting better and
better even if the training takes weeks

2008
00:51:32,580 --> 00:51:32,590
better even if the training takes weeks
 

2009
00:51:32,590 --> 00:51:35,430
better even if the training takes weeks
for you it's constantly updating the

2010
00:51:35,430 --> 00:51:35,440
for you it's constantly updating the
 

2011
00:51:35,440 --> 00:51:37,500
for you it's constantly updating the
network you see on the left so if the

2012
00:51:37,500 --> 00:51:37,510
network you see on the left so if the
 

2013
00:51:37,510 --> 00:51:39,540
network you see on the left so if the
car for the network that you're training

2014
00:51:39,540 --> 00:51:39,550
car for the network that you're training
 

2015
00:51:39,550 --> 00:51:41,840
car for the network that you're training
is just standing in place and not moving

2016
00:51:41,840 --> 00:51:41,850
is just standing in place and not moving
 

2017
00:51:41,850 --> 00:51:46,440
is just standing in place and not moving
it's probably time to restart and change

2018
00:51:46,440 --> 00:51:46,450
it's probably time to restart and change
 

2019
00:51:46,450 --> 00:51:48,870
it's probably time to restart and change
the parameters maybe add a few layers to

2020
00:51:48,870 --> 00:51:48,880
the parameters maybe add a few layers to
 

2021
00:51:48,880 --> 00:51:52,200
the parameters maybe add a few layers to
your network number of iterations is

2022
00:51:52,200 --> 00:51:52,210
your network number of iterations is
 

2023
00:51:52,210 --> 00:51:54,330
your network number of iterations is
certainly an important parameter to

2024
00:51:54,330 --> 00:51:54,340
certainly an important parameter to
 

2025
00:51:54,340 --> 00:51:58,080
certainly an important parameter to
control and the evaluation is something

2026
00:51:58,080 --> 00:51:58,090
control and the evaluation is something
 

2027
00:51:58,090 --> 00:52:00,180
control and the evaluation is something
we've done a lot of worked on since last

2028
00:52:00,180 --> 00:52:00,190
we've done a lot of worked on since last
 

2029
00:52:00,190 --> 00:52:02,850
we've done a lot of worked on since last
year to remove the degree of randomness

2030
00:52:02,850 --> 00:52:02,860
year to remove the degree of randomness
 

2031
00:52:02,860 --> 00:52:07,260
year to remove the degree of randomness
to remove the the incentive to submit

2032
00:52:07,260 --> 00:52:07,270
to remove the the incentive to submit
 

2033
00:52:07,270 --> 00:52:09,150
to remove the the incentive to submit
the same code over and over again to

2034
00:52:09,150 --> 00:52:09,160
the same code over and over again to
 

2035
00:52:09,160 --> 00:52:11,880
the same code over and over again to
hope to produce a higher reward a higher

2036
00:52:11,880 --> 00:52:11,890
hope to produce a higher reward a higher
 

2037
00:52:11,890 --> 00:52:15,780
hope to produce a higher reward a higher
evaluation score the method for

2038
00:52:15,780 --> 00:52:15,790
evaluation score the method for
 

2039
00:52:15,790 --> 00:52:17,760
evaluation score the method for
evaluation is we collect the average

2040
00:52:17,760 --> 00:52:17,770
evaluation is we collect the average
 

2041
00:52:17,770 --> 00:52:23,400
evaluation is we collect the average
speed over ten runs about 45 seconds of

2042
00:52:23,400 --> 00:52:23,410
speed over ten runs about 45 seconds of
 

2043
00:52:23,410 --> 00:52:27,570
speed over ten runs about 45 seconds of
game each not minutes 45 simulated

2044
00:52:27,570 --> 00:52:27,580
game each not minutes 45 simulated
 

2045
00:52:27,580 --> 00:52:30,630
game each not minutes 45 simulated
seconds and there is five hundreds of

2046
00:52:30,630 --> 00:52:30,640
seconds and there is five hundreds of
 

2047
00:52:30,640 --> 00:52:32,370
seconds and there is five hundreds of
those and we take the median speed of

2048
00:52:32,370 --> 00:52:32,380
those and we take the median speed of
 

2049
00:52:32,380 --> 00:52:35,670
those and we take the median speed of
the 500 runs it's done server-side so

2050
00:52:35,670 --> 00:52:35,680
the 500 runs it's done server-side so
 

2051
00:52:35,680 --> 00:52:37,920
the 500 runs it's done server-side so
extremely difficult to cheat I urge you

2052
00:52:37,920 --> 00:52:37,930
extremely difficult to cheat I urge you
 

2053
00:52:37,930 --> 00:52:42,090
extremely difficult to cheat I urge you
to try you can try it locally there's a

2054
00:52:42,090 --> 00:52:42,100
to try you can try it locally there's a
 

2055
00:52:42,100 --> 00:52:43,920
to try you can try it locally there's a
start evaluation run but that one

2056
00:52:43,920 --> 00:52:43,930
start evaluation run but that one
 

2057
00:52:43,930 --> 00:52:45,450
start evaluation run but that one
doesn't count that's just for you to

2058
00:52:45,450 --> 00:52:45,460
doesn't count that's just for you to
 

2059
00:52:45,460 --> 00:52:46,030
doesn't count that's just for you to
feel better

2060
00:52:46,030 --> 00:52:46,040
feel better
 

2061
00:52:46,040 --> 00:52:49,300
feel better
by you network that's that should

2062
00:52:49,300 --> 00:52:49,310
by you network that's that should
 

2063
00:52:49,310 --> 00:52:51,070
by you network that's that should
produce a result that's very similar to

2064
00:52:51,070 --> 00:52:51,080
produce a result that's very similar to
 

2065
00:52:51,080 --> 00:52:52,800
produce a result that's very similar to
the one we were produced on the server

2066
00:52:52,800 --> 00:52:52,810
the one we were produced on the server
 

2067
00:52:52,810 --> 00:52:56,800
the one we were produced on the server
it's to build your own intuition and as

2068
00:52:56,800 --> 00:52:56,810
it's to build your own intuition and as
 

2069
00:52:56,810 --> 00:52:58,330
it's to build your own intuition and as
I said we significantly reduce the

2070
00:52:58,330 --> 00:52:58,340
I said we significantly reduce the
 

2071
00:52:58,340 --> 00:53:01,720
I said we significantly reduce the
influence of randomness so the the score

2072
00:53:01,720 --> 00:53:01,730
influence of randomness so the the score
 

2073
00:53:01,730 --> 00:53:03,760
influence of randomness so the the score
the speed you get for the network you

2074
00:53:03,760 --> 00:53:03,770
the speed you get for the network you
 

2075
00:53:03,770 --> 00:53:06,760
the speed you get for the network you
design should be very similar with every

2076
00:53:06,760 --> 00:53:06,770
design should be very similar with every
 

2077
00:53:06,770 --> 00:53:10,870
design should be very similar with every
valuation loading is saving if the

2078
00:53:10,870 --> 00:53:10,880
valuation loading is saving if the
 

2079
00:53:10,880 --> 00:53:12,700
valuation loading is saving if the
network is huge and you want to switch

2080
00:53:12,700 --> 00:53:12,710
network is huge and you want to switch
 

2081
00:53:12,710 --> 00:53:14,500
network is huge and you want to switch
computers you can save the network it

2082
00:53:14,500 --> 00:53:14,510
computers you can save the network it
 

2083
00:53:14,510 --> 00:53:16,030
computers you can save the network it
saves both the architecture of the

2084
00:53:16,030 --> 00:53:16,040
saves both the architecture of the
 

2085
00:53:16,040 --> 00:53:18,250
saves both the architecture of the
network and the weights and the on the

2086
00:53:18,250 --> 00:53:18,260
network and the weights and the on the
 

2087
00:53:18,260 --> 00:53:22,530
network and the weights and the on the
network and you can load it back in

2088
00:53:22,530 --> 00:53:22,540
network and you can load it back in
 

2089
00:53:22,540 --> 00:53:26,550
network and you can load it back in
obviously when you load it in it's not

2090
00:53:26,550 --> 00:53:26,560
obviously when you load it in it's not
 

2091
00:53:26,560 --> 00:53:28,750
obviously when you load it in it's not
saving any of the data you've already

2092
00:53:28,750 --> 00:53:28,760
saving any of the data you've already
 

2093
00:53:28,760 --> 00:53:31,270
saving any of the data you've already
done you can't do transfer learning with

2094
00:53:31,270 --> 00:53:31,280
done you can't do transfer learning with
 

2095
00:53:31,280 --> 00:53:35,050
done you can't do transfer learning with
javascript in the browser yet submitting

2096
00:53:35,050 --> 00:53:35,060
javascript in the browser yet submitting
 

2097
00:53:35,060 --> 00:53:37,390
javascript in the browser yet submitting
your network submit model to competition

2098
00:53:37,390 --> 00:53:37,400
your network submit model to competition
 

2099
00:53:37,400 --> 00:53:39,430
your network submit model to competition
and make sure you run training first

2100
00:53:39,430 --> 00:53:39,440
and make sure you run training first
 

2101
00:53:39,440 --> 00:53:42,730
and make sure you run training first
otherwise it'll be initiated the way to

2102
00:53:42,730 --> 00:53:42,740
otherwise it'll be initiated the way to
 

2103
00:53:42,740 --> 00:53:44,560
otherwise it'll be initiated the way to
initiate it randomly and will not do so

2104
00:53:44,560 --> 00:53:44,570
initiate it randomly and will not do so
 

2105
00:53:44,570 --> 00:53:44,920
initiate it randomly and will not do so
well

2106
00:53:44,920 --> 00:53:44,930
well
 

2107
00:53:44,930 --> 00:53:47,500
well
you can resubmit us off and you like and

2108
00:53:47,500 --> 00:53:47,510
you can resubmit us off and you like and
 

2109
00:53:47,510 --> 00:53:50,200
you can resubmit us off and you like and
the highest score is what counts the

2110
00:53:50,200 --> 00:53:50,210
the highest score is what counts the
 

2111
00:53:50,210 --> 00:53:52,150
the highest score is what counts the
coolest part is you can load your custom

2112
00:53:52,150 --> 00:53:52,160
coolest part is you can load your custom
 

2113
00:53:52,160 --> 00:53:55,300
coolest part is you can load your custom
image specify colors and request the

2114
00:53:55,300 --> 00:53:55,310
image specify colors and request the
 

2115
00:53:55,310 --> 00:53:59,830
image specify colors and request the
visualization we have not yet shown the

2116
00:53:59,830 --> 00:53:59,840
visualization we have not yet shown the
 

2117
00:53:59,840 --> 00:54:01,450
visualization we have not yet shown the
visualization but I promise you it's

2118
00:54:01,450 --> 00:54:01,460
visualization but I promise you it's
 

2119
00:54:01,460 --> 00:54:04,210
visualization but I promise you it's
going to be awesome again read the

2120
00:54:04,210 --> 00:54:04,220
going to be awesome again read the
 

2121
00:54:04,220 --> 00:54:06,160
going to be awesome again read the
tutorial change the parameters in the

2122
00:54:06,160 --> 00:54:06,170
tutorial change the parameters in the
 

2123
00:54:06,170 --> 00:54:08,610
tutorial change the parameters in the
code box click apply code run training

2124
00:54:08,610 --> 00:54:08,620
code box click apply code run training
 

2125
00:54:08,620 --> 00:54:11,050
code box click apply code run training
everybody in this room on the way home

2126
00:54:11,050 --> 00:54:11,060
everybody in this room on the way home
 

2127
00:54:11,060 --> 00:54:13,900
everybody in this room on the way home
on the train hopefully not in your car

2128
00:54:13,900 --> 00:54:13,910
on the train hopefully not in your car
 

2129
00:54:13,910 --> 00:54:15,490
on the train hopefully not in your car
should be able to do this in the browser

2130
00:54:15,490 --> 00:54:15,500
should be able to do this in the browser
 

2131
00:54:15,500 --> 00:54:18,040
should be able to do this in the browser
and then you can visualize request

2132
00:54:18,040 --> 00:54:18,050
and then you can visualize request
 

2133
00:54:18,050 --> 00:54:19,480
and then you can visualize request
visualization because it's an expensive

2134
00:54:19,480 --> 00:54:19,490
visualization because it's an expensive
 

2135
00:54:19,490 --> 00:54:22,540
visualization because it's an expensive
process you have to want it for us to do

2136
00:54:22,540 --> 00:54:22,550
process you have to want it for us to do
 

2137
00:54:22,550 --> 00:54:27,750
process you have to want it for us to do
it because we have to run in server-side

2138
00:54:27,750 --> 00:54:27,760

 

2139
00:54:27,760 --> 00:54:31,810

competition link is there github starter

2140
00:54:31,810 --> 00:54:31,820
competition link is there github starter
 

2141
00:54:31,820 --> 00:54:34,180
competition link is there github starter
code is there and the details for those

2142
00:54:34,180 --> 00:54:34,190
code is there and the details for those
 

2143
00:54:34,190 --> 00:54:35,800
code is there and the details for those
that truly want to win is in the archive

2144
00:54:35,800 --> 00:54:35,810
that truly want to win is in the archive
 

2145
00:54:35,810 --> 00:54:39,940
that truly want to win is in the archive
paper so the question that will come up

2146
00:54:39,940 --> 00:54:39,950
paper so the question that will come up
 

2147
00:54:39,950 --> 00:54:41,590
paper so the question that will come up
throughout is whether these

2148
00:54:41,590 --> 00:54:41,600
throughout is whether these
 

2149
00:54:41,600 --> 00:54:44,110
throughout is whether these
reinforcement learning approaches are at

2150
00:54:44,110 --> 00:54:44,120
reinforcement learning approaches are at
 

2151
00:54:44,120 --> 00:54:46,930
reinforcement learning approaches are at
all or rather if action planning control

2152
00:54:46,930 --> 00:54:46,940
all or rather if action planning control
 

2153
00:54:46,940 --> 00:54:51,550
all or rather if action planning control
is amenable to learning certainly in the

2154
00:54:51,550 --> 00:54:51,560
is amenable to learning certainly in the
 

2155
00:54:51,560 --> 00:54:54,490
is amenable to learning certainly in the
case of driving we can't do it alpha go

2156
00:54:54,490 --> 00:54:54,500
case of driving we can't do it alpha go
 

2157
00:54:54,500 --> 00:54:56,910
case of driving we can't do it alpha go
zero did we can

2158
00:54:56,910 --> 00:54:56,920
zero did we can
 

2159
00:54:56,920 --> 00:54:59,550
zero did we can
learn from scratch from self play

2160
00:54:59,550 --> 00:54:59,560
learn from scratch from self play
 

2161
00:54:59,560 --> 00:55:02,640
learn from scratch from self play
because that will result in millions of

2162
00:55:02,640 --> 00:55:02,650
because that will result in millions of
 

2163
00:55:02,650 --> 00:55:07,200
because that will result in millions of
crashes in order to learn to avoid the

2164
00:55:07,200 --> 00:55:07,210
crashes in order to learn to avoid the
 

2165
00:55:07,210 --> 00:55:09,840
crashes in order to learn to avoid the
crashes unless we're working like we are

2166
00:55:09,840 --> 00:55:09,850
crashes unless we're working like we are
 

2167
00:55:09,850 --> 00:55:12,240
crashes unless we're working like we are
deep crash on the RC car or we're

2168
00:55:12,240 --> 00:55:12,250
deep crash on the RC car or we're
 

2169
00:55:12,250 --> 00:55:15,030
deep crash on the RC car or we're
working in a simulation so we can look

2170
00:55:15,030 --> 00:55:15,040
working in a simulation so we can look
 

2171
00:55:15,040 --> 00:55:16,860
working in a simulation so we can look
at export data we can look at driver

2172
00:55:16,860 --> 00:55:16,870
at export data we can look at driver
 

2173
00:55:16,870 --> 00:55:18,480
at export data we can look at driver
data which we have a lot of and learn

2174
00:55:18,480 --> 00:55:18,490
data which we have a lot of and learn
 

2175
00:55:18,490 --> 00:55:20,400
data which we have a lot of and learn
from it's an open question whether this

2176
00:55:20,400 --> 00:55:20,410
from it's an open question whether this
 

2177
00:55:20,410 --> 00:55:24,720
from it's an open question whether this
is applicable to date and I'll bring up

2178
00:55:24,720 --> 00:55:24,730
is applicable to date and I'll bring up
 

2179
00:55:24,730 --> 00:55:26,250
is applicable to date and I'll bring up
two companies because they're both guest

2180
00:55:26,250 --> 00:55:26,260
two companies because they're both guest
 

2181
00:55:26,260 --> 00:55:30,240
two companies because they're both guest
speakers deep IRL is not involved in the

2182
00:55:30,240 --> 00:55:30,250
speakers deep IRL is not involved in the
 

2183
00:55:30,250 --> 00:55:32,550
speakers deep IRL is not involved in the
most successful robots operating in the

2184
00:55:32,550 --> 00:55:32,560
most successful robots operating in the
 

2185
00:55:32,560 --> 00:55:36,690
most successful robots operating in the
real world in the case of Boston

2186
00:55:36,690 --> 00:55:36,700
real world in the case of Boston
 

2187
00:55:36,700 --> 00:55:42,270
real world in the case of Boston
Dynamics most of the perception control

2188
00:55:42,270 --> 00:55:42,280
Dynamics most of the perception control
 

2189
00:55:42,280 --> 00:55:45,900
Dynamics most of the perception control
and planning like in this robot does not

2190
00:55:45,900 --> 00:55:45,910
and planning like in this robot does not
 

2191
00:55:45,910 --> 00:55:48,990
and planning like in this robot does not
involve learning approaches except with

2192
00:55:48,990 --> 00:55:49,000
involve learning approaches except with
 

2193
00:55:49,000 --> 00:55:52,250
involve learning approaches except with
minimal addition on the perception side

2194
00:55:52,250 --> 00:55:52,260
minimal addition on the perception side
 

2195
00:55:52,260 --> 00:55:56,310
minimal addition on the perception side
best of our knowledge and certainly the

2196
00:55:56,310 --> 00:55:56,320
best of our knowledge and certainly the
 

2197
00:55:56,320 --> 00:55:59,040
best of our knowledge and certainly the
same is true with Wei MO as the speaker

2198
00:55:59,040 --> 00:55:59,050
same is true with Wei MO as the speaker
 

2199
00:55:59,050 --> 00:56:01,350
same is true with Wei MO as the speaker
on Friday will talk about deep learning

2200
00:56:01,350 --> 00:56:01,360
on Friday will talk about deep learning
 

2201
00:56:01,360 --> 00:56:03,300
on Friday will talk about deep learning
is used a little bit in perception on

2202
00:56:03,300 --> 00:56:03,310
is used a little bit in perception on
 

2203
00:56:03,310 --> 00:56:06,090
is used a little bit in perception on
top but most of the work is done from

2204
00:56:06,090 --> 00:56:06,100
top but most of the work is done from
 

2205
00:56:06,100 --> 00:56:09,990
top but most of the work is done from
the sensors and the optimization base

2206
00:56:09,990 --> 00:56:10,000
the sensors and the optimization base
 

2207
00:56:10,000 --> 00:56:12,510
the sensors and the optimization base
the model-based approaches trajectory

2208
00:56:12,510 --> 00:56:12,520
the model-based approaches trajectory
 

2209
00:56:12,520 --> 00:56:14,400
the model-based approaches trajectory
generation and optimizing which

2210
00:56:14,400 --> 00:56:14,410
generation and optimizing which
 

2211
00:56:14,410 --> 00:56:17,430
generation and optimizing which
trajectory trajectory is best to avoid

2212
00:56:17,430 --> 00:56:17,440
trajectory trajectory is best to avoid
 

2213
00:56:17,440 --> 00:56:23,090
trajectory trajectory is best to avoid
collisions deep IRL is not involved and

2214
00:56:23,090 --> 00:56:23,100
collisions deep IRL is not involved and
 

2215
00:56:23,100 --> 00:56:25,130
collisions deep IRL is not involved and
coming back and back again

2216
00:56:25,130 --> 00:56:25,140
coming back and back again
 

2217
00:56:25,140 --> 00:56:27,750
coming back and back again
the unexpected local POC is a high

2218
00:56:27,750 --> 00:56:27,760
the unexpected local POC is a high
 

2219
00:56:27,760 --> 00:56:30,000
the unexpected local POC is a high
reward which arises in all of these

2220
00:56:30,000 --> 00:56:30,010
reward which arises in all of these
 

2221
00:56:30,010 --> 00:56:31,850
reward which arises in all of these
situations and apply in the real world

2222
00:56:31,850 --> 00:56:31,860
situations and apply in the real world
 

2223
00:56:31,860 --> 00:56:35,670
situations and apply in the real world
so for the cat video that's pretty short

2224
00:56:35,670 --> 00:56:35,680
so for the cat video that's pretty short
 

2225
00:56:35,680 --> 00:56:37,620
so for the cat video that's pretty short
where the cats are ringing the bell and

2226
00:56:37,620 --> 00:56:37,630
where the cats are ringing the bell and
 

2227
00:56:37,630 --> 00:56:39,870
where the cats are ringing the bell and
they're learning that the ring in the

2228
00:56:39,870 --> 00:56:39,880
they're learning that the ring in the
 

2229
00:56:39,880 --> 00:56:46,050
they're learning that the ring in the
bell is is mapping to food I urge you to

2230
00:56:46,050 --> 00:56:46,060
bell is is mapping to food I urge you to
 

2231
00:56:46,060 --> 00:56:48,180
bell is is mapping to food I urge you to
think about how that can evolve over

2232
00:56:48,180 --> 00:56:48,190
think about how that can evolve over
 

2233
00:56:48,190 --> 00:56:51,210
think about how that can evolve over
time in unexpected ways they may not

2234
00:56:51,210 --> 00:56:51,220
time in unexpected ways they may not
 

2235
00:56:51,220 --> 00:56:54,030
time in unexpected ways they may not
have a desirable effect where the final

2236
00:56:54,030 --> 00:56:54,040
have a desirable effect where the final
 

2237
00:56:54,040 --> 00:56:58,050
have a desirable effect where the final
reward is in the form of food and the

2238
00:56:58,050 --> 00:56:58,060
reward is in the form of food and the
 

2239
00:56:58,060 --> 00:57:02,570
reward is in the form of food and the
intended effect is to ring the bell

2240
00:57:02,570 --> 00:57:02,580

 

2241
00:57:02,580 --> 00:57:03,800

that's

2242
00:57:03,800 --> 00:57:03,810
that's
 

2243
00:57:03,810 --> 00:57:06,710
that's
ASAT comes in for the artificial general

2244
00:57:06,710 --> 00:57:06,720
ASAT comes in for the artificial general
 

2245
00:57:06,720 --> 00:57:08,540
ASAT comes in for the artificial general
intelligence course in two weeks that

2246
00:57:08,540 --> 00:57:08,550
intelligence course in two weeks that
 

2247
00:57:08,550 --> 00:57:11,660
intelligence course in two weeks that
something will explore extensively its

2248
00:57:11,660 --> 00:57:11,670
something will explore extensively its
 

2249
00:57:11,670 --> 00:57:15,610
something will explore extensively its
how these reinforcement learning

2250
00:57:15,610 --> 00:57:15,620
how these reinforcement learning
 

2251
00:57:15,620 --> 00:57:18,920
how these reinforcement learning
planning algorithms will evolve in ways

2252
00:57:18,920 --> 00:57:18,930
planning algorithms will evolve in ways
 

2253
00:57:18,930 --> 00:57:22,250
planning algorithms will evolve in ways
they're not expected and how we can

2254
00:57:22,250 --> 00:57:22,260
they're not expected and how we can
 

2255
00:57:22,260 --> 00:57:24,830
they're not expected and how we can
constrain them how we can design reward

2256
00:57:24,830 --> 00:57:24,840
constrain them how we can design reward
 

2257
00:57:24,840 --> 00:57:28,630
constrain them how we can design reward
functions that result in safe operation

2258
00:57:28,630 --> 00:57:28,640
functions that result in safe operation
 

2259
00:57:28,640 --> 00:57:31,580
functions that result in safe operation
so I encourage you to come to the talk

2260
00:57:31,580 --> 00:57:31,590
so I encourage you to come to the talk
 

2261
00:57:31,590 --> 00:57:35,090
so I encourage you to come to the talk
on Friday at 1:00 p.m. as a reminder so

2262
00:57:35,090 --> 00:57:35,100
on Friday at 1:00 p.m. as a reminder so
 

2263
00:57:35,100 --> 00:57:38,390
on Friday at 1:00 p.m. as a reminder so
1:00 p.m. not 7:00 p.m. in Stata 32 one

2264
00:57:38,390 --> 00:57:38,400
1:00 p.m. not 7:00 p.m. in Stata 32 one
 

2265
00:57:38,400 --> 00:57:40,970
1:00 p.m. not 7:00 p.m. in Stata 32 one
two three and two the awesome talks in

2266
00:57:40,970 --> 00:57:40,980
two three and two the awesome talks in
 

2267
00:57:40,980 --> 00:57:43,850
two three and two the awesome talks in
two weeks from Boston Dynamics to Ray

2268
00:57:43,850 --> 00:57:43,860
two weeks from Boston Dynamics to Ray
 

2269
00:57:43,860 --> 00:57:47,420
two weeks from Boston Dynamics to Ray
Kurzweil and so on for AGI now tomorrow

2270
00:57:47,420 --> 00:57:47,430
Kurzweil and so on for AGI now tomorrow
 

2271
00:57:47,430 --> 00:57:49,820
Kurzweil and so on for AGI now tomorrow
we'll talk about computer vision and

2272
00:57:49,820 --> 00:57:49,830
we'll talk about computer vision and
 

2273
00:57:49,830 --> 00:57:52,770
we'll talk about computer vision and
psyche fuse thank you everybody

2274
00:57:52,770 --> 00:57:52,780
psyche fuse thank you everybody
 

2275
00:57:52,780 --> 00:57:56,589
psyche fuse thank you everybody
[Applause]

