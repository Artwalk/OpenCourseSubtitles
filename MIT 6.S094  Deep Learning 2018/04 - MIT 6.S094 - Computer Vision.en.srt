1
00:00:00,089 --> 00:00:03,260

today we'll talk about how to make

2
00:00:03,260 --> 00:00:03,270
today we'll talk about how to make
 

3
00:00:03,270 --> 00:00:06,320
today we'll talk about how to make
machines see computer vision and we'll

4
00:00:06,320 --> 00:00:06,330
machines see computer vision and we'll
 

5
00:00:06,330 --> 00:00:10,810
machines see computer vision and we'll
present Thank You Claire said yes and

6
00:00:10,810 --> 00:00:10,820
present Thank You Claire said yes and
 

7
00:00:10,820 --> 00:00:14,570
present Thank You Claire said yes and
today we will present a competition that

8
00:00:14,570 --> 00:00:14,580
today we will present a competition that
 

9
00:00:14,580 --> 00:00:18,650
today we will present a competition that
unlike deep traffic which is designed to

10
00:00:18,650 --> 00:00:18,660
unlike deep traffic which is designed to
 

11
00:00:18,660 --> 00:00:22,910
unlike deep traffic which is designed to
explore ideas teach you about concepts

12
00:00:22,910 --> 00:00:22,920
explore ideas teach you about concepts
 

13
00:00:22,920 --> 00:00:26,269
explore ideas teach you about concepts
of deep reinforcement learning seg fuse

14
00:00:26,269 --> 00:00:26,279
of deep reinforcement learning seg fuse
 

15
00:00:26,279 --> 00:00:28,790
of deep reinforcement learning seg fuse
the deep dynamic driving scene

16
00:00:28,790 --> 00:00:28,800
the deep dynamic driving scene
 

17
00:00:28,800 --> 00:00:30,950
the deep dynamic driving scene
segmentation competition that I'll

18
00:00:30,950 --> 00:00:30,960
segmentation competition that I'll
 

19
00:00:30,960 --> 00:00:33,709
segmentation competition that I'll
present today is at the very cutting

20
00:00:33,709 --> 00:00:33,719
present today is at the very cutting
 

21
00:00:33,719 --> 00:00:36,709
present today is at the very cutting
edge whoever does well in this

22
00:00:36,709 --> 00:00:36,719
edge whoever does well in this
 

23
00:00:36,719 --> 00:00:39,430
edge whoever does well in this
competition is likely to produce a

24
00:00:39,430 --> 00:00:39,440
competition is likely to produce a
 

25
00:00:39,440 --> 00:00:42,650
competition is likely to produce a
publication or ideas that would lead the

26
00:00:42,650 --> 00:00:42,660
publication or ideas that would lead the
 

27
00:00:42,660 --> 00:00:46,400
publication or ideas that would lead the
world in the area of perception perhaps

28
00:00:46,400 --> 00:00:46,410
world in the area of perception perhaps
 

29
00:00:46,410 --> 00:00:48,529
world in the area of perception perhaps
together with the people running this

30
00:00:48,529 --> 00:00:48,539
together with the people running this
 

31
00:00:48,539 --> 00:00:51,319
together with the people running this
class perhaps in your own and I

32
00:00:51,319 --> 00:00:51,329
class perhaps in your own and I
 

33
00:00:51,329 --> 00:00:56,930
class perhaps in your own and I
encourage you to do so even more cats

34
00:00:56,930 --> 00:00:56,940
encourage you to do so even more cats
 

35
00:00:56,940 --> 00:01:01,910
encourage you to do so even more cats
today computer vision today as it stands

36
00:01:01,910 --> 00:01:01,920
today computer vision today as it stands
 

37
00:01:01,920 --> 00:01:06,170
today computer vision today as it stands
is deep learning majority of the

38
00:01:06,170 --> 00:01:06,180
is deep learning majority of the
 

39
00:01:06,180 --> 00:01:08,840
is deep learning majority of the
successes in how we interpret form

40
00:01:08,840 --> 00:01:08,850
successes in how we interpret form
 

41
00:01:08,850 --> 00:01:11,719
successes in how we interpret form
representations understand images and

42
00:01:11,719 --> 00:01:11,729
representations understand images and
 

43
00:01:11,729 --> 00:01:14,929
representations understand images and
videos utilize to a significant degree

44
00:01:14,929 --> 00:01:14,939
videos utilize to a significant degree
 

45
00:01:14,939 --> 00:01:18,050
videos utilize to a significant degree
neural networks the very ideas we've

46
00:01:18,050 --> 00:01:18,060
neural networks the very ideas we've
 

47
00:01:18,060 --> 00:01:20,300
neural networks the very ideas we've
been talking about that applies for

48
00:01:20,300 --> 00:01:20,310
been talking about that applies for
 

49
00:01:20,310 --> 00:01:22,929
been talking about that applies for
supervised unsupervised and

50
00:01:22,929 --> 00:01:22,939
supervised unsupervised and
 

51
00:01:22,939 --> 00:01:26,660
supervised unsupervised and
reinforcement learning and for the

52
00:01:26,660 --> 00:01:26,670
reinforcement learning and for the
 

53
00:01:26,670 --> 00:01:29,060
reinforcement learning and for the
supervised case is just the focus of

54
00:01:29,060 --> 00:01:29,070
supervised case is just the focus of
 

55
00:01:29,070 --> 00:01:33,800
supervised case is just the focus of
today the process is the same the data

56
00:01:33,800 --> 00:01:33,810
today the process is the same the data
 

57
00:01:33,810 --> 00:01:36,020
today the process is the same the data
is essential there's annotated data

58
00:01:36,020 --> 00:01:36,030
is essential there's annotated data
 

59
00:01:36,030 --> 00:01:38,480
is essential there's annotated data
where the human provides the labels that

60
00:01:38,480 --> 00:01:38,490
where the human provides the labels that
 

61
00:01:38,490 --> 00:01:40,010
where the human provides the labels that
serves as the ground truth in the

62
00:01:40,010 --> 00:01:40,020
serves as the ground truth in the
 

63
00:01:40,020 --> 00:01:43,030
serves as the ground truth in the
training process then the neural network

64
00:01:43,030 --> 00:01:43,040
training process then the neural network
 

65
00:01:43,040 --> 00:01:47,960
training process then the neural network
ghost's through that data learning to

66
00:01:47,960 --> 00:01:47,970
ghost's through that data learning to
 

67
00:01:47,970 --> 00:01:51,679
ghost's through that data learning to
map from the raw sensory input to the

68
00:01:51,679 --> 00:01:51,689
map from the raw sensory input to the
 

69
00:01:51,689 --> 00:01:53,899
map from the raw sensory input to the
ground truth labels and then generalize

70
00:01:53,899 --> 00:01:53,909
ground truth labels and then generalize
 

71
00:01:53,909 --> 00:01:57,920
ground truth labels and then generalize
or the testing data set and the kind of

72
00:01:57,920 --> 00:01:57,930
or the testing data set and the kind of
 

73
00:01:57,930 --> 00:01:59,480
or the testing data set and the kind of
raw sensors were dealing with their

74
00:01:59,480 --> 00:01:59,490
raw sensors were dealing with their
 

75
00:01:59,490 --> 00:02:02,510
raw sensors were dealing with their
numbers I'll say this again and again

76
00:02:02,510 --> 00:02:02,520
numbers I'll say this again and again
 

77
00:02:02,520 --> 00:02:06,080
numbers I'll say this again and again
that for human vision for us here would

78
00:02:06,080 --> 00:02:06,090
that for human vision for us here would
 

79
00:02:06,090 --> 00:02:08,180
that for human vision for us here would
take for granted this particular aspect

80
00:02:08,180 --> 00:02:08,190
take for granted this particular aspect
 

81
00:02:08,190 --> 00:02:10,999
take for granted this particular aspect
of our ability is to take in raw sensory

82
00:02:10,999 --> 00:02:11,009
of our ability is to take in raw sensory
 

83
00:02:11,009 --> 00:02:12,380
of our ability is to take in raw sensory
information through our eyes and

84
00:02:12,380 --> 00:02:12,390
information through our eyes and
 

85
00:02:12,390 --> 00:02:13,040
information through our eyes and
interpret

86
00:02:13,040 --> 00:02:13,050
interpret
 

87
00:02:13,050 --> 00:02:16,050
interpret
but it's just numbers that's something

88
00:02:16,050 --> 00:02:16,060
but it's just numbers that's something
 

89
00:02:16,060 --> 00:02:18,480
but it's just numbers that's something
whether you're an expert computer vision

90
00:02:18,480 --> 00:02:18,490
whether you're an expert computer vision
 

91
00:02:18,490 --> 00:02:21,450
whether you're an expert computer vision
person or new to the field you have to

92
00:02:21,450 --> 00:02:21,460
person or new to the field you have to
 

93
00:02:21,460 --> 00:02:24,660
person or new to the field you have to
always go back to meditate on is what

94
00:02:24,660 --> 00:02:24,670
always go back to meditate on is what
 

95
00:02:24,670 --> 00:02:28,410
always go back to meditate on is what
kind of things the Machine is given what

96
00:02:28,410 --> 00:02:28,420
kind of things the Machine is given what
 

97
00:02:28,420 --> 00:02:31,110
kind of things the Machine is given what
what what is the data that is tasked to

98
00:02:31,110 --> 00:02:31,120
what what is the data that is tasked to
 

99
00:02:31,120 --> 00:02:33,780
what what is the data that is tasked to
work with in order to perform the tasks

100
00:02:33,780 --> 00:02:33,790
work with in order to perform the tasks
 

101
00:02:33,790 --> 00:02:35,120
work with in order to perform the tasks
you're asking it to do

102
00:02:35,120 --> 00:02:35,130
you're asking it to do
 

103
00:02:35,130 --> 00:02:38,810
you're asking it to do
perhaps the data is given is highly

104
00:02:38,810 --> 00:02:38,820
perhaps the data is given is highly
 

105
00:02:38,820 --> 00:02:40,860
perhaps the data is given is highly
insufficient to do what you want it to

106
00:02:40,860 --> 00:02:40,870
insufficient to do what you want it to
 

107
00:02:40,870 --> 00:02:41,310
insufficient to do what you want it to
do

108
00:02:41,310 --> 00:02:41,320
do
 

109
00:02:41,320 --> 00:02:43,020
do
that's the question I'll come up again

110
00:02:43,020 --> 00:02:43,030
that's the question I'll come up again
 

111
00:02:43,030 --> 00:02:46,380
that's the question I'll come up again
and again our images enough to

112
00:02:46,380 --> 00:02:46,390
and again our images enough to
 

113
00:02:46,390 --> 00:02:51,020
and again our images enough to
understand the world around you and

114
00:02:51,020 --> 00:02:51,030

 

115
00:02:51,030 --> 00:02:54,390

given these numbers the set of numbers

116
00:02:54,390 --> 00:02:54,400
given these numbers the set of numbers
 

117
00:02:54,400 --> 00:02:56,460
given these numbers the set of numbers
sometimes with one channel sometimes

118
00:02:56,460 --> 00:02:56,470
sometimes with one channel sometimes
 

119
00:02:56,470 --> 00:02:59,280
sometimes with one channel sometimes
with three RGB where every single pixel

120
00:02:59,280 --> 00:02:59,290
with three RGB where every single pixel
 

121
00:02:59,290 --> 00:03:02,340
with three RGB where every single pixel
have three different colors the task is

122
00:03:02,340 --> 00:03:02,350
have three different colors the task is
 

123
00:03:02,350 --> 00:03:07,860
have three different colors the task is
to classify or regress produce a

124
00:03:07,860 --> 00:03:07,870
to classify or regress produce a
 

125
00:03:07,870 --> 00:03:11,640
to classify or regress produce a
continuous variable or one of a set of

126
00:03:11,640 --> 00:03:11,650
continuous variable or one of a set of
 

127
00:03:11,650 --> 00:03:17,160
continuous variable or one of a set of
class labels as before we must be

128
00:03:17,160 --> 00:03:17,170
class labels as before we must be
 

129
00:03:17,170 --> 00:03:21,180
class labels as before we must be
careful about our intuition of what is

130
00:03:21,180 --> 00:03:21,190
careful about our intuition of what is
 

131
00:03:21,190 --> 00:03:27,350
careful about our intuition of what is
hard and what is easy in computer vision

132
00:03:27,350 --> 00:03:27,360

 

133
00:03:27,360 --> 00:03:30,510

let's take a step back to the

134
00:03:30,510 --> 00:03:30,520
let's take a step back to the
 

135
00:03:30,520 --> 00:03:34,830
let's take a step back to the
inspiration for neural networks our own

136
00:03:34,830 --> 00:03:34,840
inspiration for neural networks our own
 

137
00:03:34,840 --> 00:03:38,160
inspiration for neural networks our own
biological neural networks because the

138
00:03:38,160 --> 00:03:38,170
biological neural networks because the
 

139
00:03:38,170 --> 00:03:40,500
biological neural networks because the
human vision system and the computer

140
00:03:40,500 --> 00:03:40,510
human vision system and the computer
 

141
00:03:40,510 --> 00:03:42,600
human vision system and the computer
vision system is a little bit more

142
00:03:42,600 --> 00:03:42,610
vision system is a little bit more
 

143
00:03:42,610 --> 00:03:53,490
vision system is a little bit more
similar in these regards this

144
00:03:53,490 --> 00:03:53,500
similar in these regards this
 

145
00:03:53,500 --> 00:03:56,940
similar in these regards this
and visual cortex is in layers and as

146
00:03:56,940 --> 00:03:56,950
and visual cortex is in layers and as
 

147
00:03:56,950 --> 00:03:59,900
and visual cortex is in layers and as
information passes from the eyes to the

148
00:03:59,900 --> 00:03:59,910
information passes from the eyes to the
 

149
00:03:59,910 --> 00:04:02,370
information passes from the eyes to the
to the parts of the brain that makes

150
00:04:02,370 --> 00:04:02,380
to the parts of the brain that makes
 

151
00:04:02,380 --> 00:04:04,680
to the parts of the brain that makes
sense of the raw sensor information

152
00:04:04,680 --> 00:04:04,690
sense of the raw sensor information
 

153
00:04:04,690 --> 00:04:07,080
sense of the raw sensor information
higher and higher order representations

154
00:04:07,080 --> 00:04:07,090
higher and higher order representations
 

155
00:04:07,090 --> 00:04:10,380
higher and higher order representations
have formed this is the inspiration the

156
00:04:10,380 --> 00:04:10,390
have formed this is the inspiration the
 

157
00:04:10,390 --> 00:04:13,199
have formed this is the inspiration the
idea behind using deep neural networks

158
00:04:13,199 --> 00:04:13,209
idea behind using deep neural networks
 

159
00:04:13,209 --> 00:04:15,870
idea behind using deep neural networks
for images higher and higher order

160
00:04:15,870 --> 00:04:15,880
for images higher and higher order
 

161
00:04:15,880 --> 00:04:17,550
for images higher and higher order
representations of form through the

162
00:04:17,550 --> 00:04:17,560
representations of form through the
 

163
00:04:17,560 --> 00:04:22,860
representations of form through the
layers there early layers taking in the

164
00:04:22,860 --> 00:04:22,870
layers there early layers taking in the
 

165
00:04:22,870 --> 00:04:25,110
layers there early layers taking in the
very raw and sensory information then

166
00:04:25,110 --> 00:04:25,120
very raw and sensory information then
 

167
00:04:25,120 --> 00:04:28,770
very raw and sensory information then
extracting edges connecting those edges

168
00:04:28,770 --> 00:04:28,780
extracting edges connecting those edges
 

169
00:04:28,780 --> 00:04:30,780
extracting edges connecting those edges
forming those edges to form more complex

170
00:04:30,780 --> 00:04:30,790
forming those edges to form more complex
 

171
00:04:30,790 --> 00:04:33,030
forming those edges to form more complex
features and finally into the

172
00:04:33,030 --> 00:04:33,040
features and finally into the
 

173
00:04:33,040 --> 00:04:35,159
features and finally into the
higher-order semantic meaning that we

174
00:04:35,159 --> 00:04:35,169
higher-order semantic meaning that we
 

175
00:04:35,169 --> 00:04:38,700
higher-order semantic meaning that we
hope to get from these images in

176
00:04:38,700 --> 00:04:38,710
hope to get from these images in
 

177
00:04:38,710 --> 00:04:41,210
hope to get from these images in
computer vision deep learning is hard

178
00:04:41,210 --> 00:04:41,220
computer vision deep learning is hard
 

179
00:04:41,220 --> 00:04:43,350
computer vision deep learning is hard
I'll say this again

180
00:04:43,350 --> 00:04:43,360
I'll say this again
 

181
00:04:43,360 --> 00:04:45,180
I'll say this again
the illumination variability is the

182
00:04:45,180 --> 00:04:45,190
the illumination variability is the
 

183
00:04:45,190 --> 00:04:47,909
the illumination variability is the
biggest challenge or at least one of the

184
00:04:47,909 --> 00:04:47,919
biggest challenge or at least one of the
 

185
00:04:47,919 --> 00:04:51,260
biggest challenge or at least one of the
one of the biggest challenges in driving

186
00:04:51,260 --> 00:04:51,270
one of the biggest challenges in driving
 

187
00:04:51,270 --> 00:04:55,230
one of the biggest challenges in driving
for visible light cameras pose

188
00:04:55,230 --> 00:04:55,240
for visible light cameras pose
 

189
00:04:55,240 --> 00:04:59,520
for visible light cameras pose
variability the objects as I'll also

190
00:04:59,520 --> 00:04:59,530
variability the objects as I'll also
 

191
00:04:59,530 --> 00:05:01,860
variability the objects as I'll also
discuss about some of the advances geoff

192
00:05:01,860 --> 00:05:01,870
discuss about some of the advances geoff
 

193
00:05:01,870 --> 00:05:04,820
discuss about some of the advances geoff
hinton and the capsule networks the idea

194
00:05:04,820 --> 00:05:04,830
hinton and the capsule networks the idea
 

195
00:05:04,830 --> 00:05:07,409
hinton and the capsule networks the idea
with the neural networks as they're

196
00:05:07,409 --> 00:05:07,419
with the neural networks as they're
 

197
00:05:07,419 --> 00:05:09,780
with the neural networks as they're
currently useful computer vision are not

198
00:05:09,780 --> 00:05:09,790
currently useful computer vision are not
 

199
00:05:09,790 --> 00:05:13,250
currently useful computer vision are not
good with representing variable pose

200
00:05:13,250 --> 00:05:13,260
good with representing variable pose
 

201
00:05:13,260 --> 00:05:17,040
good with representing variable pose
these objects in images and this 2d

202
00:05:17,040 --> 00:05:17,050
these objects in images and this 2d
 

203
00:05:17,050 --> 00:05:20,520
these objects in images and this 2d
plane of color and texture look very

204
00:05:20,520 --> 00:05:20,530
plane of color and texture look very
 

205
00:05:20,530 --> 00:05:23,880
plane of color and texture look very
different numerically when the object is

206
00:05:23,880 --> 00:05:23,890
different numerically when the object is
 

207
00:05:23,890 --> 00:05:27,630
different numerically when the object is
rotated and the object is mangled and

208
00:05:27,630 --> 00:05:27,640
rotated and the object is mangled and
 

209
00:05:27,640 --> 00:05:29,550
rotated and the object is mangled and
shaped in different ways the deformable

210
00:05:29,550 --> 00:05:29,560
shaped in different ways the deformable
 

211
00:05:29,560 --> 00:05:32,430
shaped in different ways the deformable
will truncated cat intraclass

212
00:05:32,430 --> 00:05:32,440
will truncated cat intraclass
 

213
00:05:32,440 --> 00:05:36,270
will truncated cat intraclass
variability the for the classification

214
00:05:36,270 --> 00:05:36,280
variability the for the classification
 

215
00:05:36,280 --> 00:05:38,610
variability the for the classification
task which would be an example today

216
00:05:38,610 --> 00:05:38,620
task which would be an example today
 

217
00:05:38,620 --> 00:05:41,010
task which would be an example today
throughout to introduce some of the

218
00:05:41,010 --> 00:05:41,020
throughout to introduce some of the
 

219
00:05:41,020 --> 00:05:43,260
throughout to introduce some of the
networks over the past decade that have

220
00:05:43,260 --> 00:05:43,270
networks over the past decade that have
 

221
00:05:43,270 --> 00:05:44,610
networks over the past decade that have
received success in some of the

222
00:05:44,610 --> 00:05:44,620
received success in some of the
 

223
00:05:44,620 --> 00:05:46,350
received success in some of the
intuition and insight that made those

224
00:05:46,350 --> 00:05:46,360
intuition and insight that made those
 

225
00:05:46,360 --> 00:05:50,250
intuition and insight that made those
networks work classification there is a

226
00:05:50,250 --> 00:05:50,260
networks work classification there is a
 

227
00:05:50,260 --> 00:05:52,650
networks work classification there is a
lot of variability inside the classes

228
00:05:52,650 --> 00:05:52,660
lot of variability inside the classes
 

229
00:05:52,660 --> 00:05:54,750
lot of variability inside the classes
and very little variability between the

230
00:05:54,750 --> 00:05:54,760
and very little variability between the
 

231
00:05:54,760 --> 00:05:59,070
and very little variability between the
classes all of these are cats on top all

232
00:05:59,070 --> 00:05:59,080
classes all of these are cats on top all
 

233
00:05:59,080 --> 00:06:01,290
classes all of these are cats on top all
of those are dogs are bottom they look

234
00:06:01,290 --> 00:06:01,300
of those are dogs are bottom they look
 

235
00:06:01,300 --> 00:06:04,080
of those are dogs are bottom they look
very different and the other I would say

236
00:06:04,080 --> 00:06:04,090
very different and the other I would say
 

237
00:06:04,090 --> 00:06:06,600
very different and the other I would say
the second biggest problem in driving

238
00:06:06,600 --> 00:06:06,610
the second biggest problem in driving
 

239
00:06:06,610 --> 00:06:07,260
the second biggest problem in driving
perception

240
00:06:07,260 --> 00:06:07,270
perception
 

241
00:06:07,270 --> 00:06:08,730
perception
visible light camera perceptions

242
00:06:08,730 --> 00:06:08,740
visible light camera perceptions
 

243
00:06:08,740 --> 00:06:11,070
visible light camera perceptions
occlusion when part of the object is

244
00:06:11,070 --> 00:06:11,080
occlusion when part of the object is
 

245
00:06:11,080 --> 00:06:14,780
occlusion when part of the object is
occluded due to the three-dimensional

246
00:06:14,780 --> 00:06:14,790
occluded due to the three-dimensional
 

247
00:06:14,790 --> 00:06:18,240
occluded due to the three-dimensional
nature of our world some objects in

248
00:06:18,240 --> 00:06:18,250
nature of our world some objects in
 

249
00:06:18,250 --> 00:06:21,840
nature of our world some objects in
front of others and they occlude the

250
00:06:21,840 --> 00:06:21,850
front of others and they occlude the
 

251
00:06:21,850 --> 00:06:24,750
front of others and they occlude the
background object and yet we're still

252
00:06:24,750 --> 00:06:24,760
background object and yet we're still
 

253
00:06:24,760 --> 00:06:27,030
background object and yet we're still
tasked with identifying the object when

254
00:06:27,030 --> 00:06:27,040
tasked with identifying the object when
 

255
00:06:27,040 --> 00:06:29,550
tasked with identifying the object when
only part of it is visible and sometimes

256
00:06:29,550 --> 00:06:29,560
only part of it is visible and sometimes
 

257
00:06:29,560 --> 00:06:33,540
only part of it is visible and sometimes
that part told you there's cats is very

258
00:06:33,540 --> 00:06:33,550
that part told you there's cats is very
 

259
00:06:33,550 --> 00:06:35,250
that part told you there's cats is very
hardly visible here

260
00:06:35,250 --> 00:06:35,260
hardly visible here
 

261
00:06:35,260 --> 00:06:37,500
hardly visible here
we're tasked with classifying a cat with

262
00:06:37,500 --> 00:06:37,510
we're tasked with classifying a cat with
 

263
00:06:37,510 --> 00:06:45,120
we're tasked with classifying a cat with
just an ears visible just the leg and in

264
00:06:45,120 --> 00:06:45,130
just an ears visible just the leg and in
 

265
00:06:45,130 --> 00:06:47,310
just an ears visible just the leg and in
the philosophical level as we'll talk

266
00:06:47,310 --> 00:06:47,320
the philosophical level as we'll talk
 

267
00:06:47,320 --> 00:06:49,050
the philosophical level as we'll talk
about the motivation for our competition

268
00:06:49,050 --> 00:06:49,060
about the motivation for our competition
 

269
00:06:49,060 --> 00:06:53,400
about the motivation for our competition
here here's a cat dressed as a monkey

270
00:06:53,400 --> 00:06:53,410
here here's a cat dressed as a monkey
 

271
00:06:53,410 --> 00:06:57,560
here here's a cat dressed as a monkey
eating a banana on a philosophical level

272
00:06:57,560 --> 00:06:57,570
eating a banana on a philosophical level
 

273
00:06:57,570 --> 00:07:02,430
eating a banana on a philosophical level
most of us understand what's going on in

274
00:07:02,430 --> 00:07:02,440
most of us understand what's going on in
 

275
00:07:02,440 --> 00:07:07,100
most of us understand what's going on in
the scene in fact a neural network it's

276
00:07:07,100 --> 00:07:07,110
the scene in fact a neural network it's
 

277
00:07:07,110 --> 00:07:13,250
the scene in fact a neural network it's
to today successfully classify this

278
00:07:13,250 --> 00:07:13,260

 

279
00:07:13,260 --> 00:07:18,830

image this video as a cat but the

280
00:07:18,830 --> 00:07:18,840
image this video as a cat but the
 

281
00:07:18,840 --> 00:07:21,960
image this video as a cat but the
context the humour of the situation and

282
00:07:21,960 --> 00:07:21,970
context the humour of the situation and
 

283
00:07:21,970 --> 00:07:25,520
context the humour of the situation and
in fact you could argue it's a monkey is

284
00:07:25,520 --> 00:07:25,530
in fact you could argue it's a monkey is
 

285
00:07:25,530 --> 00:07:29,340
in fact you could argue it's a monkey is
missing and what else is missing is the

286
00:07:29,340 --> 00:07:29,350
missing and what else is missing is the
 

287
00:07:29,350 --> 00:07:31,320
missing and what else is missing is the
dynamic information the temporal

288
00:07:31,320 --> 00:07:31,330
dynamic information the temporal
 

289
00:07:31,330 --> 00:07:35,460
dynamic information the temporal
dynamics of the scene that's what's

290
00:07:35,460 --> 00:07:35,470
dynamics of the scene that's what's
 

291
00:07:35,470 --> 00:07:37,620
dynamics of the scene that's what's
missing in a lot of the perception work

292
00:07:37,620 --> 00:07:37,630
missing in a lot of the perception work
 

293
00:07:37,630 --> 00:07:40,170
missing in a lot of the perception work
that has been done to date in the

294
00:07:40,170 --> 00:07:40,180
that has been done to date in the
 

295
00:07:40,180 --> 00:07:43,200
that has been done to date in the
autonomous vehicle space in terms of

296
00:07:43,200 --> 00:07:43,210
autonomous vehicle space in terms of
 

297
00:07:43,210 --> 00:07:45,600
autonomous vehicle space in terms of
visible light cameras and we're looking

298
00:07:45,600 --> 00:07:45,610
visible light cameras and we're looking
 

299
00:07:45,610 --> 00:07:46,830
visible light cameras and we're looking
to expand on that

300
00:07:46,830 --> 00:07:46,840
to expand on that
 

301
00:07:46,840 --> 00:07:49,670
to expand on that
that's what psyche fuse is all about

302
00:07:49,670 --> 00:07:49,680
that's what psyche fuse is all about
 

303
00:07:49,680 --> 00:07:52,710
that's what psyche fuse is all about
image classification pipeline there's a

304
00:07:52,710 --> 00:07:52,720
image classification pipeline there's a
 

305
00:07:52,720 --> 00:07:54,870
image classification pipeline there's a
bin with different categories inside

306
00:07:54,870 --> 00:07:54,880
bin with different categories inside
 

307
00:07:54,880 --> 00:07:59,400
bin with different categories inside
each class cat dog mug hat those bins

308
00:07:59,400 --> 00:07:59,410
each class cat dog mug hat those bins
 

309
00:07:59,410 --> 00:08:01,500
each class cat dog mug hat those bins
there's a lot of examples of each and

310
00:08:01,500 --> 00:08:01,510
there's a lot of examples of each and
 

311
00:08:01,510 --> 00:08:04,530
there's a lot of examples of each and
your task with when a new example comes

312
00:08:04,530 --> 00:08:04,540
your task with when a new example comes
 

313
00:08:04,540 --> 00:08:06,330
your task with when a new example comes
along you never seen before to put that

314
00:08:06,330 --> 00:08:06,340
along you never seen before to put that
 

315
00:08:06,340 --> 00:08:09,450
along you never seen before to put that
image in a bin it's the same as the

316
00:08:09,450 --> 00:08:09,460
image in a bin it's the same as the
 

317
00:08:09,460 --> 00:08:11,570
image in a bin it's the same as the
machine learning tasks before and

318
00:08:11,570 --> 00:08:11,580
machine learning tasks before and
 

319
00:08:11,580 --> 00:08:15,150
machine learning tasks before and
everything relies on the data that's

320
00:08:15,150 --> 00:08:15,160
everything relies on the data that's
 

321
00:08:15,160 --> 00:08:17,400
everything relies on the data that's
been ground truth that been labeled by

322
00:08:17,400 --> 00:08:17,410
been ground truth that been labeled by
 

323
00:08:17,410 --> 00:08:19,120
been ground truth that been labeled by
human beings

324
00:08:19,120 --> 00:08:19,130
human beings
 

325
00:08:19,130 --> 00:08:22,600
human beings
amnesty is a toy data set of handwritten

326
00:08:22,600 --> 00:08:22,610
amnesty is a toy data set of handwritten
 

327
00:08:22,610 --> 00:08:26,140
amnesty is a toy data set of handwritten
digits often used as examples and Koko

328
00:08:26,140 --> 00:08:26,150
digits often used as examples and Koko
 

329
00:08:26,150 --> 00:08:29,140
digits often used as examples and Koko
safar imagenet places and a lot of other

330
00:08:29,140 --> 00:08:29,150
safar imagenet places and a lot of other
 

331
00:08:29,150 --> 00:08:31,870
safar imagenet places and a lot of other
incredible datasets rich data sets of a

332
00:08:31,870 --> 00:08:31,880
incredible datasets rich data sets of a
 

333
00:08:31,880 --> 00:08:34,390
incredible datasets rich data sets of a
hundred thousands millions of images out

334
00:08:34,390 --> 00:08:34,400
hundred thousands millions of images out
 

335
00:08:34,400 --> 00:08:37,360
hundred thousands millions of images out
there represent scenes people's faces

336
00:08:37,360 --> 00:08:37,370
there represent scenes people's faces
 

337
00:08:37,370 --> 00:08:40,500
there represent scenes people's faces
and different objects those are all

338
00:08:40,500 --> 00:08:40,510
and different objects those are all
 

339
00:08:40,510 --> 00:08:43,659
and different objects those are all
ground truth data for testing algorithms

340
00:08:43,659 --> 00:08:43,669
ground truth data for testing algorithms
 

341
00:08:43,669 --> 00:08:47,500
ground truth data for testing algorithms
and for competing architectures to be

342
00:08:47,500 --> 00:08:47,510
and for competing architectures to be
 

343
00:08:47,510 --> 00:08:50,530
and for competing architectures to be
evaluated against each other see far ten

344
00:08:50,530 --> 00:08:50,540
evaluated against each other see far ten
 

345
00:08:50,540 --> 00:08:54,070
evaluated against each other see far ten
one of the simplest almost toy datasets

346
00:08:54,070 --> 00:08:54,080
one of the simplest almost toy datasets
 

347
00:08:54,080 --> 00:08:56,860
one of the simplest almost toy datasets
of tiny icons with ten categories of

348
00:08:56,860 --> 00:08:56,870
of tiny icons with ten categories of
 

349
00:08:56,870 --> 00:08:59,590
of tiny icons with ten categories of
airplane automobile bird cat deer dog

350
00:08:59,590 --> 00:08:59,600
airplane automobile bird cat deer dog
 

351
00:08:59,600 --> 00:09:01,960
airplane automobile bird cat deer dog
for our course ship and truck is

352
00:09:01,960 --> 00:09:01,970
for our course ship and truck is
 

353
00:09:01,970 --> 00:09:03,820
for our course ship and truck is
commonly used to explore some of the

354
00:09:03,820 --> 00:09:03,830
commonly used to explore some of the
 

355
00:09:03,830 --> 00:09:05,530
commonly used to explore some of the
basic convolution neural networks we'll

356
00:09:05,530 --> 00:09:05,540
basic convolution neural networks we'll
 

357
00:09:05,540 --> 00:09:07,660
basic convolution neural networks we'll
discuss so let's come up with a very

358
00:09:07,660 --> 00:09:07,670
discuss so let's come up with a very
 

359
00:09:07,670 --> 00:09:09,730
discuss so let's come up with a very
trivial classifier to explain the

360
00:09:09,730 --> 00:09:09,740
trivial classifier to explain the
 

361
00:09:09,740 --> 00:09:12,610
trivial classifier to explain the
concept of how we could go about it in

362
00:09:12,610 --> 00:09:12,620
concept of how we could go about it in
 

363
00:09:12,620 --> 00:09:15,040
concept of how we could go about it in
fact this is maybe if you start to think

364
00:09:15,040 --> 00:09:15,050
fact this is maybe if you start to think
 

365
00:09:15,050 --> 00:09:17,140
fact this is maybe if you start to think
about how to classify an image if you

366
00:09:17,140 --> 00:09:17,150
about how to classify an image if you
 

367
00:09:17,150 --> 00:09:19,300
about how to classify an image if you
don't know any of these techniques this

368
00:09:19,300 --> 00:09:19,310
don't know any of these techniques this
 

369
00:09:19,310 --> 00:09:20,860
don't know any of these techniques this
is perhaps the approach you would take

370
00:09:20,860 --> 00:09:20,870
is perhaps the approach you would take
 

371
00:09:20,870 --> 00:09:23,890
is perhaps the approach you would take
is you would subtract images so in order

372
00:09:23,890 --> 00:09:23,900
is you would subtract images so in order
 

373
00:09:23,900 --> 00:09:26,290
is you would subtract images so in order
to know that an image of a cat is

374
00:09:26,290 --> 00:09:26,300
to know that an image of a cat is
 

375
00:09:26,300 --> 00:09:28,060
to know that an image of a cat is
different than image of a dog if to

376
00:09:28,060 --> 00:09:28,070
different than image of a dog if to
 

377
00:09:28,070 --> 00:09:30,430
different than image of a dog if to
compare them when given those two images

378
00:09:30,430 --> 00:09:30,440
compare them when given those two images
 

379
00:09:30,440 --> 00:09:32,290
compare them when given those two images
what what's the what's the way you

380
00:09:32,290 --> 00:09:32,300
what what's the what's the way you
 

381
00:09:32,300 --> 00:09:34,870
what what's the what's the way you
compare them one way you could do it is

382
00:09:34,870 --> 00:09:34,880
compare them one way you could do it is
 

383
00:09:34,880 --> 00:09:37,810
compare them one way you could do it is
you just subtract it and then sum all

384
00:09:37,810 --> 00:09:37,820
you just subtract it and then sum all
 

385
00:09:37,820 --> 00:09:40,210
you just subtract it and then sum all
the pixel wise differences in the image

386
00:09:40,210 --> 00:09:40,220
the pixel wise differences in the image
 

387
00:09:40,220 --> 00:09:42,430
the pixel wise differences in the image
just subtract the intensity of the image

388
00:09:42,430 --> 00:09:42,440
just subtract the intensity of the image
 

389
00:09:42,440 --> 00:09:46,450
just subtract the intensity of the image
pixel by pixel sum it up if that intent

390
00:09:46,450 --> 00:09:46,460
pixel by pixel sum it up if that intent
 

391
00:09:46,460 --> 00:09:48,190
pixel by pixel sum it up if that intent
if that difference is really high that

392
00:09:48,190 --> 00:09:48,200
if that difference is really high that
 

393
00:09:48,200 --> 00:09:50,580
if that difference is really high that
means the images are very different

394
00:09:50,580 --> 00:09:50,590
means the images are very different
 

395
00:09:50,590 --> 00:09:53,200
means the images are very different
using that metric we can look at C for

396
00:09:53,200 --> 00:09:53,210
using that metric we can look at C for
 

397
00:09:53,210 --> 00:09:57,630
using that metric we can look at C for
10 and use it as a classifier saying

398
00:09:57,630 --> 00:09:57,640
10 and use it as a classifier saying
 

399
00:09:57,640 --> 00:10:00,160
10 and use it as a classifier saying
based on this difference function I'm

400
00:10:00,160 --> 00:10:00,170
based on this difference function I'm
 

401
00:10:00,170 --> 00:10:02,560
based on this difference function I'm
going to find one of the 10 bins for a

402
00:10:02,560 --> 00:10:02,570
going to find one of the 10 bins for a
 

403
00:10:02,570 --> 00:10:07,870
going to find one of the 10 bins for a
new image that that is that has the

404
00:10:07,870 --> 00:10:07,880
new image that that is that has the
 

405
00:10:07,880 --> 00:10:12,280
new image that that is that has the
lowest difference find an image in this

406
00:10:12,280 --> 00:10:12,290
lowest difference find an image in this
 

407
00:10:12,290 --> 00:10:14,560
lowest difference find an image in this
data set that is most like the image I

408
00:10:14,560 --> 00:10:14,570
data set that is most like the image I
 

409
00:10:14,570 --> 00:10:16,810
data set that is most like the image I
have and put it in the same bin as that

410
00:10:16,810 --> 00:10:16,820
have and put it in the same bin as that
 

411
00:10:16,820 --> 00:10:21,730
have and put it in the same bin as that
images in so there's 10 classes if we

412
00:10:21,730 --> 00:10:21,740
images in so there's 10 classes if we
 

413
00:10:21,740 --> 00:10:23,530
images in so there's 10 classes if we
just flip a coin the accuracy of our

414
00:10:23,530 --> 00:10:23,540
just flip a coin the accuracy of our
 

415
00:10:23,540 --> 00:10:27,340
just flip a coin the accuracy of our
classifier will be 10% using our image

416
00:10:27,340 --> 00:10:27,350
classifier will be 10% using our image
 

417
00:10:27,350 --> 00:10:29,590
classifier will be 10% using our image
difference classifier we can actually do

418
00:10:29,590 --> 00:10:29,600
difference classifier we can actually do
 

419
00:10:29,600 --> 00:10:31,390
difference classifier we can actually do
pretty good much better than random much

420
00:10:31,390 --> 00:10:31,400
pretty good much better than random much
 

421
00:10:31,400 --> 00:10:32,960
pretty good much better than random much
better than 10%

422
00:10:32,960 --> 00:10:32,970
better than 10%
 

423
00:10:32,970 --> 00:10:36,520
better than 10%
we can do 35 38 percent accuracy

424
00:10:36,520 --> 00:10:36,530
we can do 35 38 percent accuracy
 

425
00:10:36,530 --> 00:10:40,390
we can do 35 38 percent accuracy
that's a classifier we have our first

426
00:10:40,390 --> 00:10:40,400
that's a classifier we have our first
 

427
00:10:40,400 --> 00:10:46,700
that's a classifier we have our first
classifier K nearest neighbors let's

428
00:10:46,700 --> 00:10:46,710
classifier K nearest neighbors let's
 

429
00:10:46,710 --> 00:10:48,610
classifier K nearest neighbors let's
take our classifier to a whole new level

430
00:10:48,610 --> 00:10:48,620
take our classifier to a whole new level
 

431
00:10:48,620 --> 00:10:52,220
take our classifier to a whole new level
instead of comparing it to just fight

432
00:10:52,220 --> 00:10:52,230
instead of comparing it to just fight
 

433
00:10:52,230 --> 00:10:54,410
instead of comparing it to just fight
trying to find one image that's the

434
00:10:54,410 --> 00:10:54,420
trying to find one image that's the
 

435
00:10:54,420 --> 00:10:57,200
trying to find one image that's the
closest in our data set we tried to find

436
00:10:57,200 --> 00:10:57,210
closest in our data set we tried to find
 

437
00:10:57,210 --> 00:11:01,220
closest in our data set we tried to find
K closest and say what is what class do

438
00:11:01,220 --> 00:11:01,230
K closest and say what is what class do
 

439
00:11:01,230 --> 00:11:03,560
K closest and say what is what class do
the majority of them belong to and we

440
00:11:03,560 --> 00:11:03,570
the majority of them belong to and we
 

441
00:11:03,570 --> 00:11:06,140
the majority of them belong to and we
take that k and increase it for 1 to 2

442
00:11:06,140 --> 00:11:06,150
take that k and increase it for 1 to 2
 

443
00:11:06,150 --> 00:11:09,560
take that k and increase it for 1 to 2
to 3 to 4 to 5 and see how that changes

444
00:11:09,560 --> 00:11:09,570
to 3 to 4 to 5 and see how that changes
 

445
00:11:09,570 --> 00:11:14,450
to 3 to 4 to 5 and see how that changes
the problem with seven years neighbors

446
00:11:14,450 --> 00:11:14,460
the problem with seven years neighbors
 

447
00:11:14,460 --> 00:11:16,190
the problem with seven years neighbors
which is the optimal under this approach

448
00:11:16,190 --> 00:11:16,200
which is the optimal under this approach
 

449
00:11:16,200 --> 00:11:22,630
which is the optimal under this approach
for CFR 10 we achieve 30% accuracy

450
00:11:22,630 --> 00:11:22,640
for CFR 10 we achieve 30% accuracy
 

451
00:11:22,640 --> 00:11:28,400
for CFR 10 we achieve 30% accuracy
human level is 95% accuracy and with

452
00:11:28,400 --> 00:11:28,410
human level is 95% accuracy and with
 

453
00:11:28,410 --> 00:11:30,200
human level is 95% accuracy and with
convolutional neural networks will get

454
00:11:30,200 --> 00:11:30,210
convolutional neural networks will get
 

455
00:11:30,210 --> 00:11:38,480
convolutional neural networks will get
very close to 100% that's where you'll

456
00:11:38,480 --> 00:11:38,490
very close to 100% that's where you'll
 

457
00:11:38,490 --> 00:11:41,990
very close to 100% that's where you'll
networks shine this very task of bending

458
00:11:41,990 --> 00:11:42,000
networks shine this very task of bending
 

459
00:11:42,000 --> 00:11:44,480
networks shine this very task of bending
images it all starts at this basic

460
00:11:44,480 --> 00:11:44,490
images it all starts at this basic
 

461
00:11:44,490 --> 00:11:47,960
images it all starts at this basic
computational unit signal in each of the

462
00:11:47,960 --> 00:11:47,970
computational unit signal in each of the
 

463
00:11:47,970 --> 00:11:52,880
computational unit signal in each of the
signals are weighed summed bias added

464
00:11:52,880 --> 00:11:52,890
signals are weighed summed bias added
 

465
00:11:52,890 --> 00:11:57,350
signals are weighed summed bias added
and put an input into a nonlinear

466
00:11:57,350 --> 00:11:57,360
and put an input into a nonlinear
 

467
00:11:57,360 --> 00:11:59,150
and put an input into a nonlinear
activation function that produces an

468
00:11:59,150 --> 00:11:59,160
activation function that produces an
 

469
00:11:59,160 --> 00:12:02,630
activation function that produces an
output the nonlinear activation function

470
00:12:02,630 --> 00:12:02,640
output the nonlinear activation function
 

471
00:12:02,640 --> 00:12:06,820
output the nonlinear activation function
is key all of these put together and

472
00:12:06,820 --> 00:12:06,830
is key all of these put together and
 

473
00:12:06,830 --> 00:12:11,690
is key all of these put together and
more and more hidden layers form a deep

474
00:12:11,690 --> 00:12:11,700
more and more hidden layers form a deep
 

475
00:12:11,700 --> 00:12:13,400
more and more hidden layers form a deep
neural network and that deep neural

476
00:12:13,400 --> 00:12:13,410
neural network and that deep neural
 

477
00:12:13,410 --> 00:12:16,580
neural network and that deep neural
network is trained as we've discussed by

478
00:12:16,580 --> 00:12:16,590
network is trained as we've discussed by
 

479
00:12:16,590 --> 00:12:19,310
network is trained as we've discussed by
taking a forward pass and examples have

480
00:12:19,310 --> 00:12:19,320
taking a forward pass and examples have
 

481
00:12:19,320 --> 00:12:21,440
taking a forward pass and examples have
garage with labels seeing how close

482
00:12:21,440 --> 00:12:21,450
garage with labels seeing how close
 

483
00:12:21,450 --> 00:12:23,480
garage with labels seeing how close
those labels are to the real ground

484
00:12:23,480 --> 00:12:23,490
those labels are to the real ground
 

485
00:12:23,490 --> 00:12:26,390
those labels are to the real ground
truth and then punishing the weights

486
00:12:26,390 --> 00:12:26,400
truth and then punishing the weights
 

487
00:12:26,400 --> 00:12:29,270
truth and then punishing the weights
that resulted in the incorrect decisions

488
00:12:29,270 --> 00:12:29,280
that resulted in the incorrect decisions
 

489
00:12:29,280 --> 00:12:31,430
that resulted in the incorrect decisions
and rewarding the weights that resulted

490
00:12:31,430 --> 00:12:31,440
and rewarding the weights that resulted
 

491
00:12:31,440 --> 00:12:34,760
and rewarding the weights that resulted
in correct decisions for the case of 10

492
00:12:34,760 --> 00:12:34,770
in correct decisions for the case of 10
 

493
00:12:34,770 --> 00:12:39,950
in correct decisions for the case of 10
examples the output of the network is

494
00:12:39,950 --> 00:12:39,960
examples the output of the network is
 

495
00:12:39,960 --> 00:12:44,770
examples the output of the network is
different values the input being

496
00:12:44,770 --> 00:12:44,780
different values the input being
 

497
00:12:44,780 --> 00:12:48,320
different values the input being
handwritten digits from 0 to 9 for 10 of

498
00:12:48,320 --> 00:12:48,330
handwritten digits from 0 to 9 for 10 of
 

499
00:12:48,330 --> 00:12:52,280
handwritten digits from 0 to 9 for 10 of
those and we wanted our network to

500
00:12:52,280 --> 00:12:52,290
those and we wanted our network to
 

501
00:12:52,290 --> 00:12:54,980
those and we wanted our network to
classify what is in this image of a

502
00:12:54,980 --> 00:12:54,990
classify what is in this image of a
 

503
00:12:54,990 --> 00:12:58,580
classify what is in this image of a
handwritten digit is it 1 is 0 1 2 3

504
00:12:58,580 --> 00:12:58,590
handwritten digit is it 1 is 0 1 2 3
 

505
00:12:58,590 --> 00:13:02,450
handwritten digit is it 1 is 0 1 2 3
through 9 the way it's often done is

506
00:13:02,450 --> 00:13:02,460
through 9 the way it's often done is
 

507
00:13:02,460 --> 00:13:06,680
through 9 the way it's often done is
there's ten outputs of the network and

508
00:13:06,680 --> 00:13:06,690
there's ten outputs of the network and
 

509
00:13:06,690 --> 00:13:11,050
there's ten outputs of the network and
each of the neurons on the output is

510
00:13:11,050 --> 00:13:11,060
each of the neurons on the output is
 

511
00:13:11,060 --> 00:13:13,610
each of the neurons on the output is
responsible for getting really excited

512
00:13:13,610 --> 00:13:13,620
responsible for getting really excited
 

513
00:13:13,620 --> 00:13:17,840
responsible for getting really excited
when it's number is called and everybody

514
00:13:17,840 --> 00:13:17,850
when it's number is called and everybody
 

515
00:13:17,850 --> 00:13:20,200
when it's number is called and everybody
else is supposed to be not excited

516
00:13:20,200 --> 00:13:20,210
else is supposed to be not excited
 

517
00:13:20,210 --> 00:13:24,290
else is supposed to be not excited
therefore the number of classes is the

518
00:13:24,290 --> 00:13:24,300
therefore the number of classes is the
 

519
00:13:24,300 --> 00:13:25,700
therefore the number of classes is the
number of outputs that's how it's

520
00:13:25,700 --> 00:13:25,710
number of outputs that's how it's
 

521
00:13:25,710 --> 00:13:30,110
number of outputs that's how it's
commonly done and you assign a class to

522
00:13:30,110 --> 00:13:30,120
commonly done and you assign a class to
 

523
00:13:30,120 --> 00:13:32,900
commonly done and you assign a class to
the input image based on the highest the

524
00:13:32,900 --> 00:13:32,910
the input image based on the highest the
 

525
00:13:32,910 --> 00:13:36,010
the input image based on the highest the
neuron which produces the highest output

526
00:13:36,010 --> 00:13:36,020
neuron which produces the highest output
 

527
00:13:36,020 --> 00:13:38,570
neuron which produces the highest output
but that's for a fully connected network

528
00:13:38,570 --> 00:13:38,580
but that's for a fully connected network
 

529
00:13:38,580 --> 00:13:42,530
but that's for a fully connected network
that we've discussed on Monday there is

530
00:13:42,530 --> 00:13:42,540
that we've discussed on Monday there is
 

531
00:13:42,540 --> 00:13:46,070
that we've discussed on Monday there is
in deep learning a lot of tricks that

532
00:13:46,070 --> 00:13:46,080
in deep learning a lot of tricks that
 

533
00:13:46,080 --> 00:13:48,290
in deep learning a lot of tricks that
make things work that make training much

534
00:13:48,290 --> 00:13:48,300
make things work that make training much
 

535
00:13:48,300 --> 00:13:53,090
make things work that make training much
more efficient on large class problems

536
00:13:53,090 --> 00:13:53,100
more efficient on large class problems
 

537
00:13:53,100 --> 00:13:55,400
more efficient on large class problems
where there's a lot of classes on large

538
00:13:55,400 --> 00:13:55,410
where there's a lot of classes on large
 

539
00:13:55,410 --> 00:13:58,280
where there's a lot of classes on large
data sets when the representation that

540
00:13:58,280 --> 00:13:58,290
data sets when the representation that
 

541
00:13:58,290 --> 00:13:59,510
data sets when the representation that
the neural network is tasked with

542
00:13:59,510 --> 00:13:59,520
the neural network is tasked with
 

543
00:13:59,520 --> 00:14:01,790
the neural network is tasked with
learning is extremely complex and that's

544
00:14:01,790 --> 00:14:01,800
learning is extremely complex and that's
 

545
00:14:01,800 --> 00:14:03,290
learning is extremely complex and that's
where convolutional neural neural

546
00:14:03,290 --> 00:14:03,300
where convolutional neural neural
 

547
00:14:03,300 --> 00:14:05,540
where convolutional neural neural
networks step in the trick they use a

548
00:14:05,540 --> 00:14:05,550
networks step in the trick they use a
 

549
00:14:05,550 --> 00:14:08,870
networks step in the trick they use a
spatial invariance they use the idea

550
00:14:08,870 --> 00:14:08,880
spatial invariance they use the idea
 

551
00:14:08,880 --> 00:14:13,550
spatial invariance they use the idea
that a cat in the top left corner of an

552
00:14:13,550 --> 00:14:13,560
that a cat in the top left corner of an
 

553
00:14:13,560 --> 00:14:15,470
that a cat in the top left corner of an
image is the same as a cat in the bottom

554
00:14:15,470 --> 00:14:15,480
image is the same as a cat in the bottom
 

555
00:14:15,480 --> 00:14:18,770
image is the same as a cat in the bottom
right corner of an image so we can learn

556
00:14:18,770 --> 00:14:18,780
right corner of an image so we can learn
 

557
00:14:18,780 --> 00:14:22,000
right corner of an image so we can learn
the same features across the image

558
00:14:22,000 --> 00:14:22,010
the same features across the image
 

559
00:14:22,010 --> 00:14:24,290
the same features across the image
that's where the convolution operation

560
00:14:24,290 --> 00:14:24,300
that's where the convolution operation
 

561
00:14:24,300 --> 00:14:28,370
that's where the convolution operation
steps in instead of the fully connected

562
00:14:28,370 --> 00:14:28,380
steps in instead of the fully connected
 

563
00:14:28,380 --> 00:14:31,040
steps in instead of the fully connected
networks here there's a third dimension

564
00:14:31,040 --> 00:14:31,050
networks here there's a third dimension
 

565
00:14:31,050 --> 00:14:34,940
networks here there's a third dimension
of depth so the blocks in this neural

566
00:14:34,940 --> 00:14:34,950
of depth so the blocks in this neural
 

567
00:14:34,950 --> 00:14:39,230
of depth so the blocks in this neural
network as input take 3d volumes and as

568
00:14:39,230 --> 00:14:39,240
network as input take 3d volumes and as
 

569
00:14:39,240 --> 00:14:47,120
network as input take 3d volumes and as
output produced 3d volumes

570
00:14:47,120 --> 00:14:47,130

 

571
00:14:47,130 --> 00:14:51,170

a slice of the image a window and slide

572
00:14:51,170 --> 00:14:51,180
a slice of the image a window and slide
 

573
00:14:51,180 --> 00:14:54,140
a slice of the image a window and slide
it across applying the same exact

574
00:14:54,140 --> 00:14:54,150
it across applying the same exact
 

575
00:14:54,150 --> 00:14:55,580
it across applying the same exact
weights and we'll go through an example

576
00:14:55,580 --> 00:14:55,590
weights and we'll go through an example
 

577
00:14:55,590 --> 00:14:59,030
weights and we'll go through an example
the same exact weights as in the fully

578
00:14:59,030 --> 00:14:59,040
the same exact weights as in the fully
 

579
00:14:59,040 --> 00:15:01,190
the same exact weights as in the fully
connected network on the edges that are

580
00:15:01,190 --> 00:15:01,200
connected network on the edges that are
 

581
00:15:01,200 --> 00:15:04,610
connected network on the edges that are
used to map the input to the output here

582
00:15:04,610 --> 00:15:04,620
used to map the input to the output here
 

583
00:15:04,620 --> 00:15:08,270
used to map the input to the output here
are used to map this slice of an image

584
00:15:08,270 --> 00:15:08,280
are used to map this slice of an image
 

585
00:15:08,280 --> 00:15:10,790
are used to map this slice of an image
this window of an image to the output

586
00:15:10,790 --> 00:15:10,800
this window of an image to the output
 

587
00:15:10,800 --> 00:15:15,970
this window of an image to the output
and you can make several many of such

588
00:15:15,970 --> 00:15:15,980
and you can make several many of such
 

589
00:15:15,980 --> 00:15:19,640
and you can make several many of such
convolutional filters many layers many

590
00:15:19,640 --> 00:15:19,650
convolutional filters many layers many
 

591
00:15:19,650 --> 00:15:22,610
convolutional filters many layers many
different options of what kind of

592
00:15:22,610 --> 00:15:22,620
different options of what kind of
 

593
00:15:22,620 --> 00:15:24,530
different options of what kind of
features you look for in an image

594
00:15:24,530 --> 00:15:24,540
features you look for in an image
 

595
00:15:24,540 --> 00:15:27,260
features you look for in an image
what kind of window you slide across in

596
00:15:27,260 --> 00:15:27,270
what kind of window you slide across in
 

597
00:15:27,270 --> 00:15:30,200
what kind of window you slide across in
order to extract all kinds of things all

598
00:15:30,200 --> 00:15:30,210
order to extract all kinds of things all
 

599
00:15:30,210 --> 00:15:32,650
order to extract all kinds of things all
kinds of edges all kind of higher-order

600
00:15:32,650 --> 00:15:32,660
kinds of edges all kind of higher-order
 

601
00:15:32,660 --> 00:15:36,860
kinds of edges all kind of higher-order
patterns in the images the very

602
00:15:36,860 --> 00:15:36,870
patterns in the images the very
 

603
00:15:36,870 --> 00:15:39,110
patterns in the images the very
important thing is the parameters on

604
00:15:39,110 --> 00:15:39,120
important thing is the parameters on
 

605
00:15:39,120 --> 00:15:41,600
important thing is the parameters on
each of these filters the subset of the

606
00:15:41,600 --> 00:15:41,610
each of these filters the subset of the
 

607
00:15:41,610 --> 00:15:45,110
each of these filters the subset of the
image these windows are shared if the

608
00:15:45,110 --> 00:15:45,120
image these windows are shared if the
 

609
00:15:45,120 --> 00:15:48,350
image these windows are shared if the
feature that defines a cat is useful in

610
00:15:48,350 --> 00:15:48,360
feature that defines a cat is useful in
 

611
00:15:48,360 --> 00:15:50,030
feature that defines a cat is useful in
the top left corner it's useful in the

612
00:15:50,030 --> 00:15:50,040
the top left corner it's useful in the
 

613
00:15:50,040 --> 00:15:52,310
the top left corner it's useful in the
top right corner it's useful in every

614
00:15:52,310 --> 00:15:52,320
top right corner it's useful in every
 

615
00:15:52,320 --> 00:15:54,650
top right corner it's useful in every
aspect of the image this is the trick

616
00:15:54,650 --> 00:15:54,660
aspect of the image this is the trick
 

617
00:15:54,660 --> 00:15:56,630
aspect of the image this is the trick
that makes convolutional neural networks

618
00:15:56,630 --> 00:15:56,640
that makes convolutional neural networks
 

619
00:15:56,640 --> 00:16:00,710
that makes convolutional neural networks
save a lot of a lot of parameters reduce

620
00:16:00,710 --> 00:16:00,720
save a lot of a lot of parameters reduce
 

621
00:16:00,720 --> 00:16:04,610
save a lot of a lot of parameters reduce
parameter significantly it's the reuse

622
00:16:04,610 --> 00:16:04,620
parameter significantly it's the reuse
 

623
00:16:04,620 --> 00:16:07,040
parameter significantly it's the reuse
the spatial sharing of features across

624
00:16:07,040 --> 00:16:07,050
the spatial sharing of features across
 

625
00:16:07,050 --> 00:16:13,760
the spatial sharing of features across
the space of the image the depth of

626
00:16:13,760 --> 00:16:13,770
the space of the image the depth of
 

627
00:16:13,770 --> 00:16:15,740
the space of the image the depth of
these 3d volumes is the number of

628
00:16:15,740 --> 00:16:15,750
these 3d volumes is the number of
 

629
00:16:15,750 --> 00:16:20,270
these 3d volumes is the number of
filters the stride is the skip of the

630
00:16:20,270 --> 00:16:20,280
filters the stride is the skip of the
 

631
00:16:20,280 --> 00:16:23,210
filters the stride is the skip of the
filter the step size how many pixels you

632
00:16:23,210 --> 00:16:23,220
filter the step size how many pixels you
 

633
00:16:23,220 --> 00:16:26,540
filter the step size how many pixels you
skip when you apply the filter to the

634
00:16:26,540 --> 00:16:26,550
skip when you apply the filter to the
 

635
00:16:26,550 --> 00:16:31,670
skip when you apply the filter to the
input and the padding is

636
00:16:31,670 --> 00:16:31,680
input and the padding is
 

637
00:16:31,680 --> 00:16:33,350
input and the padding is
they're padding the zero padding on the

638
00:16:33,350 --> 00:16:33,360
they're padding the zero padding on the
 

639
00:16:33,360 --> 00:16:36,710
they're padding the zero padding on the
outside of the input to a convolutional

640
00:16:36,710 --> 00:16:36,720
outside of the input to a convolutional
 

641
00:16:36,720 --> 00:16:41,780
outside of the input to a convolutional
layer let's go through an example so on

642
00:16:41,780 --> 00:16:41,790
layer let's go through an example so on
 

643
00:16:41,790 --> 00:16:44,809
layer let's go through an example so on
the left here and the slides are now

644
00:16:44,809 --> 00:16:44,819
the left here and the slides are now
 

645
00:16:44,819 --> 00:16:46,340
the left here and the slides are now
available online you can follow them

646
00:16:46,340 --> 00:16:46,350
available online you can follow them
 

647
00:16:46,350 --> 00:16:48,980
available online you can follow them
along and I'll step through this example

648
00:16:48,980 --> 00:16:48,990
along and I'll step through this example
 

649
00:16:48,990 --> 00:16:52,939
along and I'll step through this example
on the left here is a input volume of

650
00:16:52,939 --> 00:16:52,949
on the left here is a input volume of
 

651
00:16:52,949 --> 00:16:56,239
on the left here is a input volume of
three channels the left column is the

652
00:16:56,239 --> 00:16:56,249
three channels the left column is the
 

653
00:16:56,249 --> 00:16:59,299
three channels the left column is the
input the three block the three squares

654
00:16:59,299 --> 00:16:59,309
input the three block the three squares
 

655
00:16:59,309 --> 00:17:02,389
input the three block the three squares
there are the three channels and there's

656
00:17:02,389 --> 00:17:02,399
there are the three channels and there's
 

657
00:17:02,399 --> 00:17:07,880
there are the three channels and there's
numbers inside those channels and then

658
00:17:07,880 --> 00:17:07,890
numbers inside those channels and then
 

659
00:17:07,890 --> 00:17:14,120
numbers inside those channels and then
we have a filter in red two of them two

660
00:17:14,120 --> 00:17:14,130
we have a filter in red two of them two
 

661
00:17:14,130 --> 00:17:17,689
we have a filter in red two of them two
channels of filters with a bias and we

662
00:17:17,689 --> 00:17:17,699
channels of filters with a bias and we
 

663
00:17:17,699 --> 00:17:20,329
channels of filters with a bias and we
those filters are three by three each

664
00:17:20,329 --> 00:17:20,339
those filters are three by three each
 

665
00:17:20,339 --> 00:17:24,620
those filters are three by three each
one of them is size three by three and

666
00:17:24,620 --> 00:17:24,630
one of them is size three by three and
 

667
00:17:24,630 --> 00:17:26,929
one of them is size three by three and
what we do is we take those three by

668
00:17:26,929 --> 00:17:26,939
what we do is we take those three by
 

669
00:17:26,939 --> 00:17:29,840
what we do is we take those three by
three filters that are to be learned

670
00:17:29,840 --> 00:17:29,850
three filters that are to be learned
 

671
00:17:29,850 --> 00:17:32,299
three filters that are to be learned
these are our variables our weights that

672
00:17:32,299 --> 00:17:32,309
these are our variables our weights that
 

673
00:17:32,309 --> 00:17:34,880
these are our variables our weights that
we have to learn and then we slide it

674
00:17:34,880 --> 00:17:34,890
we have to learn and then we slide it
 

675
00:17:34,890 --> 00:17:38,210
we have to learn and then we slide it
across an image to produce the output on

676
00:17:38,210 --> 00:17:38,220
across an image to produce the output on
 

677
00:17:38,220 --> 00:17:41,510
across an image to produce the output on
the right the green so by applying the

678
00:17:41,510 --> 00:17:41,520
the right the green so by applying the
 

679
00:17:41,520 --> 00:17:44,269
the right the green so by applying the
filters in the red there's two of them

680
00:17:44,269 --> 00:17:44,279
filters in the red there's two of them
 

681
00:17:44,279 --> 00:17:46,340
filters in the red there's two of them
and within each one there's one for

682
00:17:46,340 --> 00:17:46,350
and within each one there's one for
 

683
00:17:46,350 --> 00:17:49,519
and within each one there's one for
every input channel we go from the left

684
00:17:49,519 --> 00:17:49,529
every input channel we go from the left
 

685
00:17:49,529 --> 00:17:53,389
every input channel we go from the left
to the right from the input volume on

686
00:17:53,389 --> 00:17:53,399
to the right from the input volume on
 

687
00:17:53,399 --> 00:17:55,880
to the right from the input volume on
the left to the output volume green on

688
00:17:55,880 --> 00:17:55,890
the left to the output volume green on
 

689
00:17:55,890 --> 00:18:00,980
the left to the output volume green on
the right and you can look it you can

690
00:18:00,980 --> 00:18:00,990
the right and you can look it you can
 

691
00:18:00,990 --> 00:18:02,389
the right and you can look it you can
pull up the slides yourself now if you

692
00:18:02,389 --> 00:18:02,399
pull up the slides yourself now if you
 

693
00:18:02,399 --> 00:18:04,880
pull up the slides yourself now if you
can't see the numbers on the screen but

694
00:18:04,880 --> 00:18:04,890
can't see the numbers on the screen but
 

695
00:18:04,890 --> 00:18:09,860
can't see the numbers on the screen but
the the operations are performed on the

696
00:18:09,860 --> 00:18:09,870
the the operations are performed on the
 

697
00:18:09,870 --> 00:18:13,039
the the operations are performed on the
input to produce the single value that's

698
00:18:13,039 --> 00:18:13,049
input to produce the single value that's
 

699
00:18:13,049 --> 00:18:14,510
input to produce the single value that's
highlighted there in the green and the

700
00:18:14,510 --> 00:18:14,520
highlighted there in the green and the
 

701
00:18:14,520 --> 00:18:18,649
highlighted there in the green and the
output and we slide this convolution no

702
00:18:18,649 --> 00:18:18,659
output and we slide this convolution no
 

703
00:18:18,659 --> 00:18:23,330
output and we slide this convolution no
filter along the image with a stride in

704
00:18:23,330 --> 00:18:23,340
filter along the image with a stride in
 

705
00:18:23,340 --> 00:18:29,769
filter along the image with a stride in
this case of to skipping skipping along

706
00:18:29,769 --> 00:18:29,779
this case of to skipping skipping along
 

707
00:18:29,779 --> 00:18:34,120
this case of to skipping skipping along
they sum to the to the right the two

708
00:18:34,120 --> 00:18:34,130
they sum to the to the right the two
 

709
00:18:34,130 --> 00:18:39,980
they sum to the to the right the two
channel output in green that's it

710
00:18:39,980 --> 00:18:39,990
channel output in green that's it
 

711
00:18:39,990 --> 00:18:42,530
channel output in green that's it
the convolutional operation that's

712
00:18:42,530 --> 00:18:42,540
the convolutional operation that's
 

713
00:18:42,540 --> 00:18:44,180
the convolutional operation that's
what's called the convolutional layer

714
00:18:44,180 --> 00:18:44,190
what's called the convolutional layer
 

715
00:18:44,190 --> 00:18:47,180
what's called the convolutional layer
neural networks and the parameters here

716
00:18:47,180 --> 00:18:47,190
neural networks and the parameters here
 

717
00:18:47,190 --> 00:18:51,080
neural networks and the parameters here
besides the bias are the read values in

718
00:18:51,080 --> 00:18:51,090
besides the bias are the read values in
 

719
00:18:51,090 --> 00:18:53,060
besides the bias are the read values in
the middle that's what we're trying to

720
00:18:53,060 --> 00:18:53,070
the middle that's what we're trying to
 

721
00:18:53,070 --> 00:18:56,060
the middle that's what we're trying to
learn and there's a lot of interesting

722
00:18:56,060 --> 00:18:56,070
learn and there's a lot of interesting
 

723
00:18:56,070 --> 00:18:58,400
learn and there's a lot of interesting
tricks we'll discuss today on top of

724
00:18:58,400 --> 00:18:58,410
tricks we'll discuss today on top of
 

725
00:18:58,410 --> 00:19:00,830
tricks we'll discuss today on top of
those but this is at the core this is

726
00:19:00,830 --> 00:19:00,840
those but this is at the core this is
 

727
00:19:00,840 --> 00:19:03,410
those but this is at the core this is
the spatially invariant sharing of

728
00:19:03,410 --> 00:19:03,420
the spatially invariant sharing of
 

729
00:19:03,420 --> 00:19:06,110
the spatially invariant sharing of
parameters that make convolutional

730
00:19:06,110 --> 00:19:06,120
parameters that make convolutional
 

731
00:19:06,120 --> 00:19:09,770
parameters that make convolutional
neural networks able to efficiently

732
00:19:09,770 --> 00:19:09,780
neural networks able to efficiently
 

733
00:19:09,780 --> 00:19:13,880
neural networks able to efficiently
learn and find patterns and images to

734
00:19:13,880 --> 00:19:13,890
learn and find patterns and images to
 

735
00:19:13,890 --> 00:19:16,270
learn and find patterns and images to
build your intuition a little bit more

736
00:19:16,270 --> 00:19:16,280
build your intuition a little bit more
 

737
00:19:16,280 --> 00:19:18,950
build your intuition a little bit more
about convolution here's an input image

738
00:19:18,950 --> 00:19:18,960
about convolution here's an input image
 

739
00:19:18,960 --> 00:19:22,400
about convolution here's an input image
on the left and on the right the

740
00:19:22,400 --> 00:19:22,410
on the left and on the right the
 

741
00:19:22,410 --> 00:19:25,430
on the left and on the right the
identity filter produces the output you

742
00:19:25,430 --> 00:19:25,440
identity filter produces the output you
 

743
00:19:25,440 --> 00:19:26,930
identity filter produces the output you
see on the right and then there's

744
00:19:26,930 --> 00:19:26,940
see on the right and then there's
 

745
00:19:26,940 --> 00:19:29,450
see on the right and then there's
different ways you can different kinds

746
00:19:29,450 --> 00:19:29,460
different ways you can different kinds
 

747
00:19:29,460 --> 00:19:33,350
different ways you can different kinds
of edges you can extract with the

748
00:19:33,350 --> 00:19:33,360
of edges you can extract with the
 

749
00:19:33,360 --> 00:19:35,510
of edges you can extract with the
activate or the resulting activation map

750
00:19:35,510 --> 00:19:35,520
activate or the resulting activation map
 

751
00:19:35,520 --> 00:19:38,360
activate or the resulting activation map
seen on the right so when applying the

752
00:19:38,360 --> 00:19:38,370
seen on the right so when applying the
 

753
00:19:38,370 --> 00:19:40,850
seen on the right so when applying the
filters with those edge detection

754
00:19:40,850 --> 00:19:40,860
filters with those edge detection
 

755
00:19:40,860 --> 00:19:44,090
filters with those edge detection
filters to the image on the left you

756
00:19:44,090 --> 00:19:44,100
filters to the image on the left you
 

757
00:19:44,100 --> 00:19:46,299
filters to the image on the left you
produce in white are the parts that

758
00:19:46,299 --> 00:19:46,309
produce in white are the parts that
 

759
00:19:46,309 --> 00:19:50,720
produce in white are the parts that
activate the convolution the results of

760
00:19:50,720 --> 00:19:50,730
activate the convolution the results of
 

761
00:19:50,730 --> 00:19:56,570
activate the convolution the results of
these filters and so you can do any kind

762
00:19:56,570 --> 00:19:56,580
these filters and so you can do any kind
 

763
00:19:56,580 --> 00:19:57,980
these filters and so you can do any kind
of filter that's what we're trying to

764
00:19:57,980 --> 00:19:57,990
of filter that's what we're trying to
 

765
00:19:57,990 --> 00:20:02,270
of filter that's what we're trying to
learn any kind of edge any kind of any

766
00:20:02,270 --> 00:20:02,280
learn any kind of edge any kind of any
 

767
00:20:02,280 --> 00:20:05,000
learn any kind of edge any kind of any
kind of pattern you can move along in

768
00:20:05,000 --> 00:20:05,010
kind of pattern you can move along in
 

769
00:20:05,010 --> 00:20:06,560
kind of pattern you can move along in
this window and this way that's shown

770
00:20:06,560 --> 00:20:06,570
this window and this way that's shown
 

771
00:20:06,570 --> 00:20:08,780
this window and this way that's shown
here you slide along the image and you

772
00:20:08,780 --> 00:20:08,790
here you slide along the image and you
 

773
00:20:08,790 --> 00:20:11,090
here you slide along the image and you
produce the output you see on the right

774
00:20:11,090 --> 00:20:11,100
produce the output you see on the right
 

775
00:20:11,100 --> 00:20:13,669
produce the output you see on the right
and depending on how many filters you

776
00:20:13,669 --> 00:20:13,679
and depending on how many filters you
 

777
00:20:13,679 --> 00:20:15,410
and depending on how many filters you
have in every level you have many of

778
00:20:15,410 --> 00:20:15,420
have in every level you have many of
 

779
00:20:15,420 --> 00:20:18,440
have in every level you have many of
such slices VC on the right the input on

780
00:20:18,440 --> 00:20:18,450
such slices VC on the right the input on
 

781
00:20:18,450 --> 00:20:21,020
such slices VC on the right the input on
the left the output on the right if you

782
00:20:21,020 --> 00:20:21,030
the left the output on the right if you
 

783
00:20:21,030 --> 00:20:24,380
the left the output on the right if you
have dozens of filters you have dozens

784
00:20:24,380 --> 00:20:24,390
have dozens of filters you have dozens
 

785
00:20:24,390 --> 00:20:26,390
have dozens of filters you have dozens
of images on the right each with

786
00:20:26,390 --> 00:20:26,400
of images on the right each with
 

787
00:20:26,400 --> 00:20:31,250
of images on the right each with
different results that show where each

788
00:20:31,250 --> 00:20:31,260
different results that show where each
 

789
00:20:31,260 --> 00:20:32,960
different results that show where each
of the individual filter patterns were

790
00:20:32,960 --> 00:20:32,970
of the individual filter patterns were
 

791
00:20:32,970 --> 00:20:36,169
of the individual filter patterns were
found and we learned what patterns are

792
00:20:36,169 --> 00:20:36,179
found and we learned what patterns are
 

793
00:20:36,179 --> 00:20:38,690
found and we learned what patterns are
useful to look for in order to perform

794
00:20:38,690 --> 00:20:38,700
useful to look for in order to perform
 

795
00:20:38,700 --> 00:20:41,510
useful to look for in order to perform
the classification task that's the task

796
00:20:41,510 --> 00:20:41,520
the classification task that's the task
 

797
00:20:41,520 --> 00:20:43,940
the classification task that's the task
for the neural network to learn these

798
00:20:43,940 --> 00:20:43,950
for the neural network to learn these
 

799
00:20:43,950 --> 00:20:45,800
for the neural network to learn these
filters

800
00:20:45,800 --> 00:20:45,810
filters
 

801
00:20:45,810 --> 00:20:48,230
filters
and the filters have higher and higher

802
00:20:48,230 --> 00:20:48,240
and the filters have higher and higher
 

803
00:20:48,240 --> 00:20:54,350
and the filters have higher and higher
order of representation going from the

804
00:20:54,350 --> 00:20:54,360
order of representation going from the
 

805
00:20:54,360 --> 00:20:57,190
order of representation going from the
very basic edges to the high semantics

806
00:20:57,190 --> 00:20:57,200
very basic edges to the high semantics
 

807
00:20:57,200 --> 00:21:02,840
very basic edges to the high semantics
meaning that spans entire images and the

808
00:21:02,840 --> 00:21:02,850
meaning that spans entire images and the
 

809
00:21:02,850 --> 00:21:05,420
meaning that spans entire images and the
ability to spend images can be done in

810
00:21:05,420 --> 00:21:05,430
ability to spend images can be done in
 

811
00:21:05,430 --> 00:21:07,670
ability to spend images can be done in
several ways but traditionally has been

812
00:21:07,670 --> 00:21:07,680
several ways but traditionally has been
 

813
00:21:07,680 --> 00:21:09,440
several ways but traditionally has been
successfully done through max pooling

814
00:21:09,440 --> 00:21:09,450
successfully done through max pooling
 

815
00:21:09,450 --> 00:21:15,370
successfully done through max pooling
through pooling of taking the output of

816
00:21:15,370 --> 00:21:15,380
through pooling of taking the output of
 

817
00:21:15,380 --> 00:21:19,070
through pooling of taking the output of
convolutional operation and reducing the

818
00:21:19,070 --> 00:21:19,080
convolutional operation and reducing the
 

819
00:21:19,080 --> 00:21:23,420
convolutional operation and reducing the
resolution of that byte by condensing

820
00:21:23,420 --> 00:21:23,430
resolution of that byte by condensing
 

821
00:21:23,430 --> 00:21:25,340
resolution of that byte by condensing
that information by for example taking

822
00:21:25,340 --> 00:21:25,350
that information by for example taking
 

823
00:21:25,350 --> 00:21:27,110
that information by for example taking
the maximum values the maximum

824
00:21:27,110 --> 00:21:27,120
the maximum values the maximum
 

825
00:21:27,120 --> 00:21:33,430
the maximum values the maximum
activations therefore reducing the

826
00:21:33,430 --> 00:21:33,440
activations therefore reducing the
 

827
00:21:33,440 --> 00:21:36,500
activations therefore reducing the
spatial resolution which has detrimental

828
00:21:36,500 --> 00:21:36,510
spatial resolution which has detrimental
 

829
00:21:36,510 --> 00:21:38,330
spatial resolution which has detrimental
effects as we'll talk about in the scene

830
00:21:38,330 --> 00:21:38,340
effects as we'll talk about in the scene
 

831
00:21:38,340 --> 00:21:41,150
effects as we'll talk about in the scene
segmentation but it's beneficial for

832
00:21:41,150 --> 00:21:41,160
segmentation but it's beneficial for
 

833
00:21:41,160 --> 00:21:43,730
segmentation but it's beneficial for
finding higher order representations and

834
00:21:43,730 --> 00:21:43,740
finding higher order representations and
 

835
00:21:43,740 --> 00:21:45,850
finding higher order representations and
the images that bring images together

836
00:21:45,850 --> 00:21:45,860
the images that bring images together
 

837
00:21:45,860 --> 00:21:49,070
the images that bring images together
that bring features together to form an

838
00:21:49,070 --> 00:21:49,080
that bring features together to form an
 

839
00:21:49,080 --> 00:21:50,960
that bring features together to form an
entity that we're trying to identify and

840
00:21:50,960 --> 00:21:50,970
entity that we're trying to identify and
 

841
00:21:50,970 --> 00:21:56,080
entity that we're trying to identify and
classify okay so that forms a

842
00:21:56,080 --> 00:21:56,090
classify okay so that forms a
 

843
00:21:56,090 --> 00:21:57,980
classify okay so that forms a
convolution Yool network such

844
00:21:57,980 --> 00:21:57,990
convolution Yool network such
 

845
00:21:57,990 --> 00:21:59,600
convolution Yool network such
convolutional layers stacked on top of

846
00:21:59,600 --> 00:21:59,610
convolutional layers stacked on top of
 

847
00:21:59,610 --> 00:22:01,940
convolutional layers stacked on top of
each other is the only addition to a

848
00:22:01,940 --> 00:22:01,950
each other is the only addition to a
 

849
00:22:01,950 --> 00:22:03,800
each other is the only addition to a
neural network that makes for a

850
00:22:03,800 --> 00:22:03,810
neural network that makes for a
 

851
00:22:03,810 --> 00:22:06,320
neural network that makes for a
convolutional neural network and then at

852
00:22:06,320 --> 00:22:06,330
convolutional neural network and then at
 

853
00:22:06,330 --> 00:22:08,630
convolutional neural network and then at
the end the fully connected layers or

854
00:22:08,630 --> 00:22:08,640
the end the fully connected layers or
 

855
00:22:08,640 --> 00:22:12,620
the end the fully connected layers or
any kind of other architectures allow us

856
00:22:12,620 --> 00:22:12,630
any kind of other architectures allow us
 

857
00:22:12,630 --> 00:22:14,990
any kind of other architectures allow us
to apply particular domains

858
00:22:14,990 --> 00:22:15,000
to apply particular domains
 

859
00:22:15,000 --> 00:22:20,630
to apply particular domains
let's take image net as a case study an

860
00:22:20,630 --> 00:22:20,640
let's take image net as a case study an
 

861
00:22:20,640 --> 00:22:25,690
let's take image net as a case study an
image net the data set an image net the

862
00:22:25,690 --> 00:22:25,700
image net the data set an image net the
 

863
00:22:25,700 --> 00:22:26,990
image net the data set an image net the
challenge

864
00:22:26,990 --> 00:22:27,000
challenge
 

865
00:22:27,000 --> 00:22:30,380
challenge
the task is classification as I

866
00:22:30,380 --> 00:22:30,390
the task is classification as I
 

867
00:22:30,390 --> 00:22:32,780
the task is classification as I
mentioned the first lecture image net is

868
00:22:32,780 --> 00:22:32,790
mentioned the first lecture image net is
 

869
00:22:32,790 --> 00:22:35,750
mentioned the first lecture image net is
a data set one of the largest in the

870
00:22:35,750 --> 00:22:35,760
a data set one of the largest in the
 

871
00:22:35,760 --> 00:22:39,130
a data set one of the largest in the
world of images with 14 million images

872
00:22:39,130 --> 00:22:39,140
world of images with 14 million images
 

873
00:22:39,140 --> 00:22:44,720
world of images with 14 million images
21,000 categories and a lot of depth to

874
00:22:44,720 --> 00:22:44,730
21,000 categories and a lot of depth to
 

875
00:22:44,730 --> 00:22:47,270
21,000 categories and a lot of depth to
many of the categories as I mentioned

876
00:22:47,270 --> 00:22:47,280
many of the categories as I mentioned
 

877
00:22:47,280 --> 00:22:52,409
many of the categories as I mentioned
1200 granny smith apples

878
00:22:52,409 --> 00:22:52,419

 

879
00:22:52,419 --> 00:22:55,390

these allow - these allow the newer

880
00:22:55,390 --> 00:22:55,400
these allow - these allow the newer
 

881
00:22:55,400 --> 00:22:58,060
these allow - these allow the newer
networks to learn the rich

882
00:22:58,060 --> 00:22:58,070
networks to learn the rich
 

883
00:22:58,070 --> 00:23:00,400
networks to learn the rich
representations in both pose lighting

884
00:23:00,400 --> 00:23:00,410
representations in both pose lighting
 

885
00:23:00,410 --> 00:23:02,200
representations in both pose lighting
variability and intraclass class

886
00:23:02,200 --> 00:23:02,210
variability and intraclass class
 

887
00:23:02,210 --> 00:23:03,960
variability and intraclass class
variation for the particular things

888
00:23:03,960 --> 00:23:03,970
variation for the particular things
 

889
00:23:03,970 --> 00:23:07,240
variation for the particular things
particular classes like granny smith

890
00:23:07,240 --> 00:23:07,250
particular classes like granny smith
 

891
00:23:07,250 --> 00:23:10,930
particular classes like granny smith
apples so let's look through the various

892
00:23:10,930 --> 00:23:10,940
apples so let's look through the various
 

893
00:23:10,940 --> 00:23:13,120
apples so let's look through the various
networks let's discuss them let's see

894
00:23:13,120 --> 00:23:13,130
networks let's discuss them let's see
 

895
00:23:13,130 --> 00:23:13,900
networks let's discuss them let's see
the insights

896
00:23:13,900 --> 00:23:13,910
the insights
 

897
00:23:13,910 --> 00:23:16,419
the insights
it started with Alex net the first

898
00:23:16,419 --> 00:23:16,429
it started with Alex net the first
 

899
00:23:16,429 --> 00:23:19,600
it started with Alex net the first
really big successful GPU trained neural

900
00:23:19,600 --> 00:23:19,610
really big successful GPU trained neural
 

901
00:23:19,610 --> 00:23:21,850
really big successful GPU trained neural
network on image net that's achieved a

902
00:23:21,850 --> 00:23:21,860
network on image net that's achieved a
 

903
00:23:21,860 --> 00:23:23,799
network on image net that's achieved a
significant boost over the previous year

904
00:23:23,799 --> 00:23:23,809
significant boost over the previous year
 

905
00:23:23,809 --> 00:23:31,029
significant boost over the previous year
and moved on to vgg net Google net ague

906
00:23:31,029 --> 00:23:31,039
and moved on to vgg net Google net ague
 

907
00:23:31,039 --> 00:23:35,620
and moved on to vgg net Google net ague
Lynnette ResNet see you image and as

908
00:23:35,620 --> 00:23:35,630
Lynnette ResNet see you image and as
 

909
00:23:35,630 --> 00:23:42,520
Lynnette ResNet see you image and as
Annette in 2017 again the numbers will

910
00:23:42,520 --> 00:23:42,530
Annette in 2017 again the numbers will
 

911
00:23:42,530 --> 00:23:44,710
Annette in 2017 again the numbers will
show for the accuracy are based on the

912
00:23:44,710 --> 00:23:44,720
show for the accuracy are based on the
 

913
00:23:44,720 --> 00:23:48,039
show for the accuracy are based on the
top five error rate we get five guesses

914
00:23:48,039 --> 00:23:48,049
top five error rate we get five guesses
 

915
00:23:48,049 --> 00:23:51,039
top five error rate we get five guesses
and it's a one or zero if you get guess

916
00:23:51,039 --> 00:23:51,049
and it's a one or zero if you get guess
 

917
00:23:51,049 --> 00:23:52,960
and it's a one or zero if you get guess
if one of the five is correct you get a

918
00:23:52,960 --> 00:23:52,970
if one of the five is correct you get a
 

919
00:23:52,970 --> 00:23:55,240
if one of the five is correct you get a
one for that particular guess otherwise

920
00:23:55,240 --> 00:23:55,250
one for that particular guess otherwise
 

921
00:23:55,250 --> 00:24:03,700
one for that particular guess otherwise
it's a zero and human error is five

922
00:24:03,700 --> 00:24:03,710
it's a zero and human error is five
 

923
00:24:03,710 --> 00:24:05,710
it's a zero and human error is five
point one when a human tries to achieve

924
00:24:05,710 --> 00:24:05,720
point one when a human tries to achieve
 

925
00:24:05,720 --> 00:24:08,740
point one when a human tries to achieve
the same tries to perform the same task

926
00:24:08,740 --> 00:24:08,750
the same tries to perform the same task
 

927
00:24:08,750 --> 00:24:11,289
the same tries to perform the same task
as the machinist task of doing the air

928
00:24:11,289 --> 00:24:11,299
as the machinist task of doing the air
 

929
00:24:11,299 --> 00:24:13,690
as the machinist task of doing the air
is five point one the human annotation

930
00:24:13,690 --> 00:24:13,700
is five point one the human annotation
 

931
00:24:13,700 --> 00:24:15,669
is five point one the human annotation
is performed on the images based on

932
00:24:15,669 --> 00:24:15,679
is performed on the images based on
 

933
00:24:15,679 --> 00:24:18,039
is performed on the images based on
binary classification Granny Smith apple

934
00:24:18,039 --> 00:24:18,049
binary classification Granny Smith apple
 

935
00:24:18,049 --> 00:24:22,360
binary classification Granny Smith apple
or not cat or not the actual tasks that

936
00:24:22,360 --> 00:24:22,370
or not cat or not the actual tasks that
 

937
00:24:22,370 --> 00:24:24,220
or not cat or not the actual tasks that
the machine has to perform and that the

938
00:24:24,220 --> 00:24:24,230
the machine has to perform and that the
 

939
00:24:24,230 --> 00:24:27,220
the machine has to perform and that the
human competing has to perform is given

940
00:24:27,220 --> 00:24:27,230
human competing has to perform is given
 

941
00:24:27,230 --> 00:24:29,529
human competing has to perform is given
an image is provide one of the many

942
00:24:29,529 --> 00:24:29,539
an image is provide one of the many
 

943
00:24:29,539 --> 00:24:33,909
an image is provide one of the many
classes under that human errors 5.1%

944
00:24:33,909 --> 00:24:33,919
classes under that human errors 5.1%
 

945
00:24:33,919 --> 00:24:38,860
classes under that human errors 5.1%
which was surpassed in 2015 by ResNet to

946
00:24:38,860 --> 00:24:38,870
which was surpassed in 2015 by ResNet to
 

947
00:24:38,870 --> 00:24:44,850
which was surpassed in 2015 by ResNet to
achieve four percent error so let's

948
00:24:44,850 --> 00:24:44,860
achieve four percent error so let's
 

949
00:24:44,860 --> 00:24:47,610
achieve four percent error so let's
with Alex net I'll zoom in on the later

950
00:24:47,610 --> 00:24:47,620
with Alex net I'll zoom in on the later
 

951
00:24:47,620 --> 00:24:49,080
with Alex net I'll zoom in on the later
networks they have some interesting

952
00:24:49,080 --> 00:24:49,090
networks they have some interesting
 

953
00:24:49,090 --> 00:24:53,850
networks they have some interesting
insights but Alex net and vgg net both

954
00:24:53,850 --> 00:24:53,860
insights but Alex net and vgg net both
 

955
00:24:53,860 --> 00:24:56,700
insights but Alex net and vgg net both
fall at a very similar architecture very

956
00:24:56,700 --> 00:24:56,710
fall at a very similar architecture very
 

957
00:24:56,710 --> 00:25:02,850
fall at a very similar architecture very
uniform throughout its depth vgg net in

958
00:25:02,850 --> 00:25:02,860
uniform throughout its depth vgg net in
 

959
00:25:02,860 --> 00:25:08,160
uniform throughout its depth vgg net in
2014 is convolution convolution pooling

960
00:25:08,160 --> 00:25:08,170
2014 is convolution convolution pooling
 

961
00:25:08,170 --> 00:25:10,440
2014 is convolution convolution pooling
convolution pooling convolution pooling

962
00:25:10,440 --> 00:25:10,450
convolution pooling convolution pooling
 

963
00:25:10,450 --> 00:25:13,400
convolution pooling convolution pooling
and fully connected layers at the end

964
00:25:13,400 --> 00:25:13,410
and fully connected layers at the end
 

965
00:25:13,410 --> 00:25:15,720
and fully connected layers at the end
there's a certain kind of beautiful

966
00:25:15,720 --> 00:25:15,730
there's a certain kind of beautiful
 

967
00:25:15,730 --> 00:25:17,370
there's a certain kind of beautiful
simplicity uniformity to these

968
00:25:17,370 --> 00:25:17,380
simplicity uniformity to these
 

969
00:25:17,380 --> 00:25:19,380
simplicity uniformity to these
architectures because you can just make

970
00:25:19,380 --> 00:25:19,390
architectures because you can just make
 

971
00:25:19,390 --> 00:25:21,510
architectures because you can just make
it deeper and deeper and makes it very

972
00:25:21,510 --> 00:25:21,520
it deeper and deeper and makes it very
 

973
00:25:21,520 --> 00:25:24,720
it deeper and deeper and makes it very
amenable to implementation in a layer

974
00:25:24,720 --> 00:25:24,730
amenable to implementation in a layer
 

975
00:25:24,730 --> 00:25:27,570
amenable to implementation in a layer
stack kind of way and in any of the deep

976
00:25:27,570 --> 00:25:27,580
stack kind of way and in any of the deep
 

977
00:25:27,580 --> 00:25:29,909
stack kind of way and in any of the deep
learning frameworks it's clean and

978
00:25:29,909 --> 00:25:29,919
learning frameworks it's clean and
 

979
00:25:29,919 --> 00:25:32,010
learning frameworks it's clean and
beautiful to understand in the case of

980
00:25:32,010 --> 00:25:32,020
beautiful to understand in the case of
 

981
00:25:32,020 --> 00:25:35,340
beautiful to understand in the case of
eg gina was 16 or 19 layers with 138

982
00:25:35,340 --> 00:25:35,350
eg gina was 16 or 19 layers with 138
 

983
00:25:35,350 --> 00:25:37,080
eg gina was 16 or 19 layers with 138
million parameters not many

984
00:25:37,080 --> 00:25:37,090
million parameters not many
 

985
00:25:37,090 --> 00:25:38,400
million parameters not many
optimizations and these parameters

986
00:25:38,400 --> 00:25:38,410
optimizations and these parameters
 

987
00:25:38,410 --> 00:25:40,770
optimizations and these parameters
therefore the number of parameters is

988
00:25:40,770 --> 00:25:40,780
therefore the number of parameters is
 

989
00:25:40,780 --> 00:25:42,450
therefore the number of parameters is
much higher than the networks that

990
00:25:42,450 --> 00:25:42,460
much higher than the networks that
 

991
00:25:42,460 --> 00:25:44,700
much higher than the networks that
followed it despite the layers not being

992
00:25:44,700 --> 00:25:44,710
followed it despite the layers not being
 

993
00:25:44,710 --> 00:25:49,409
followed it despite the layers not being
that large Google Net introduced the

994
00:25:49,409 --> 00:25:49,419
that large Google Net introduced the
 

995
00:25:49,419 --> 00:25:52,440
that large Google Net introduced the
inception module starting to do some

996
00:25:52,440 --> 00:25:52,450
inception module starting to do some
 

997
00:25:52,450 --> 00:25:55,520
inception module starting to do some
interesting things with the small

998
00:25:55,520 --> 00:25:55,530
interesting things with the small
 

999
00:25:55,530 --> 00:25:58,140
interesting things with the small
modules within these networks which

1000
00:25:58,140 --> 00:25:58,150
modules within these networks which
 

1001
00:25:58,150 --> 00:25:59,930
modules within these networks which
allow for the training to be more

1002
00:25:59,930 --> 00:25:59,940
allow for the training to be more
 

1003
00:25:59,940 --> 00:26:04,440
allow for the training to be more
efficient and effective the idea behind

1004
00:26:04,440 --> 00:26:04,450
efficient and effective the idea behind
 

1005
00:26:04,450 --> 00:26:07,860
efficient and effective the idea behind
the inception module shown here with the

1006
00:26:07,860 --> 00:26:07,870
the inception module shown here with the
 

1007
00:26:07,870 --> 00:26:11,520
the inception module shown here with the
previous layer on bottom and the

1008
00:26:11,520 --> 00:26:11,530
previous layer on bottom and the
 

1009
00:26:11,530 --> 00:26:14,280
previous layer on bottom and the
convolutional layer here with the

1010
00:26:14,280 --> 00:26:14,290
convolutional layer here with the
 

1011
00:26:14,290 --> 00:26:18,840
convolutional layer here with the
inception module on top produced on top

1012
00:26:18,840 --> 00:26:18,850
inception module on top produced on top
 

1013
00:26:18,850 --> 00:26:24,419
inception module on top produced on top
is it used the idea that different size

1014
00:26:24,419 --> 00:26:24,429
is it used the idea that different size
 

1015
00:26:24,429 --> 00:26:26,909
is it used the idea that different size
convolutions provide different value for

1016
00:26:26,909 --> 00:26:26,919
convolutions provide different value for
 

1017
00:26:26,919 --> 00:26:29,970
convolutions provide different value for
the network smaller convolutions are

1018
00:26:29,970 --> 00:26:29,980
the network smaller convolutions are
 

1019
00:26:29,980 --> 00:26:33,710
the network smaller convolutions are
able to capture or propagate forward

1020
00:26:33,710 --> 00:26:33,720
able to capture or propagate forward
 

1021
00:26:33,720 --> 00:26:37,830
able to capture or propagate forward
features that are very local a high

1022
00:26:37,830 --> 00:26:37,840
features that are very local a high
 

1023
00:26:37,840 --> 00:26:41,970
features that are very local a high
resolution in in in texture larger

1024
00:26:41,970 --> 00:26:41,980
resolution in in in texture larger
 

1025
00:26:41,980 --> 00:26:44,460
resolution in in in texture larger
convolutions are better able to

1026
00:26:44,460 --> 00:26:44,470
convolutions are better able to
 

1027
00:26:44,470 --> 00:26:48,299
convolutions are better able to
represent and capture and catch highly

1028
00:26:48,299 --> 00:26:48,309
represent and capture and catch highly
 

1029
00:26:48,309 --> 00:26:50,010
represent and capture and catch highly
abstracted features higher-order

1030
00:26:50,010 --> 00:26:50,020
abstracted features higher-order
 

1031
00:26:50,020 --> 00:26:52,770
abstracted features higher-order
features so the idea behind the

1032
00:26:52,770 --> 00:26:52,780
features so the idea behind the
 

1033
00:26:52,780 --> 00:26:55,590
features so the idea behind the
inception module is to say well as

1034
00:26:55,590 --> 00:26:55,600
inception module is to say well as
 

1035
00:26:55,600 --> 00:26:58,350
inception module is to say well as
opposed to choosing and high in a high

1036
00:26:58,350 --> 00:26:58,360
opposed to choosing and high in a high
 

1037
00:26:58,360 --> 00:26:58,749
opposed to choosing and high in a high
pair

1038
00:26:58,749 --> 00:26:58,759
pair
 

1039
00:26:58,759 --> 00:27:01,209
pair
tuning process or architecture design

1040
00:27:01,209 --> 00:27:01,219
tuning process or architecture design
 

1041
00:27:01,219 --> 00:27:03,669
tuning process or architecture design
process choosing which convolution size

1042
00:27:03,669 --> 00:27:03,679
process choosing which convolution size
 

1043
00:27:03,679 --> 00:27:06,489
process choosing which convolution size
we want to go with why not do all of

1044
00:27:06,489 --> 00:27:06,499
we want to go with why not do all of
 

1045
00:27:06,499 --> 00:27:08,799
we want to go with why not do all of
them together while several together in

1046
00:27:08,799 --> 00:27:08,809
them together while several together in
 

1047
00:27:08,809 --> 00:27:11,979
them together while several together in
the case of the Google net model there's

1048
00:27:11,979 --> 00:27:11,989
the case of the Google net model there's
 

1049
00:27:11,989 --> 00:27:14,079
the case of the Google net model there's
the one by one three by three and five

1050
00:27:14,079 --> 00:27:14,089
the one by one three by three and five
 

1051
00:27:14,089 --> 00:27:17,079
the one by one three by three and five
by five convolutions with the old trusty

1052
00:27:17,079 --> 00:27:17,089
by five convolutions with the old trusty
 

1053
00:27:17,089 --> 00:27:19,029
by five convolutions with the old trusty
friend of max pooling still left in

1054
00:27:19,029 --> 00:27:19,039
friend of max pooling still left in
 

1055
00:27:19,039 --> 00:27:23,589
friend of max pooling still left in
there as well which has lost favor more

1056
00:27:23,589 --> 00:27:23,599
there as well which has lost favor more
 

1057
00:27:23,599 --> 00:27:24,699
there as well which has lost favor more
and more over time for the image

1058
00:27:24,699 --> 00:27:24,709
and more over time for the image
 

1059
00:27:24,709 --> 00:27:28,439
and more over time for the image
classification task and the results is

1060
00:27:28,439 --> 00:27:28,449
classification task and the results is
 

1061
00:27:28,449 --> 00:27:31,269
classification task and the results is
there's fewer parameters are required if

1062
00:27:31,269 --> 00:27:31,279
there's fewer parameters are required if
 

1063
00:27:31,279 --> 00:27:35,889
there's fewer parameters are required if
you pick the placing of these inception

1064
00:27:35,889 --> 00:27:35,899
you pick the placing of these inception
 

1065
00:27:35,899 --> 00:27:37,899
you pick the placing of these inception
modules correctly the number of

1066
00:27:37,899 --> 00:27:37,909
modules correctly the number of
 

1067
00:27:37,909 --> 00:27:40,749
modules correctly the number of
parameters required to achieve a higher

1068
00:27:40,749 --> 00:27:40,759
parameters required to achieve a higher
 

1069
00:27:40,759 --> 00:27:48,249
parameters required to achieve a higher
performance is much lower res net one of

1070
00:27:48,249 --> 00:27:48,259
performance is much lower res net one of
 

1071
00:27:48,259 --> 00:27:53,549
performance is much lower res net one of
the most popular still to date

1072
00:27:53,549 --> 00:27:53,559

 

1073
00:27:53,559 --> 00:27:56,339

architectures that we'll discuss in

1074
00:27:56,339 --> 00:27:56,349
architectures that we'll discuss in
 

1075
00:27:56,349 --> 00:28:01,419
architectures that we'll discuss in
scene segmentation as well came up and

1076
00:28:01,419 --> 00:28:01,429
scene segmentation as well came up and
 

1077
00:28:01,429 --> 00:28:05,499
scene segmentation as well came up and
use the idea of a residual block the

1078
00:28:05,499 --> 00:28:05,509
use the idea of a residual block the
 

1079
00:28:05,509 --> 00:28:08,499
use the idea of a residual block the
initial inspiring observation which

1080
00:28:08,499 --> 00:28:08,509
initial inspiring observation which
 

1081
00:28:08,509 --> 00:28:10,419
initial inspiring observation which
doesn't necessarily hold true as it

1082
00:28:10,419 --> 00:28:10,429
doesn't necessarily hold true as it
 

1083
00:28:10,429 --> 00:28:14,099
doesn't necessarily hold true as it
turns out but that network depth

1084
00:28:14,099 --> 00:28:14,109
turns out but that network depth
 

1085
00:28:14,109 --> 00:28:17,289
turns out but that network depth
increases representation power so these

1086
00:28:17,289 --> 00:28:17,299
increases representation power so these
 

1087
00:28:17,299 --> 00:28:20,499
increases representation power so these
residual blocks allow you to have much

1088
00:28:20,499 --> 00:28:20,509
residual blocks allow you to have much
 

1089
00:28:20,509 --> 00:28:23,019
residual blocks allow you to have much
deeper networks and I'll explain why in

1090
00:28:23,019 --> 00:28:23,029
deeper networks and I'll explain why in
 

1091
00:28:23,029 --> 00:28:27,489
deeper networks and I'll explain why in
a second here but the thought was they

1092
00:28:27,489 --> 00:28:27,499
a second here but the thought was they
 

1093
00:28:27,499 --> 00:28:28,989
a second here but the thought was they
work so well because the network's so

1094
00:28:28,989 --> 00:28:28,999
work so well because the network's so
 

1095
00:28:28,999 --> 00:28:32,409
work so well because the network's so
much deeper the key thing that makes

1096
00:28:32,409 --> 00:28:32,419
much deeper the key thing that makes
 

1097
00:28:32,419 --> 00:28:35,249
much deeper the key thing that makes
these blocks so effective is the same

1098
00:28:35,249 --> 00:28:35,259
these blocks so effective is the same
 

1099
00:28:35,259 --> 00:28:38,289
these blocks so effective is the same
idea that's that reminiscent of

1100
00:28:38,289 --> 00:28:38,299
idea that's that reminiscent of
 

1101
00:28:38,299 --> 00:28:40,719
idea that's that reminiscent of
recurrent neural networks that I hope

1102
00:28:40,719 --> 00:28:40,729
recurrent neural networks that I hope
 

1103
00:28:40,729 --> 00:28:43,680
recurrent neural networks that I hope
would get a chance to talk about the

1104
00:28:43,680 --> 00:28:43,690
would get a chance to talk about the
 

1105
00:28:43,690 --> 00:28:48,159
would get a chance to talk about the
training of them is much easier they

1106
00:28:48,159 --> 00:28:48,169
training of them is much easier they
 

1107
00:28:48,169 --> 00:28:51,189
training of them is much easier they
take a simple block repeated over and

1108
00:28:51,189 --> 00:28:51,199
take a simple block repeated over and
 

1109
00:28:51,199 --> 00:28:54,369
take a simple block repeated over and
over and they pass the input along

1110
00:28:54,369 --> 00:28:54,379
over and they pass the input along
 

1111
00:28:54,379 --> 00:28:57,819
over and they pass the input along
without transformation along with the

1112
00:28:57,819 --> 00:28:57,829
without transformation along with the
 

1113
00:28:57,829 --> 00:29:00,639
without transformation along with the
ability to transform it to learn to

1114
00:29:00,639 --> 00:29:00,649
ability to transform it to learn to
 

1115
00:29:00,649 --> 00:29:04,450
ability to transform it to learn to
learn the filters learn the weights

1116
00:29:04,450 --> 00:29:04,460
learn the filters learn the weights
 

1117
00:29:04,460 --> 00:29:08,169
learn the filters learn the weights
so you're allowed to you're allow every

1118
00:29:08,169 --> 00:29:08,179
so you're allowed to you're allow every
 

1119
00:29:08,179 --> 00:29:12,129
so you're allowed to you're allow every
layer to not only take on the processing

1120
00:29:12,129 --> 00:29:12,139
layer to not only take on the processing
 

1121
00:29:12,139 --> 00:29:14,980
layer to not only take on the processing
of previous layers but to take in the

1122
00:29:14,980 --> 00:29:14,990
of previous layers but to take in the
 

1123
00:29:14,990 --> 00:29:17,470
of previous layers but to take in the
wrong transform data and learn something

1124
00:29:17,470 --> 00:29:17,480
wrong transform data and learn something
 

1125
00:29:17,480 --> 00:29:21,340
wrong transform data and learn something
new the ability to learn something new

1126
00:29:21,340 --> 00:29:21,350
new the ability to learn something new
 

1127
00:29:21,350 --> 00:29:24,190
new the ability to learn something new
allows you to have much deeper networks

1128
00:29:24,190 --> 00:29:24,200
allows you to have much deeper networks
 

1129
00:29:24,200 --> 00:29:27,460
allows you to have much deeper networks
and the simplicity of this block allows

1130
00:29:27,460 --> 00:29:27,470
and the simplicity of this block allows
 

1131
00:29:27,470 --> 00:29:34,119
and the simplicity of this block allows
for more effective training the state of

1132
00:29:34,119 --> 00:29:34,129
for more effective training the state of
 

1133
00:29:34,129 --> 00:29:36,940
for more effective training the state of
the art in 2017 the winner is squeezed

1134
00:29:36,940 --> 00:29:36,950
the art in 2017 the winner is squeezed
 

1135
00:29:36,950 --> 00:29:40,899
the art in 2017 the winner is squeezed
and excitation networks that unlike the

1136
00:29:40,899 --> 00:29:40,909
and excitation networks that unlike the
 

1137
00:29:40,909 --> 00:29:42,940
and excitation networks that unlike the
previous year will see you image which

1138
00:29:42,940 --> 00:29:42,950
previous year will see you image which
 

1139
00:29:42,950 --> 00:29:44,799
previous year will see you image which
simply took ensemble methods and

1140
00:29:44,799 --> 00:29:44,809
simply took ensemble methods and
 

1141
00:29:44,809 --> 00:29:46,989
simply took ensemble methods and
combined a lot of successful approaches

1142
00:29:46,989 --> 00:29:46,999
combined a lot of successful approaches
 

1143
00:29:46,999 --> 00:29:51,389
combined a lot of successful approaches
to take a marginal improvement se net

1144
00:29:51,389 --> 00:29:51,399
to take a marginal improvement se net
 

1145
00:29:51,399 --> 00:29:55,029
to take a marginal improvement se net
got a significant improvement at least

1146
00:29:55,029 --> 00:29:55,039
got a significant improvement at least
 

1147
00:29:55,039 --> 00:29:57,039
got a significant improvement at least
in percentages I think there's a 25%

1148
00:29:57,039 --> 00:29:57,049
in percentages I think there's a 25%
 

1149
00:29:57,049 --> 00:30:02,470
in percentages I think there's a 25%
reduction in error from 4 percent to 3

1150
00:30:02,470 --> 00:30:02,480
reduction in error from 4 percent to 3
 

1151
00:30:02,480 --> 00:30:07,299
reduction in error from 4 percent to 3
percent something like that by using a

1152
00:30:07,299 --> 00:30:07,309
percent something like that by using a
 

1153
00:30:07,309 --> 00:30:09,190
percent something like that by using a
very simple idea that I think is

1154
00:30:09,190 --> 00:30:09,200
very simple idea that I think is
 

1155
00:30:09,200 --> 00:30:12,239
very simple idea that I think is
important to mention a simple insight

1156
00:30:12,239 --> 00:30:12,249
important to mention a simple insight
 

1157
00:30:12,249 --> 00:30:16,180
important to mention a simple insight
it added a parameter to each channel and

1158
00:30:16,180 --> 00:30:16,190
it added a parameter to each channel and
 

1159
00:30:16,190 --> 00:30:19,389
it added a parameter to each channel and
the convolutional layer in the

1160
00:30:19,389 --> 00:30:19,399
the convolutional layer in the
 

1161
00:30:19,399 --> 00:30:22,480
the convolutional layer in the
convolutional block so the network can

1162
00:30:22,480 --> 00:30:22,490
convolutional block so the network can
 

1163
00:30:22,490 --> 00:30:25,859
convolutional block so the network can
now adjust the weighting on each channel

1164
00:30:25,859 --> 00:30:25,869
now adjust the weighting on each channel
 

1165
00:30:25,869 --> 00:30:29,440
now adjust the weighting on each channel
based for for each feature map based on

1166
00:30:29,440 --> 00:30:29,450
based for for each feature map based on
 

1167
00:30:29,450 --> 00:30:30,999
based for for each feature map based on
the content based on the input to the

1168
00:30:30,999 --> 00:30:31,009
the content based on the input to the
 

1169
00:30:31,009 --> 00:30:34,299
the content based on the input to the
network this is kind of a take away to

1170
00:30:34,299 --> 00:30:34,309
network this is kind of a take away to
 

1171
00:30:34,309 --> 00:30:36,700
network this is kind of a take away to
think about about any of the networks

1172
00:30:36,700 --> 00:30:36,710
think about about any of the networks
 

1173
00:30:36,710 --> 00:30:38,289
think about about any of the networks
who talk about any of the architectures

1174
00:30:38,289 --> 00:30:38,299
who talk about any of the architectures
 

1175
00:30:38,299 --> 00:30:42,580
who talk about any of the architectures
is a lot of times your recurrent neural

1176
00:30:42,580 --> 00:30:42,590
is a lot of times your recurrent neural
 

1177
00:30:42,590 --> 00:30:44,230
is a lot of times your recurrent neural
networks and convolutional neural

1178
00:30:44,230 --> 00:30:44,240
networks and convolutional neural
 

1179
00:30:44,240 --> 00:30:47,619
networks and convolutional neural
networks have tricks that significantly

1180
00:30:47,619 --> 00:30:47,629
networks have tricks that significantly
 

1181
00:30:47,629 --> 00:30:50,950
networks have tricks that significantly
reduce the number of parameters the bulk

1182
00:30:50,950 --> 00:30:50,960
reduce the number of parameters the bulk
 

1183
00:30:50,960 --> 00:30:53,230
reduce the number of parameters the bulk
the sort of low-hanging fruit they use

1184
00:30:53,230 --> 00:30:53,240
the sort of low-hanging fruit they use
 

1185
00:30:53,240 --> 00:30:55,450
the sort of low-hanging fruit they use
spatial invariants a temporal invariants

1186
00:30:55,450 --> 00:30:55,460
spatial invariants a temporal invariants
 

1187
00:30:55,460 --> 00:30:57,310
spatial invariants a temporal invariants
to reduce the number of parameters to

1188
00:30:57,310 --> 00:30:57,320
to reduce the number of parameters to
 

1189
00:30:57,320 --> 00:31:00,879
to reduce the number of parameters to
represent the input data but they also

1190
00:31:00,879 --> 00:31:00,889
represent the input data but they also
 

1191
00:31:00,889 --> 00:31:03,279
represent the input data but they also
leave certain things not parameterize

1192
00:31:03,279 --> 00:31:03,289
leave certain things not parameterize
 

1193
00:31:03,289 --> 00:31:05,560
leave certain things not parameterize
they don't allow the network to learn it

1194
00:31:05,560 --> 00:31:05,570
they don't allow the network to learn it
 

1195
00:31:05,570 --> 00:31:07,930
they don't allow the network to learn it
allow in this case the network to learn

1196
00:31:07,930 --> 00:31:07,940
allow in this case the network to learn
 

1197
00:31:07,940 --> 00:31:10,029
allow in this case the network to learn
the weighting on each of the individual

1198
00:31:10,029 --> 00:31:10,039
the weighting on each of the individual
 

1199
00:31:10,039 --> 00:31:11,919
the weighting on each of the individual
channels so each of the individual

1200
00:31:11,919 --> 00:31:11,929
channels so each of the individual
 

1201
00:31:11,929 --> 00:31:14,529
channels so each of the individual
filters is something that you learn as

1202
00:31:14,529 --> 00:31:14,539
filters is something that you learn as
 

1203
00:31:14,539 --> 00:31:17,349
filters is something that you learn as
along with the filters takes it makes a

1204
00:31:17,349 --> 00:31:17,359
along with the filters takes it makes a
 

1205
00:31:17,359 --> 00:31:18,349
along with the filters takes it makes a
huge boost

1206
00:31:18,349 --> 00:31:18,359
huge boost
 

1207
00:31:18,359 --> 00:31:20,389
huge boost
the cool thing about this is it's

1208
00:31:20,389 --> 00:31:20,399
the cool thing about this is it's
 

1209
00:31:20,399 --> 00:31:22,789
the cool thing about this is it's
applicable to any architecture this kind

1210
00:31:22,789 --> 00:31:22,799
applicable to any architecture this kind
 

1211
00:31:22,799 --> 00:31:24,709
applicable to any architecture this kind
of block that's kind of what the the

1212
00:31:24,709 --> 00:31:24,719
of block that's kind of what the the
 

1213
00:31:24,719 --> 00:31:26,769
of block that's kind of what the the
squeeze and excitation block is

1214
00:31:26,769 --> 00:31:26,779
squeeze and excitation block is
 

1215
00:31:26,779 --> 00:31:31,689
squeeze and excitation block is
applicable to any architecture and

1216
00:31:31,689 --> 00:31:31,699
applicable to any architecture and
 

1217
00:31:31,699 --> 00:31:35,389
applicable to any architecture and
because obviously it it just simply

1218
00:31:35,389 --> 00:31:35,399
because obviously it it just simply
 

1219
00:31:35,399 --> 00:31:37,759
because obviously it it just simply
permit Rises the ability to choose which

1220
00:31:37,759 --> 00:31:37,769
permit Rises the ability to choose which
 

1221
00:31:37,769 --> 00:31:39,769
permit Rises the ability to choose which
filter you go with based on the content

1222
00:31:39,769 --> 00:31:39,779
filter you go with based on the content
 

1223
00:31:39,779 --> 00:31:42,649
filter you go with based on the content
it's a subtle but crucial thing I think

1224
00:31:42,649 --> 00:31:42,659
it's a subtle but crucial thing I think
 

1225
00:31:42,659 --> 00:31:45,049
it's a subtle but crucial thing I think
it's pretty cool and for future research

1226
00:31:45,049 --> 00:31:45,059
it's pretty cool and for future research
 

1227
00:31:45,059 --> 00:31:48,499
it's pretty cool and for future research
it inspires to think about what else can

1228
00:31:48,499 --> 00:31:48,509
it inspires to think about what else can
 

1229
00:31:48,509 --> 00:31:49,879
it inspires to think about what else can
be parameterize in your own networks

1230
00:31:49,879 --> 00:31:49,889
be parameterize in your own networks
 

1231
00:31:49,889 --> 00:31:52,249
be parameterize in your own networks
what else can be controlled as part of

1232
00:31:52,249 --> 00:31:52,259
what else can be controlled as part of
 

1233
00:31:52,259 --> 00:31:54,529
what else can be controlled as part of
the learning process including hiring

1234
00:31:54,529 --> 00:31:54,539
the learning process including hiring
 

1235
00:31:54,539 --> 00:31:57,349
the learning process including hiring
higher-order hyper parameters which

1236
00:31:57,349 --> 00:31:57,359
higher-order hyper parameters which
 

1237
00:31:57,359 --> 00:32:00,319
higher-order hyper parameters which
which aspects of the training and the

1238
00:32:00,319 --> 00:32:00,329
which aspects of the training and the
 

1239
00:32:00,329 --> 00:32:02,059
which aspects of the training and the
architecture of the network can be part

1240
00:32:02,059 --> 00:32:02,069
architecture of the network can be part
 

1241
00:32:02,069 --> 00:32:04,129
architecture of the network can be part
of the learning this is what this

1242
00:32:04,129 --> 00:32:04,139
of the learning this is what this
 

1243
00:32:04,139 --> 00:32:14,299
of the learning this is what this
network inspires another network has

1244
00:32:14,299 --> 00:32:14,309
network inspires another network has
 

1245
00:32:14,309 --> 00:32:17,349
network inspires another network has
been in development since the 90s ideas

1246
00:32:17,349 --> 00:32:17,359
been in development since the 90s ideas
 

1247
00:32:17,359 --> 00:32:19,909
been in development since the 90s ideas
with geoff hinton but really received

1248
00:32:19,909 --> 00:32:19,919
with geoff hinton but really received
 

1249
00:32:19,919 --> 00:32:21,409
with geoff hinton but really received
has been published on received

1250
00:32:21,409 --> 00:32:21,419
has been published on received
 

1251
00:32:21,419 --> 00:32:24,199
has been published on received
significant attention 2017 that i won't

1252
00:32:24,199 --> 00:32:24,209
significant attention 2017 that i won't
 

1253
00:32:24,209 --> 00:32:28,669
significant attention 2017 that i won't
go into detail here we are going to

1254
00:32:28,669 --> 00:32:28,679
go into detail here we are going to
 

1255
00:32:28,679 --> 00:32:31,939
go into detail here we are going to
release an online-only

1256
00:32:31,939 --> 00:32:31,949
release an online-only
 

1257
00:32:31,949 --> 00:32:35,479
release an online-only
video about capsule networks it's a

1258
00:32:35,479 --> 00:32:35,489
video about capsule networks it's a
 

1259
00:32:35,489 --> 00:32:37,069
video about capsule networks it's a
little bit too technical but they

1260
00:32:37,069 --> 00:32:37,079
little bit too technical but they
 

1261
00:32:37,079 --> 00:32:41,749
little bit too technical but they
inspire a very important point that we

1262
00:32:41,749 --> 00:32:41,759
inspire a very important point that we
 

1263
00:32:41,759 --> 00:32:43,009
inspire a very important point that we
should always think about with deep

1264
00:32:43,009 --> 00:32:43,019
should always think about with deep
 

1265
00:32:43,019 --> 00:32:46,129
should always think about with deep
learning whenever it's successful is to

1266
00:32:46,129 --> 00:32:46,139
learning whenever it's successful is to
 

1267
00:32:46,139 --> 00:32:48,889
learning whenever it's successful is to
think about what as I mentioned with the

1268
00:32:48,889 --> 00:32:48,899
think about what as I mentioned with the
 

1269
00:32:48,899 --> 00:32:52,159
think about what as I mentioned with the
cat eating a banana on a philosophical

1270
00:32:52,159 --> 00:32:52,169
cat eating a banana on a philosophical
 

1271
00:32:52,169 --> 00:32:54,229
cat eating a banana on a philosophical
and the mathematical level you have to

1272
00:32:54,229 --> 00:32:54,239
and the mathematical level you have to
 

1273
00:32:54,239 --> 00:32:57,709
and the mathematical level you have to
consider what assumptions these networks

1274
00:32:57,709 --> 00:32:57,719
consider what assumptions these networks
 

1275
00:32:57,719 --> 00:33:00,919
consider what assumptions these networks
make and what through those assumptions

1276
00:33:00,919 --> 00:33:00,929
make and what through those assumptions
 

1277
00:33:00,929 --> 00:33:03,889
make and what through those assumptions
they throw away so neural networks due

1278
00:33:03,889 --> 00:33:03,899
they throw away so neural networks due
 

1279
00:33:03,899 --> 00:33:05,689
they throw away so neural networks due
to the spatial with convolutional neural

1280
00:33:05,689 --> 00:33:05,699
to the spatial with convolutional neural
 

1281
00:33:05,699 --> 00:33:08,229
to the spatial with convolutional neural
networks due to their spatial invariants

1282
00:33:08,229 --> 00:33:08,239
networks due to their spatial invariants
 

1283
00:33:08,239 --> 00:33:10,489
networks due to their spatial invariants
throw away information about the

1284
00:33:10,489 --> 00:33:10,499
throw away information about the
 

1285
00:33:10,499 --> 00:33:15,619
throw away information about the
relationship between the the hierarchies

1286
00:33:15,619 --> 00:33:15,629
relationship between the the hierarchies
 

1287
00:33:15,629 --> 00:33:17,179
relationship between the the hierarchies
between the simple and the complex

1288
00:33:17,179 --> 00:33:17,189
between the simple and the complex
 

1289
00:33:17,189 --> 00:33:19,369
between the simple and the complex
objects so the face on the left and the

1290
00:33:19,369 --> 00:33:19,379
objects so the face on the left and the
 

1291
00:33:19,379 --> 00:33:21,949
objects so the face on the left and the
face on the right looks the same to

1292
00:33:21,949 --> 00:33:21,959
face on the right looks the same to
 

1293
00:33:21,959 --> 00:33:24,199
face on the right looks the same to
accomplish a neural network the presence

1294
00:33:24,199 --> 00:33:24,209
accomplish a neural network the presence
 

1295
00:33:24,209 --> 00:33:27,919
accomplish a neural network the presence
of eyes and nose and mouth is the

1296
00:33:27,919 --> 00:33:27,929
of eyes and nose and mouth is the
 

1297
00:33:27,929 --> 00:33:32,010
of eyes and nose and mouth is the
central aspect of what makes

1298
00:33:32,010 --> 00:33:32,020
central aspect of what makes
 

1299
00:33:32,020 --> 00:33:33,750
central aspect of what makes
classification tasks work for

1300
00:33:33,750 --> 00:33:33,760
classification tasks work for
 

1301
00:33:33,760 --> 00:33:36,300
classification tasks work for
convolution Network where it will fire

1302
00:33:36,300 --> 00:33:36,310
convolution Network where it will fire
 

1303
00:33:36,310 --> 00:33:39,360
convolution Network where it will fire
and say this is definitely a face but

1304
00:33:39,360 --> 00:33:39,370
and say this is definitely a face but
 

1305
00:33:39,370 --> 00:33:42,510
and say this is definitely a face but
the spatial relationship is lost is

1306
00:33:42,510 --> 00:33:42,520
the spatial relationship is lost is
 

1307
00:33:42,520 --> 00:33:43,350
the spatial relationship is lost is
ignored

1308
00:33:43,350 --> 00:33:43,360
ignored
 

1309
00:33:43,360 --> 00:33:45,810
ignored
which means there's a lot of

1310
00:33:45,810 --> 00:33:45,820
which means there's a lot of
 

1311
00:33:45,820 --> 00:33:49,380
which means there's a lot of
implications to this but for things like

1312
00:33:49,380 --> 00:33:49,390
implications to this but for things like
 

1313
00:33:49,390 --> 00:33:53,180
implications to this but for things like
pose variation that information is lost

1314
00:33:53,180 --> 00:33:53,190
pose variation that information is lost
 

1315
00:33:53,190 --> 00:33:55,860
pose variation that information is lost
we're throwing away that away completely

1316
00:33:55,860 --> 00:33:55,870
we're throwing away that away completely
 

1317
00:33:55,870 --> 00:33:59,010
we're throwing away that away completely
and hoping that the pooling operation

1318
00:33:59,010 --> 00:33:59,020
and hoping that the pooling operation
 

1319
00:33:59,020 --> 00:34:02,310
and hoping that the pooling operation
that's performing these networks is able

1320
00:34:02,310 --> 00:34:02,320
that's performing these networks is able
 

1321
00:34:02,320 --> 00:34:04,680
that's performing these networks is able
to sort of mesh everything together to

1322
00:34:04,680 --> 00:34:04,690
to sort of mesh everything together to
 

1323
00:34:04,690 --> 00:34:06,600
to sort of mesh everything together to
come up with the features that are

1324
00:34:06,600 --> 00:34:06,610
come up with the features that are
 

1325
00:34:06,610 --> 00:34:08,760
come up with the features that are
firing of the different parts of the

1326
00:34:08,760 --> 00:34:08,770
firing of the different parts of the
 

1327
00:34:08,770 --> 00:34:10,050
firing of the different parts of the
face that then come up with the total

1328
00:34:10,050 --> 00:34:10,060
face that then come up with the total
 

1329
00:34:10,060 --> 00:34:12,139
face that then come up with the total
classification that it's a face without

1330
00:34:12,139 --> 00:34:12,149
classification that it's a face without
 

1331
00:34:12,149 --> 00:34:14,220
classification that it's a face without
representing really the relationship

1332
00:34:14,220 --> 00:34:14,230
representing really the relationship
 

1333
00:34:14,230 --> 00:34:16,290
representing really the relationship
between these features at the low level

1334
00:34:16,290 --> 00:34:16,300
between these features at the low level
 

1335
00:34:16,300 --> 00:34:19,560
between these features at the low level
and and the high level at the low level

1336
00:34:19,560 --> 00:34:19,570
and and the high level at the low level
 

1337
00:34:19,570 --> 00:34:21,240
and and the high level at the low level
of the hierarchy at the simple and the

1338
00:34:21,240 --> 00:34:21,250
of the hierarchy at the simple and the
 

1339
00:34:21,250 --> 00:34:24,870
of the hierarchy at the simple and the
complex level this is a super exciting

1340
00:34:24,870 --> 00:34:24,880
complex level this is a super exciting
 

1341
00:34:24,880 --> 00:34:27,260
complex level this is a super exciting
field now that's hopefully will spark

1342
00:34:27,260 --> 00:34:27,270
field now that's hopefully will spark
 

1343
00:34:27,270 --> 00:34:29,310
field now that's hopefully will spark
developments of how we design your own

1344
00:34:29,310 --> 00:34:29,320
developments of how we design your own
 

1345
00:34:29,320 --> 00:34:32,750
developments of how we design your own
networks that are able to learn this the

1346
00:34:32,750 --> 00:34:32,760
networks that are able to learn this the
 

1347
00:34:32,760 --> 00:34:37,380
networks that are able to learn this the
rotational the orientation invariance as

1348
00:34:37,380 --> 00:34:37,390
rotational the orientation invariance as
 

1349
00:34:37,390 --> 00:34:44,340
rotational the orientation invariance as
well ok so as I mentioned you take these

1350
00:34:44,340 --> 00:34:44,350
well ok so as I mentioned you take these
 

1351
00:34:44,350 --> 00:34:46,560
well ok so as I mentioned you take these
combos in your networks chop off the

1352
00:34:46,560 --> 00:34:46,570
combos in your networks chop off the
 

1353
00:34:46,570 --> 00:34:49,409
combos in your networks chop off the
final layer in order to apply to a

1354
00:34:49,409 --> 00:34:49,419
final layer in order to apply to a
 

1355
00:34:49,419 --> 00:34:52,290
final layer in order to apply to a
particular domain and that is what we'll

1356
00:34:52,290 --> 00:34:52,300
particular domain and that is what we'll
 

1357
00:34:52,300 --> 00:34:53,700
particular domain and that is what we'll
do with fully convolutional neural

1358
00:34:53,700 --> 00:34:53,710
do with fully convolutional neural
 

1359
00:34:53,710 --> 00:34:55,800
do with fully convolutional neural
networks the ones that we task to

1360
00:34:55,800 --> 00:34:55,810
networks the ones that we task to
 

1361
00:34:55,810 --> 00:35:01,190
networks the ones that we task to
segment the image at a pixel level as a

1362
00:35:01,190 --> 00:35:01,200
segment the image at a pixel level as a
 

1363
00:35:01,200 --> 00:35:03,780
segment the image at a pixel level as a
reminder these networks through the

1364
00:35:03,780 --> 00:35:03,790
reminder these networks through the
 

1365
00:35:03,790 --> 00:35:07,770
reminder these networks through the
convolutional process are really

1366
00:35:07,770 --> 00:35:07,780
convolutional process are really
 

1367
00:35:07,780 --> 00:35:11,910
convolutional process are really
producing a heat map different parts of

1368
00:35:11,910 --> 00:35:11,920
producing a heat map different parts of
 

1369
00:35:11,920 --> 00:35:13,530
producing a heat map different parts of
the network are getting excited based on

1370
00:35:13,530 --> 00:35:13,540
the network are getting excited based on
 

1371
00:35:13,540 --> 00:35:15,630
the network are getting excited based on
the different aspects of the image and

1372
00:35:15,630 --> 00:35:15,640
the different aspects of the image and
 

1373
00:35:15,640 --> 00:35:17,790
the different aspects of the image and
so it can be used to do the localization

1374
00:35:17,790 --> 00:35:17,800
so it can be used to do the localization
 

1375
00:35:17,800 --> 00:35:19,770
so it can be used to do the localization
of detecting not just classifying the

1376
00:35:19,770 --> 00:35:19,780
of detecting not just classifying the
 

1377
00:35:19,780 --> 00:35:23,040
of detecting not just classifying the
image but localizing the object and they

1378
00:35:23,040 --> 00:35:23,050
image but localizing the object and they
 

1379
00:35:23,050 --> 00:35:27,060
image but localizing the object and they
could do so at a pixel level so the

1380
00:35:27,060 --> 00:35:27,070
could do so at a pixel level so the
 

1381
00:35:27,070 --> 00:35:29,960
could do so at a pixel level so the
convolutional layers are doing the

1382
00:35:29,960 --> 00:35:29,970
convolutional layers are doing the
 

1383
00:35:29,970 --> 00:35:32,820
convolutional layers are doing the
encoding process they're taking the rich

1384
00:35:32,820 --> 00:35:32,830
encoding process they're taking the rich
 

1385
00:35:32,830 --> 00:35:36,660
encoding process they're taking the rich
raw sensory information in the image and

1386
00:35:36,660 --> 00:35:36,670
raw sensory information in the image and
 

1387
00:35:36,670 --> 00:35:39,000
raw sensory information in the image and
encoding them into an interpretable set

1388
00:35:39,000 --> 00:35:39,010
encoding them into an interpretable set
 

1389
00:35:39,010 --> 00:35:42,090
encoding them into an interpretable set
of features representation that can then

1390
00:35:42,090 --> 00:35:42,100
of features representation that can then
 

1391
00:35:42,100 --> 00:35:44,250
of features representation that can then
be used for classification but we can

1392
00:35:44,250 --> 00:35:44,260
be used for classification but we can
 

1393
00:35:44,260 --> 00:35:45,400
be used for classification but we can
also then use it

1394
00:35:45,400 --> 00:35:45,410
also then use it
 

1395
00:35:45,410 --> 00:35:48,010
also then use it
kotor up sample that information and

1396
00:35:48,010 --> 00:35:48,020
kotor up sample that information and
 

1397
00:35:48,020 --> 00:35:51,430
kotor up sample that information and
produce a map like this fully

1398
00:35:51,430 --> 00:35:51,440
produce a map like this fully
 

1399
00:35:51,440 --> 00:35:52,720
produce a map like this fully
convolutional neural network

1400
00:35:52,720 --> 00:35:52,730
convolutional neural network
 

1401
00:35:52,730 --> 00:35:55,480
convolutional neural network
segmentation semantic scene segmentation

1402
00:35:55,480 --> 00:35:55,490
segmentation semantic scene segmentation
 

1403
00:35:55,490 --> 00:35:58,180
segmentation semantic scene segmentation
image segmentation the goal is to as

1404
00:35:58,180 --> 00:35:58,190
image segmentation the goal is to as
 

1405
00:35:58,190 --> 00:36:00,430
image segmentation the goal is to as
opposed to classify the entire image you

1406
00:36:00,430 --> 00:36:00,440
opposed to classify the entire image you
 

1407
00:36:00,440 --> 00:36:03,130
opposed to classify the entire image you
classify every single pixel its pixel

1408
00:36:03,130 --> 00:36:03,140
classify every single pixel its pixel
 

1409
00:36:03,140 --> 00:36:05,260
classify every single pixel its pixel
level segmentation you color every

1410
00:36:05,260 --> 00:36:05,270
level segmentation you color every
 

1411
00:36:05,270 --> 00:36:08,290
level segmentation you color every
single pixel with what that pixel what

1412
00:36:08,290 --> 00:36:08,300
single pixel with what that pixel what
 

1413
00:36:08,300 --> 00:36:10,330
single pixel with what that pixel what
object that pixel belongs to in this 2d

1414
00:36:10,330 --> 00:36:10,340
object that pixel belongs to in this 2d
 

1415
00:36:10,340 --> 00:36:15,030
object that pixel belongs to in this 2d
space of the image the 2d projection the

1416
00:36:15,030 --> 00:36:15,040
space of the image the 2d projection the
 

1417
00:36:15,040 --> 00:36:19,330
space of the image the 2d projection the
in the image of a 3-dimensional world so

1418
00:36:19,330 --> 00:36:19,340
in the image of a 3-dimensional world so
 

1419
00:36:19,340 --> 00:36:21,280
in the image of a 3-dimensional world so
the thing is there's been a lot of

1420
00:36:21,280 --> 00:36:21,290
the thing is there's been a lot of
 

1421
00:36:21,290 --> 00:36:27,160
the thing is there's been a lot of
advancement in the last three years but

1422
00:36:27,160 --> 00:36:27,170
advancement in the last three years but
 

1423
00:36:27,170 --> 00:36:28,840
advancement in the last three years but
it's still an incredibly difficult

1424
00:36:28,840 --> 00:36:28,850
it's still an incredibly difficult
 

1425
00:36:28,850 --> 00:36:31,840
it's still an incredibly difficult
problem if you if you think if you think

1426
00:36:31,840 --> 00:36:31,850
problem if you if you think if you think
 

1427
00:36:31,850 --> 00:36:36,700
problem if you if you think if you think
about the amount of data that's used for

1428
00:36:36,700 --> 00:36:36,710
about the amount of data that's used for
 

1429
00:36:36,710 --> 00:36:40,330
about the amount of data that's used for
training and the task of pixel level of

1430
00:36:40,330 --> 00:36:40,340
training and the task of pixel level of
 

1431
00:36:40,340 --> 00:36:43,780
training and the task of pixel level of
megapixels here of millions of pixels

1432
00:36:43,780 --> 00:36:43,790
megapixels here of millions of pixels
 

1433
00:36:43,790 --> 00:36:46,120
megapixels here of millions of pixels
that are tasked with having a scientist

1434
00:36:46,120 --> 00:36:46,130
that are tasked with having a scientist
 

1435
00:36:46,130 --> 00:36:47,740
that are tasked with having a scientist
single label it's an extremely difficult

1436
00:36:47,740 --> 00:36:47,750
single label it's an extremely difficult
 

1437
00:36:47,750 --> 00:36:52,570
single label it's an extremely difficult
problem why is this interesting

1438
00:36:52,570 --> 00:36:52,580
problem why is this interesting
 

1439
00:36:52,580 --> 00:36:54,640
problem why is this interesting
important problem to try to solve as

1440
00:36:54,640 --> 00:36:54,650
important problem to try to solve as
 

1441
00:36:54,650 --> 00:36:57,480
important problem to try to solve as
opposed to bounding boxes around cats

1442
00:36:57,480 --> 00:36:57,490
opposed to bounding boxes around cats
 

1443
00:36:57,490 --> 00:37:00,910
opposed to bounding boxes around cats
well it's whenever precise boundaries of

1444
00:37:00,910 --> 00:37:00,920
well it's whenever precise boundaries of
 

1445
00:37:00,920 --> 00:37:03,250
well it's whenever precise boundaries of
objects are important certainly medical

1446
00:37:03,250 --> 00:37:03,260
objects are important certainly medical
 

1447
00:37:03,260 --> 00:37:05,680
objects are important certainly medical
applications when looking at imaging and

1448
00:37:05,680 --> 00:37:05,690
applications when looking at imaging and
 

1449
00:37:05,690 --> 00:37:07,960
applications when looking at imaging and
detecting in particular for example

1450
00:37:07,960 --> 00:37:07,970
detecting in particular for example
 

1451
00:37:07,970 --> 00:37:13,030
detecting in particular for example
detecting tumors in the in in medical

1452
00:37:13,030 --> 00:37:13,040
detecting tumors in the in in medical
 

1453
00:37:13,040 --> 00:37:15,640
detecting tumors in the in in medical
imaging of different different organs

1454
00:37:15,640 --> 00:37:15,650
imaging of different different organs
 

1455
00:37:15,650 --> 00:37:22,090
imaging of different different organs
and in driving in robotics when objects

1456
00:37:22,090 --> 00:37:22,100
and in driving in robotics when objects
 

1457
00:37:22,100 --> 00:37:24,070
and in driving in robotics when objects
are involved it's a done scene of all

1458
00:37:24,070 --> 00:37:24,080
are involved it's a done scene of all
 

1459
00:37:24,080 --> 00:37:26,080
are involved it's a done scene of all
those vehicles pedestrians cyclists we

1460
00:37:26,080 --> 00:37:26,090
those vehicles pedestrians cyclists we
 

1461
00:37:26,090 --> 00:37:29,290
those vehicles pedestrians cyclists we
need to be able to not just have a loose

1462
00:37:29,290 --> 00:37:29,300
need to be able to not just have a loose
 

1463
00:37:29,300 --> 00:37:31,540
need to be able to not just have a loose
estimate of where objects are we need to

1464
00:37:31,540 --> 00:37:31,550
estimate of where objects are we need to
 

1465
00:37:31,550 --> 00:37:33,880
estimate of where objects are we need to
be able to have the exact boundaries and

1466
00:37:33,880 --> 00:37:33,890
be able to have the exact boundaries and
 

1467
00:37:33,890 --> 00:37:37,240
be able to have the exact boundaries and
then potentially through data fusion

1468
00:37:37,240 --> 00:37:37,250
then potentially through data fusion
 

1469
00:37:37,250 --> 00:37:39,400
then potentially through data fusion
fusing sensors together

1470
00:37:39,400 --> 00:37:39,410
fusing sensors together
 

1471
00:37:39,410 --> 00:37:41,920
fusing sensors together
fusing this rich textural information

1472
00:37:41,920 --> 00:37:41,930
fusing this rich textural information
 

1473
00:37:41,930 --> 00:37:44,170
fusing this rich textural information
about pedestrians cyclists and vehicles

1474
00:37:44,170 --> 00:37:44,180
about pedestrians cyclists and vehicles
 

1475
00:37:44,180 --> 00:37:46,120
about pedestrians cyclists and vehicles
to lidar data that's providing us the

1476
00:37:46,120 --> 00:37:46,130
to lidar data that's providing us the
 

1477
00:37:46,130 --> 00:37:48,730
to lidar data that's providing us the
three-dimensional map of the world or

1478
00:37:48,730 --> 00:37:48,740
three-dimensional map of the world or
 

1479
00:37:48,740 --> 00:37:50,740
three-dimensional map of the world or
have both the semantic meaning of the

1480
00:37:50,740 --> 00:37:50,750
have both the semantic meaning of the
 

1481
00:37:50,750 --> 00:37:52,360
have both the semantic meaning of the
different objects and their exact

1482
00:37:52,360 --> 00:37:52,370
different objects and their exact
 

1483
00:37:52,370 --> 00:37:58,880
different objects and their exact
three-dimensional location

1484
00:37:58,880 --> 00:37:58,890

 

1485
00:37:58,890 --> 00:38:03,870

a lot of this work successfully a lot of

1486
00:38:03,870 --> 00:38:03,880
a lot of this work successfully a lot of
 

1487
00:38:03,880 --> 00:38:05,340
a lot of this work successfully a lot of
the work in the semantic segmentation

1488
00:38:05,340 --> 00:38:05,350
the work in the semantic segmentation
 

1489
00:38:05,350 --> 00:38:07,590
the work in the semantic segmentation
started with fully convolutional

1490
00:38:07,590 --> 00:38:07,600
started with fully convolutional
 

1491
00:38:07,600 --> 00:38:10,250
started with fully convolutional
networks for semantic segmentation paper

1492
00:38:10,250 --> 00:38:10,260
networks for semantic segmentation paper
 

1493
00:38:10,260 --> 00:38:12,660
networks for semantic segmentation paper
FCN that's where the name of FCN came

1494
00:38:12,660 --> 00:38:12,670
FCN that's where the name of FCN came
 

1495
00:38:12,670 --> 00:38:14,310
FCN that's where the name of FCN came
from in november 2014

1496
00:38:14,310 --> 00:38:14,320
from in november 2014
 

1497
00:38:14,320 --> 00:38:17,580
from in november 2014
now go through a few papers here to give

1498
00:38:17,580 --> 00:38:17,590
now go through a few papers here to give
 

1499
00:38:17,590 --> 00:38:19,080
now go through a few papers here to give
you some intuition where the field is

1500
00:38:19,080 --> 00:38:19,090
you some intuition where the field is
 

1501
00:38:19,090 --> 00:38:23,040
you some intuition where the field is
gone and how that takes us to seg fuse

1502
00:38:23,040 --> 00:38:23,050
gone and how that takes us to seg fuse
 

1503
00:38:23,050 --> 00:38:27,680
gone and how that takes us to seg fuse
the segmentation competition so FCM

1504
00:38:27,680 --> 00:38:27,690
the segmentation competition so FCM
 

1505
00:38:27,690 --> 00:38:29,640
the segmentation competition so FCM
repurposed the image net pre-trained

1506
00:38:29,640 --> 00:38:29,650
repurposed the image net pre-trained
 

1507
00:38:29,650 --> 00:38:31,620
repurposed the image net pre-trained
nets the nets that were trained to

1508
00:38:31,620 --> 00:38:31,630
nets the nets that were trained to
 

1509
00:38:31,630 --> 00:38:34,080
nets the nets that were trained to
classify what's in an image the entire

1510
00:38:34,080 --> 00:38:34,090
classify what's in an image the entire
 

1511
00:38:34,090 --> 00:38:37,710
classify what's in an image the entire
image and chopped off the fully

1512
00:38:37,710 --> 00:38:37,720
image and chopped off the fully
 

1513
00:38:37,720 --> 00:38:41,430
image and chopped off the fully
connected layers and then added decoder

1514
00:38:41,430 --> 00:38:41,440
connected layers and then added decoder
 

1515
00:38:41,440 --> 00:38:43,950
connected layers and then added decoder
parts that that up sample there the

1516
00:38:43,950 --> 00:38:43,960
parts that that up sample there the
 

1517
00:38:43,960 --> 00:38:49,410
parts that that up sample there the
image to produce a heat map here shown

1518
00:38:49,410 --> 00:38:49,420
image to produce a heat map here shown
 

1519
00:38:49,420 --> 00:38:52,470
image to produce a heat map here shown
with a tabby cat a heat map of where the

1520
00:38:52,470 --> 00:38:52,480
with a tabby cat a heat map of where the
 

1521
00:38:52,480 --> 00:38:55,620
with a tabby cat a heat map of where the
cat is in the image it's a much slower

1522
00:38:55,620 --> 00:38:55,630
cat is in the image it's a much slower
 

1523
00:38:55,630 --> 00:38:58,080
cat is in the image it's a much slower
much coarser resolution than the input

1524
00:38:58,080 --> 00:38:58,090
much coarser resolution than the input
 

1525
00:38:58,090 --> 00:39:02,510
much coarser resolution than the input
image 1/8 at best

1526
00:39:02,510 --> 00:39:02,520
image 1/8 at best
 

1527
00:39:02,520 --> 00:39:05,400
image 1/8 at best
skip connections to improve coarseness

1528
00:39:05,400 --> 00:39:05,410
skip connections to improve coarseness
 

1529
00:39:05,410 --> 00:39:10,800
skip connections to improve coarseness
of up sampling there's a few tricks if

1530
00:39:10,800 --> 00:39:10,810
of up sampling there's a few tricks if
 

1531
00:39:10,810 --> 00:39:12,540
of up sampling there's a few tricks if
you do the most naive approach the up

1532
00:39:12,540 --> 00:39:12,550
you do the most naive approach the up
 

1533
00:39:12,550 --> 00:39:14,490
you do the most naive approach the up
sampling is going to be extremely coarse

1534
00:39:14,490 --> 00:39:14,500
sampling is going to be extremely coarse
 

1535
00:39:14,500 --> 00:39:16,650
sampling is going to be extremely coarse
because that's the whole point of the

1536
00:39:16,650 --> 00:39:16,660
because that's the whole point of the
 

1537
00:39:16,660 --> 00:39:19,530
because that's the whole point of the
neural network the encoding part is you

1538
00:39:19,530 --> 00:39:19,540
neural network the encoding part is you
 

1539
00:39:19,540 --> 00:39:22,920
neural network the encoding part is you
throw away all the useless data the

1540
00:39:22,920 --> 00:39:22,930
throw away all the useless data the
 

1541
00:39:22,930 --> 00:39:25,260
throw away all the useless data the
YouTube the most essential aspects that

1542
00:39:25,260 --> 00:39:25,270
YouTube the most essential aspects that
 

1543
00:39:25,270 --> 00:39:27,120
YouTube the most essential aspects that
represent that image so you're throwing

1544
00:39:27,120 --> 00:39:27,130
represent that image so you're throwing
 

1545
00:39:27,130 --> 00:39:28,380
represent that image so you're throwing
away a lot of information that's

1546
00:39:28,380 --> 00:39:28,390
away a lot of information that's
 

1547
00:39:28,390 --> 00:39:31,380
away a lot of information that's
necessary to then form a high resolution

1548
00:39:31,380 --> 00:39:31,390
necessary to then form a high resolution
 

1549
00:39:31,390 --> 00:39:35,490
necessary to then form a high resolution
image so there's a few tricks where you

1550
00:39:35,490 --> 00:39:35,500
image so there's a few tricks where you
 

1551
00:39:35,500 --> 00:39:39,030
image so there's a few tricks where you
skip a few of the final pooling

1552
00:39:39,030 --> 00:39:39,040
skip a few of the final pooling
 

1553
00:39:39,040 --> 00:39:42,060
skip a few of the final pooling
operations to go in similar way and this

1554
00:39:42,060 --> 00:39:42,070
operations to go in similar way and this
 

1555
00:39:42,070 --> 00:39:45,270
operations to go in similar way and this
is a residual block to go to go to the

1556
00:39:45,270 --> 00:39:45,280
is a residual block to go to go to the
 

1557
00:39:45,280 --> 00:39:46,530
is a residual block to go to go to the
output produce higher and higher

1558
00:39:46,530 --> 00:39:46,540
output produce higher and higher
 

1559
00:39:46,540 --> 00:39:51,360
output produce higher and higher
resolution heat map at the end segment

1560
00:39:51,360 --> 00:39:51,370
resolution heat map at the end segment
 

1561
00:39:51,370 --> 00:39:55,350
resolution heat map at the end segment
in 2015 applied this to the driving

1562
00:39:55,350 --> 00:39:55,360
in 2015 applied this to the driving
 

1563
00:39:55,360 --> 00:39:58,170
in 2015 applied this to the driving
context and really taking it to kitty

1564
00:39:58,170 --> 00:39:58,180
context and really taking it to kitty
 

1565
00:39:58,180 --> 00:40:01,980
context and really taking it to kitty
data set and have have shown a lot of

1566
00:40:01,980 --> 00:40:01,990
data set and have have shown a lot of
 

1567
00:40:01,990 --> 00:40:03,750
data set and have have shown a lot of
interesting results and really explored

1568
00:40:03,750 --> 00:40:03,760
interesting results and really explored
 

1569
00:40:03,760 --> 00:40:06,270
interesting results and really explored
the encoder decoder or formulation of

1570
00:40:06,270 --> 00:40:06,280
the encoder decoder or formulation of
 

1571
00:40:06,280 --> 00:40:08,990
the encoder decoder or formulation of
the problem

1572
00:40:08,990 --> 00:40:09,000

 

1573
00:40:09,000 --> 00:40:12,450

really solidifying this the place of the

1574
00:40:12,450 --> 00:40:12,460
really solidifying this the place of the
 

1575
00:40:12,460 --> 00:40:14,190
really solidifying this the place of the
encoder/decoder framework for the

1576
00:40:14,190 --> 00:40:14,200
encoder/decoder framework for the
 

1577
00:40:14,200 --> 00:40:18,480
encoder/decoder framework for the
segmentation task dilated convolution

1578
00:40:18,480 --> 00:40:18,490
segmentation task dilated convolution
 

1579
00:40:18,490 --> 00:40:20,670
segmentation task dilated convolution
I'm taking you through a few components

1580
00:40:20,670 --> 00:40:20,680
I'm taking you through a few components
 

1581
00:40:20,680 --> 00:40:22,320
I'm taking you through a few components
which are critical here to the state of

1582
00:40:22,320 --> 00:40:22,330
which are critical here to the state of
 

1583
00:40:22,330 --> 00:40:27,180
which are critical here to the state of
the art dilated convolutions so the

1584
00:40:27,180 --> 00:40:27,190
the art dilated convolutions so the
 

1585
00:40:27,190 --> 00:40:30,570
the art dilated convolutions so the
convolution operation as the pooling

1586
00:40:30,570 --> 00:40:30,580
convolution operation as the pooling
 

1587
00:40:30,580 --> 00:40:33,840
convolution operation as the pooling
operation reduces resolution

1588
00:40:33,840 --> 00:40:33,850
operation reduces resolution
 

1589
00:40:33,850 --> 00:40:38,220
operation reduces resolution
significantly and dilated convolution

1590
00:40:38,220 --> 00:40:38,230
significantly and dilated convolution
 

1591
00:40:38,230 --> 00:40:40,530
significantly and dilated convolution
has a certain kind of gritting as

1592
00:40:40,530 --> 00:40:40,540
has a certain kind of gritting as
 

1593
00:40:40,540 --> 00:40:45,260
has a certain kind of gritting as
visualized there that maintains the

1594
00:40:45,260 --> 00:40:45,270
visualized there that maintains the
 

1595
00:40:45,270 --> 00:40:50,340
visualized there that maintains the
local high resolution textures while

1596
00:40:50,340 --> 00:40:50,350
local high resolution textures while
 

1597
00:40:50,350 --> 00:40:54,690
local high resolution textures while
still capturing the spatial window

1598
00:40:54,690 --> 00:40:54,700
still capturing the spatial window
 

1599
00:40:54,700 --> 00:40:55,910
still capturing the spatial window
necessary

1600
00:40:55,910 --> 00:40:55,920
necessary
 

1601
00:40:55,920 --> 00:40:59,400
necessary
it's called dilated convolutional layer

1602
00:40:59,400 --> 00:40:59,410
it's called dilated convolutional layer
 

1603
00:40:59,410 --> 00:41:04,320
it's called dilated convolutional layer
and that's in a 2015 paper proved to be

1604
00:41:04,320 --> 00:41:04,330
and that's in a 2015 paper proved to be
 

1605
00:41:04,330 --> 00:41:07,410
and that's in a 2015 paper proved to be
much better at up sampling a high

1606
00:41:07,410 --> 00:41:07,420
much better at up sampling a high
 

1607
00:41:07,420 --> 00:41:15,210
much better at up sampling a high
resolution image deep lab with a be v1

1608
00:41:15,210 --> 00:41:15,220
resolution image deep lab with a be v1
 

1609
00:41:15,220 --> 00:41:19,710
resolution image deep lab with a be v1
v2 Navi 3 added conditional random

1610
00:41:19,710 --> 00:41:19,720
v2 Navi 3 added conditional random
 

1611
00:41:19,720 --> 00:41:23,030
v2 Navi 3 added conditional random
fields which is the final piece of the

1612
00:41:23,030 --> 00:41:23,040
fields which is the final piece of the
 

1613
00:41:23,040 --> 00:41:25,980
fields which is the final piece of the
of the state-of-the-art puzzle here a

1614
00:41:25,980 --> 00:41:25,990
of the state-of-the-art puzzle here a
 

1615
00:41:25,990 --> 00:41:28,130
of the state-of-the-art puzzle here a
lot of the successful networks today

1616
00:41:28,130 --> 00:41:28,140
lot of the successful networks today
 

1617
00:41:28,140 --> 00:41:33,450
lot of the successful networks today
that do segmentation not all do post

1618
00:41:33,450 --> 00:41:33,460
that do segmentation not all do post
 

1619
00:41:33,460 --> 00:41:37,050
that do segmentation not all do post
process using CRFs conditional random

1620
00:41:37,050 --> 00:41:37,060
process using CRFs conditional random
 

1621
00:41:37,060 --> 00:41:39,630
process using CRFs conditional random
fields and what they do is they smooth

1622
00:41:39,630 --> 00:41:39,640
fields and what they do is they smooth
 

1623
00:41:39,640 --> 00:41:41,970
fields and what they do is they smooth
the segmentation the up sample

1624
00:41:41,970 --> 00:41:41,980
the segmentation the up sample
 

1625
00:41:41,980 --> 00:41:43,980
the segmentation the up sample
segmentation that results from the FCN

1626
00:41:43,980 --> 00:41:43,990
segmentation that results from the FCN
 

1627
00:41:43,990 --> 00:41:46,290
segmentation that results from the FCN
by looking at the underlying image

1628
00:41:46,290 --> 00:41:46,300
by looking at the underlying image
 

1629
00:41:46,300 --> 00:41:52,290
by looking at the underlying image
intensities so that's the key aspects of

1630
00:41:52,290 --> 00:41:52,300
intensities so that's the key aspects of
 

1631
00:41:52,300 --> 00:41:55,020
intensities so that's the key aspects of
the successful approaches today you have

1632
00:41:55,020 --> 00:41:55,030
the successful approaches today you have
 

1633
00:41:55,030 --> 00:41:56,970
the successful approaches today you have
the encoder decoder framework of a fully

1634
00:41:56,970 --> 00:41:56,980
the encoder decoder framework of a fully
 

1635
00:41:56,980 --> 00:41:59,250
the encoder decoder framework of a fully
accomplished in your network it replaces

1636
00:41:59,250 --> 00:41:59,260
accomplished in your network it replaces
 

1637
00:41:59,260 --> 00:42:00,840
accomplished in your network it replaces
the fully connected layers with the

1638
00:42:00,840 --> 00:42:00,850
the fully connected layers with the
 

1639
00:42:00,850 --> 00:42:03,780
the fully connected layers with the
convolutional layers deconvolution

1640
00:42:03,780 --> 00:42:03,790
convolutional layers deconvolution
 

1641
00:42:03,790 --> 00:42:07,410
convolutional layers deconvolution
layers and as the years progress from

1642
00:42:07,410 --> 00:42:07,420
layers and as the years progress from
 

1643
00:42:07,420 --> 00:42:12,420
layers and as the years progress from
2014 to today as usual than underlying

1644
00:42:12,420 --> 00:42:12,430
2014 to today as usual than underlying
 

1645
00:42:12,430 --> 00:42:17,130
2014 to today as usual than underlying
networks from alex net to vgg net and to

1646
00:42:17,130 --> 00:42:17,140
networks from alex net to vgg net and to
 

1647
00:42:17,140 --> 00:42:20,610
networks from alex net to vgg net and to
now ResNet have been one of the big

1648
00:42:20,610 --> 00:42:20,620
now ResNet have been one of the big
 

1649
00:42:20,620 --> 00:42:22,440
now ResNet have been one of the big
reasons for the improvements of these

1650
00:42:22,440 --> 00:42:22,450
reasons for the improvements of these
 

1651
00:42:22,450 --> 00:42:24,380
reasons for the improvements of these
to be able to perform the segmentation

1652
00:42:24,380 --> 00:42:24,390
to be able to perform the segmentation
 

1653
00:42:24,390 --> 00:42:27,450
to be able to perform the segmentation
so naturally they mirrored the imagenet

1654
00:42:27,450 --> 00:42:27,460
so naturally they mirrored the imagenet
 

1655
00:42:27,460 --> 00:42:30,240
so naturally they mirrored the imagenet
challenge performance in adapting these

1656
00:42:30,240 --> 00:42:30,250
challenge performance in adapting these
 

1657
00:42:30,250 --> 00:42:32,280
challenge performance in adapting these
networks so the state-of-the-art uses

1658
00:42:32,280 --> 00:42:32,290
networks so the state-of-the-art uses
 

1659
00:42:32,290 --> 00:42:34,920
networks so the state-of-the-art uses
ResNet or similar networks conditional

1660
00:42:34,920 --> 00:42:34,930
ResNet or similar networks conditional
 

1661
00:42:34,930 --> 00:42:38,490
ResNet or similar networks conditional
random fields for smoothing based on the

1662
00:42:38,490 --> 00:42:38,500
random fields for smoothing based on the
 

1663
00:42:38,500 --> 00:42:42,470
random fields for smoothing based on the
input image intensities and the dilated

1664
00:42:42,470 --> 00:42:42,480
input image intensities and the dilated
 

1665
00:42:42,480 --> 00:42:45,030
input image intensities and the dilated
convolution that maintains the

1666
00:42:45,030 --> 00:42:45,040
convolution that maintains the
 

1667
00:42:45,040 --> 00:42:47,550
convolution that maintains the
computational cost but increases the

1668
00:42:47,550 --> 00:42:47,560
computational cost but increases the
 

1669
00:42:47,560 --> 00:42:50,700
computational cost but increases the
resolution of the up sampling throughout

1670
00:42:50,700 --> 00:42:50,710
resolution of the up sampling throughout
 

1671
00:42:50,710 --> 00:42:54,599
resolution of the up sampling throughout
the intermediate feature Maps and that

1672
00:42:54,599 --> 00:42:54,609
the intermediate feature Maps and that
 

1673
00:42:54,609 --> 00:42:57,780
the intermediate feature Maps and that
takes us to the state of the art that we

1674
00:42:57,780 --> 00:42:57,790
takes us to the state of the art that we
 

1675
00:42:57,790 --> 00:43:03,660
takes us to the state of the art that we
used to produce the images to produce

1676
00:43:03,660 --> 00:43:03,670
used to produce the images to produce
 

1677
00:43:03,670 --> 00:43:07,530
used to produce the images to produce
the images for the competition present

1678
00:43:07,530 --> 00:43:07,540
the images for the competition present
 

1679
00:43:07,540 --> 00:43:09,390
the images for the competition present
that do you see for dance up sampling

1680
00:43:09,390 --> 00:43:09,400
that do you see for dance up sampling
 

1681
00:43:09,400 --> 00:43:12,780
that do you see for dance up sampling
convolution instead of bilinear up

1682
00:43:12,780 --> 00:43:12,790
convolution instead of bilinear up
 

1683
00:43:12,790 --> 00:43:15,630
convolution instead of bilinear up
sampling you make the up sampling learn

1684
00:43:15,630 --> 00:43:15,640
sampling you make the up sampling learn
 

1685
00:43:15,640 --> 00:43:20,089
sampling you make the up sampling learn
about you learn the upscaling filters

1686
00:43:20,089 --> 00:43:20,099
about you learn the upscaling filters
 

1687
00:43:20,099 --> 00:43:23,099
about you learn the upscaling filters
that's on the bottom that's really the

1688
00:43:23,099 --> 00:43:23,109
that's on the bottom that's really the
 

1689
00:43:23,109 --> 00:43:26,640
that's on the bottom that's really the
key part that made it work there should

1690
00:43:26,640 --> 00:43:26,650
key part that made it work there should
 

1691
00:43:26,650 --> 00:43:28,890
key part that made it work there should
be a theme here sometimes the the

1692
00:43:28,890 --> 00:43:28,900
be a theme here sometimes the the
 

1693
00:43:28,900 --> 00:43:31,050
be a theme here sometimes the the
biggest addition they can be done this

1694
00:43:31,050 --> 00:43:31,060
biggest addition they can be done this
 

1695
00:43:31,060 --> 00:43:33,390
biggest addition they can be done this
parameter izing one of the aspects of

1696
00:43:33,390 --> 00:43:33,400
parameter izing one of the aspects of
 

1697
00:43:33,400 --> 00:43:34,920
parameter izing one of the aspects of
the network they've taken for granted

1698
00:43:34,920 --> 00:43:34,930
the network they've taken for granted
 

1699
00:43:34,930 --> 00:43:37,079
the network they've taken for granted
letting the network learn that aspect

1700
00:43:37,079 --> 00:43:37,089
letting the network learn that aspect
 

1701
00:43:37,089 --> 00:43:41,790
letting the network learn that aspect
and the other I'm not sure how important

1702
00:43:41,790 --> 00:43:41,800
and the other I'm not sure how important
 

1703
00:43:41,800 --> 00:43:43,710
and the other I'm not sure how important
it is to the success but it's a it's a

1704
00:43:43,710 --> 00:43:43,720
it is to the success but it's a it's a
 

1705
00:43:43,720 --> 00:43:46,230
it is to the success but it's a it's a
cool little addition is a hybrid dilated

1706
00:43:46,230 --> 00:43:46,240
cool little addition is a hybrid dilated
 

1707
00:43:46,240 --> 00:43:49,200
cool little addition is a hybrid dilated
convolution as I showed that

1708
00:43:49,200 --> 00:43:49,210
convolution as I showed that
 

1709
00:43:49,210 --> 00:43:51,990
convolution as I showed that
visualization where the convolution is

1710
00:43:51,990 --> 00:43:52,000
visualization where the convolution is
 

1711
00:43:52,000 --> 00:43:55,380
visualization where the convolution is
spread apart a little bit in the input

1712
00:43:55,380 --> 00:43:55,390
spread apart a little bit in the input
 

1713
00:43:55,390 --> 00:43:57,809
spread apart a little bit in the input
from the input to the output the steps

1714
00:43:57,809 --> 00:43:57,819
from the input to the output the steps
 

1715
00:43:57,819 --> 00:44:00,930
from the input to the output the steps
of that dilated convolution filter when

1716
00:44:00,930 --> 00:44:00,940
of that dilated convolution filter when
 

1717
00:44:00,940 --> 00:44:02,700
of that dilated convolution filter when
they're changed it produces a smoother

1718
00:44:02,700 --> 00:44:02,710
they're changed it produces a smoother
 

1719
00:44:02,710 --> 00:44:06,380
they're changed it produces a smoother
result because when it's kept the same

1720
00:44:06,380 --> 00:44:06,390
result because when it's kept the same
 

1721
00:44:06,390 --> 00:44:09,569
result because when it's kept the same
there certain input pixels get a lot

1722
00:44:09,569 --> 00:44:09,579
there certain input pixels get a lot
 

1723
00:44:09,579 --> 00:44:12,240
there certain input pixels get a lot
more attention than others so losing

1724
00:44:12,240 --> 00:44:12,250
more attention than others so losing
 

1725
00:44:12,250 --> 00:44:15,450
more attention than others so losing
that favoritism is what's achieved by

1726
00:44:15,450 --> 00:44:15,460
that favoritism is what's achieved by
 

1727
00:44:15,460 --> 00:44:19,430
that favoritism is what's achieved by
using a variable different dilation rate

1728
00:44:19,430 --> 00:44:19,440
using a variable different dilation rate
 

1729
00:44:19,440 --> 00:44:21,809
using a variable different dilation rate
those are the two tricks but really the

1730
00:44:21,809 --> 00:44:21,819
those are the two tricks but really the
 

1731
00:44:21,819 --> 00:44:24,120
those are the two tricks but really the
biggest one is the parameterization of

1732
00:44:24,120 --> 00:44:24,130
biggest one is the parameterization of
 

1733
00:44:24,130 --> 00:44:28,470
biggest one is the parameterization of
the upscaling filters okay so that's

1734
00:44:28,470 --> 00:44:28,480
the upscaling filters okay so that's
 

1735
00:44:28,480 --> 00:44:29,790
the upscaling filters okay so that's
what we're that's what we used to

1736
00:44:29,790 --> 00:44:29,800
what we're that's what we used to
 

1737
00:44:29,800 --> 00:44:31,230
what we're that's what we used to
generate that data and that's what we

1738
00:44:31,230 --> 00:44:31,240
generate that data and that's what we
 

1739
00:44:31,240 --> 00:44:33,300
generate that data and that's what we
provides you the code with if you're

1740
00:44:33,300 --> 00:44:33,310
provides you the code with if you're
 

1741
00:44:33,310 --> 00:44:35,730
provides you the code with if you're
interested in competing in psyche views

1742
00:44:35,730 --> 00:44:35,740
interested in competing in psyche views
 

1743
00:44:35,740 --> 00:44:38,310
interested in competing in psyche views
the other aspect here that everything

1744
00:44:38,310 --> 00:44:38,320
the other aspect here that everything
 

1745
00:44:38,320 --> 00:44:39,359
the other aspect here that everything
we've talked about from the

1746
00:44:39,359 --> 00:44:39,369
we've talked about from the
 

1747
00:44:39,369 --> 00:44:43,260
we've talked about from the
classification to the segmentation to

1748
00:44:43,260 --> 00:44:43,270
classification to the segmentation to
 

1749
00:44:43,270 --> 00:44:46,349
classification to the segmentation to
making sense of images is it there the

1750
00:44:46,349 --> 00:44:46,359
making sense of images is it there the
 

1751
00:44:46,359 --> 00:44:49,859
making sense of images is it there the
information about time the temporal

1752
00:44:49,859 --> 00:44:49,869
information about time the temporal
 

1753
00:44:49,869 --> 00:44:53,570
information about time the temporal
dynamics of the scene is thrown away and

1754
00:44:53,570 --> 00:44:53,580
dynamics of the scene is thrown away and
 

1755
00:44:53,580 --> 00:44:56,070
dynamics of the scene is thrown away and
for the driving context of the robotics

1756
00:44:56,070 --> 00:44:56,080
for the driving context of the robotics
 

1757
00:44:56,080 --> 00:44:58,320
for the driving context of the robotics
contest and what we'd like to do with

1758
00:44:58,320 --> 00:44:58,330
contest and what we'd like to do with
 

1759
00:44:58,330 --> 00:45:00,089
contest and what we'd like to do with
psyche fuse for the segmentation

1760
00:45:00,089 --> 00:45:00,099
psyche fuse for the segmentation
 

1761
00:45:00,099 --> 00:45:02,640
psyche fuse for the segmentation
dynamics scene segmentation context of

1762
00:45:02,640 --> 00:45:02,650
dynamics scene segmentation context of
 

1763
00:45:02,650 --> 00:45:04,680
dynamics scene segmentation context of
when you try to interpret what's going

1764
00:45:04,680 --> 00:45:04,690
when you try to interpret what's going
 

1765
00:45:04,690 --> 00:45:06,630
when you try to interpret what's going
on in the scene over time and use that

1766
00:45:06,630 --> 00:45:06,640
on in the scene over time and use that
 

1767
00:45:06,640 --> 00:45:12,180
on in the scene over time and use that
information time is essential thus the

1768
00:45:12,180 --> 00:45:12,190
information time is essential thus the
 

1769
00:45:12,190 --> 00:45:14,609
information time is essential thus the
movement of pixels is essential through

1770
00:45:14,609 --> 00:45:14,619
movement of pixels is essential through
 

1771
00:45:14,619 --> 00:45:18,510
movement of pixels is essential through
time that that understanding how those

1772
00:45:18,510 --> 00:45:18,520
time that that understanding how those
 

1773
00:45:18,520 --> 00:45:23,490
time that that understanding how those
objects move in a 3d space through the

1774
00:45:23,490 --> 00:45:23,500
objects move in a 3d space through the
 

1775
00:45:23,500 --> 00:45:25,770
objects move in a 3d space through the
2d projection of an image it's

1776
00:45:25,770 --> 00:45:25,780
2d projection of an image it's
 

1777
00:45:25,780 --> 00:45:28,950
2d projection of an image it's
fascinating and us there's a lot of set

1778
00:45:28,950 --> 00:45:28,960
fascinating and us there's a lot of set
 

1779
00:45:28,960 --> 00:45:32,970
fascinating and us there's a lot of set
of open problems there so flow is what's

1780
00:45:32,970 --> 00:45:32,980
of open problems there so flow is what's
 

1781
00:45:32,980 --> 00:45:37,740
of open problems there so flow is what's
very helpful to as a starting point to

1782
00:45:37,740 --> 00:45:37,750
very helpful to as a starting point to
 

1783
00:45:37,750 --> 00:45:39,829
very helpful to as a starting point to
help us understand how these pixels move

1784
00:45:39,829 --> 00:45:39,839
help us understand how these pixels move
 

1785
00:45:39,839 --> 00:45:44,609
help us understand how these pixels move
flow optical flow dense optical flow is

1786
00:45:44,609 --> 00:45:44,619
flow optical flow dense optical flow is
 

1787
00:45:44,619 --> 00:45:49,560
flow optical flow dense optical flow is
the computation that our best of a best

1788
00:45:49,560 --> 00:45:49,570
the computation that our best of a best
 

1789
00:45:49,570 --> 00:45:51,930
the computation that our best of a best
approximation of where each pixel in

1790
00:45:51,930 --> 00:45:51,940
approximation of where each pixel in
 

1791
00:45:51,940 --> 00:45:57,359
approximation of where each pixel in
image one and moved in the in

1792
00:45:57,359 --> 00:45:57,369
image one and moved in the in
 

1793
00:45:57,369 --> 00:46:00,570
image one and moved in the in
temporarily following image after that

1794
00:46:00,570 --> 00:46:00,580
temporarily following image after that
 

1795
00:46:00,580 --> 00:46:03,660
temporarily following image after that
there's two images in 30 frames a second

1796
00:46:03,660 --> 00:46:03,670
there's two images in 30 frames a second
 

1797
00:46:03,670 --> 00:46:06,480
there's two images in 30 frames a second
there's one image at time zero the other

1798
00:46:06,480 --> 00:46:06,490
there's one image at time zero the other
 

1799
00:46:06,490 --> 00:46:09,480
there's one image at time zero the other
is 33.3 milliseconds later and the

1800
00:46:09,480 --> 00:46:09,490
is 33.3 milliseconds later and the
 

1801
00:46:09,490 --> 00:46:11,609
is 33.3 milliseconds later and the
idents optical flow is our best estimate

1802
00:46:11,609 --> 00:46:11,619
idents optical flow is our best estimate
 

1803
00:46:11,619 --> 00:46:14,099
idents optical flow is our best estimate
of how each pixel in the input image

1804
00:46:14,099 --> 00:46:14,109
of how each pixel in the input image
 

1805
00:46:14,109 --> 00:46:17,760
of how each pixel in the input image
moved to in the output image the optical

1806
00:46:17,760 --> 00:46:17,770
moved to in the output image the optical
 

1807
00:46:17,770 --> 00:46:20,099
moved to in the output image the optical
flow for every pixel produces a

1808
00:46:20,099 --> 00:46:20,109
flow for every pixel produces a
 

1809
00:46:20,109 --> 00:46:22,680
flow for every pixel produces a
direction of where we think that pixel

1810
00:46:22,680 --> 00:46:22,690
direction of where we think that pixel
 

1811
00:46:22,690 --> 00:46:24,810
direction of where we think that pixel
moved and the magnitude of how far moved

1812
00:46:24,810 --> 00:46:24,820
moved and the magnitude of how far moved
 

1813
00:46:24,820 --> 00:46:27,900
moved and the magnitude of how far moved
that allows us to take information that

1814
00:46:27,900 --> 00:46:27,910
that allows us to take information that
 

1815
00:46:27,910 --> 00:46:30,050
that allows us to take information that
we detected about the first frame and

1816
00:46:30,050 --> 00:46:30,060
we detected about the first frame and
 

1817
00:46:30,060 --> 00:46:34,170
we detected about the first frame and
try to propagate it forward this is the

1818
00:46:34,170 --> 00:46:34,180
try to propagate it forward this is the
 

1819
00:46:34,180 --> 00:46:37,470
try to propagate it forward this is the
competition it's to try to segment an

1820
00:46:37,470 --> 00:46:37,480
competition it's to try to segment an
 

1821
00:46:37,480 --> 00:46:39,690
competition it's to try to segment an
image and propagate that information

1822
00:46:39,690 --> 00:46:39,700
image and propagate that information
 

1823
00:46:39,700 --> 00:46:46,670
image and propagate that information
forward for manual annotation of a

1824
00:46:46,670 --> 00:46:46,680
forward for manual annotation of a
 

1825
00:46:46,680 --> 00:46:48,770
forward for manual annotation of a
image so this kind of coloring book

1826
00:46:48,770 --> 00:46:48,780
image so this kind of coloring book
 

1827
00:46:48,780 --> 00:46:50,300
image so this kind of coloring book
annotation where you color every single

1828
00:46:50,300 --> 00:46:50,310
annotation where you color every single
 

1829
00:46:50,310 --> 00:46:53,920
annotation where you color every single
pixel in the state-of-the-art dataset

1830
00:46:53,920 --> 00:46:53,930
pixel in the state-of-the-art dataset
 

1831
00:46:53,930 --> 00:46:59,630
pixel in the state-of-the-art dataset
for driving cityscapes that it takes 1.5

1832
00:46:59,630 --> 00:46:59,640
for driving cityscapes that it takes 1.5
 

1833
00:46:59,640 --> 00:47:02,270
for driving cityscapes that it takes 1.5
ninth and 1.5 hours 90 minutes to do

1834
00:47:02,270 --> 00:47:02,280
ninth and 1.5 hours 90 minutes to do
 

1835
00:47:02,280 --> 00:47:04,850
ninth and 1.5 hours 90 minutes to do
that coloring that's 90 minutes per

1836
00:47:04,850 --> 00:47:04,860
that coloring that's 90 minutes per
 

1837
00:47:04,860 --> 00:47:08,600
that coloring that's 90 minutes per
image that's extremely long time that's

1838
00:47:08,600 --> 00:47:08,610
image that's extremely long time that's
 

1839
00:47:08,610 --> 00:47:11,060
image that's extremely long time that's
why there doesn't exist today dataset

1840
00:47:11,060 --> 00:47:11,070
why there doesn't exist today dataset
 

1841
00:47:11,070 --> 00:47:13,460
why there doesn't exist today dataset
and in this class we're going to create

1842
00:47:13,460 --> 00:47:13,470
and in this class we're going to create
 

1843
00:47:13,470 --> 00:47:17,630
and in this class we're going to create
one of segmentation of these images

1844
00:47:17,630 --> 00:47:17,640
one of segmentation of these images
 

1845
00:47:17,640 --> 00:47:22,730
one of segmentation of these images
through time through video so long

1846
00:47:22,730 --> 00:47:22,740
through time through video so long
 

1847
00:47:22,740 --> 00:47:25,850
through time through video so long
videos where every single frame is fully

1848
00:47:25,850 --> 00:47:25,860
videos where every single frame is fully
 

1849
00:47:25,860 --> 00:47:28,400
videos where every single frame is fully
segmented that's still an open problem

1850
00:47:28,400 --> 00:47:28,410
segmented that's still an open problem
 

1851
00:47:28,410 --> 00:47:32,660
segmented that's still an open problem
that we need to solve flows a piece of

1852
00:47:32,660 --> 00:47:32,670
that we need to solve flows a piece of
 

1853
00:47:32,670 --> 00:47:37,370
that we need to solve flows a piece of
that and we also provide you the this

1854
00:47:37,370 --> 00:47:37,380
that and we also provide you the this
 

1855
00:47:37,380 --> 00:47:39,260
that and we also provide you the this
computer state-of-the-art flow using

1856
00:47:39,260 --> 00:47:39,270
computer state-of-the-art flow using
 

1857
00:47:39,270 --> 00:47:46,360
computer state-of-the-art flow using
flow net 2.0 so flow net 1.0 in May 2015

1858
00:47:46,360 --> 00:47:46,370
flow net 2.0 so flow net 1.0 in May 2015
 

1859
00:47:46,370 --> 00:47:50,060
flow net 2.0 so flow net 1.0 in May 2015
used neural networks to learn the

1860
00:47:50,060 --> 00:47:50,070
used neural networks to learn the
 

1861
00:47:50,070 --> 00:47:53,270
used neural networks to learn the
optical flow the dense optical flow and

1862
00:47:53,270 --> 00:47:53,280
optical flow the dense optical flow and
 

1863
00:47:53,280 --> 00:47:55,340
optical flow the dense optical flow and
it did so with two kinds of

1864
00:47:55,340 --> 00:47:55,350
it did so with two kinds of
 

1865
00:47:55,350 --> 00:47:58,130
it did so with two kinds of
architectures flow net s flowing that

1866
00:47:58,130 --> 00:47:58,140
architectures flow net s flowing that
 

1867
00:47:58,140 --> 00:48:00,790
architectures flow net s flowing that
simple and flow net core flow net see

1868
00:48:00,790 --> 00:48:00,800
simple and flow net core flow net see
 

1869
00:48:00,800 --> 00:48:03,980
simple and flow net core flow net see
the simple one is simply taking the two

1870
00:48:03,980 --> 00:48:03,990
the simple one is simply taking the two
 

1871
00:48:03,990 --> 00:48:06,110
the simple one is simply taking the two
images so what's what's the task here

1872
00:48:06,110 --> 00:48:06,120
images so what's what's the task here
 

1873
00:48:06,120 --> 00:48:08,480
images so what's what's the task here
there's two images and you you want to

1874
00:48:08,480 --> 00:48:08,490
there's two images and you you want to
 

1875
00:48:08,490 --> 00:48:10,160
there's two images and you you want to
produce from those two images they

1876
00:48:10,160 --> 00:48:10,170
produce from those two images they
 

1877
00:48:10,170 --> 00:48:12,230
produce from those two images they
follow each other in time thirty-three

1878
00:48:12,230 --> 00:48:12,240
follow each other in time thirty-three
 

1879
00:48:12,240 --> 00:48:15,350
follow each other in time thirty-three
point three milliseconds apart and your

1880
00:48:15,350 --> 00:48:15,360
point three milliseconds apart and your
 

1881
00:48:15,360 --> 00:48:17,420
point three milliseconds apart and your
task is the output to produce the dense

1882
00:48:17,420 --> 00:48:17,430
task is the output to produce the dense
 

1883
00:48:17,430 --> 00:48:19,700
task is the output to produce the dense
optical flow so for the simple

1884
00:48:19,700 --> 00:48:19,710
optical flow so for the simple
 

1885
00:48:19,710 --> 00:48:21,410
optical flow so for the simple
architecture you just stack them

1886
00:48:21,410 --> 00:48:21,420
architecture you just stack them
 

1887
00:48:21,420 --> 00:48:24,500
architecture you just stack them
together each are RGB so it produces a

1888
00:48:24,500 --> 00:48:24,510
together each are RGB so it produces a
 

1889
00:48:24,510 --> 00:48:26,900
together each are RGB so it produces a
six channel input to the network there's

1890
00:48:26,900 --> 00:48:26,910
six channel input to the network there's
 

1891
00:48:26,910 --> 00:48:28,910
six channel input to the network there's
a lot of convolution and finally it's

1892
00:48:28,910 --> 00:48:28,920
a lot of convolution and finally it's
 

1893
00:48:28,920 --> 00:48:31,220
a lot of convolution and finally it's
the the same kind of process as the

1894
00:48:31,220 --> 00:48:31,230
the the same kind of process as the
 

1895
00:48:31,230 --> 00:48:33,440
the the same kind of process as the
fully convolution your networks to

1896
00:48:33,440 --> 00:48:33,450
fully convolution your networks to
 

1897
00:48:33,450 --> 00:48:35,750
fully convolution your networks to
produce the optical flow then there is

1898
00:48:35,750 --> 00:48:35,760
produce the optical flow then there is
 

1899
00:48:35,760 --> 00:48:39,620
produce the optical flow then there is
flow net correlation architecture where

1900
00:48:39,620 --> 00:48:39,630
flow net correlation architecture where
 

1901
00:48:39,630 --> 00:48:42,080
flow net correlation architecture where
you perform some convolution separately

1902
00:48:42,080 --> 00:48:42,090
you perform some convolution separately
 

1903
00:48:42,090 --> 00:48:44,150
you perform some convolution separately
before using a correlation layer to

1904
00:48:44,150 --> 00:48:44,160
before using a correlation layer to
 

1905
00:48:44,160 --> 00:48:49,380
before using a correlation layer to
combine the feature Maps both

1906
00:48:49,380 --> 00:48:49,390
combine the feature Maps both
 

1907
00:48:49,390 --> 00:48:53,700
combine the feature Maps both
effective in different data sets and

1908
00:48:53,700 --> 00:48:53,710
effective in different data sets and
 

1909
00:48:53,710 --> 00:48:56,520
effective in different data sets and
different applications so flow net 2.0

1910
00:48:56,520 --> 00:48:56,530
different applications so flow net 2.0
 

1911
00:48:56,530 --> 00:49:01,530
different applications so flow net 2.0
in December 2016 is one of the

1912
00:49:01,530 --> 00:49:01,540
in December 2016 is one of the
 

1913
00:49:01,540 --> 00:49:04,170
in December 2016 is one of the
state-of-the-art frameworks code bases

1914
00:49:04,170 --> 00:49:04,180
state-of-the-art frameworks code bases
 

1915
00:49:04,180 --> 00:49:06,540
state-of-the-art frameworks code bases
that we used to generate the data all

1916
00:49:06,540 --> 00:49:06,550
that we used to generate the data all
 

1917
00:49:06,550 --> 00:49:09,600
that we used to generate the data all
show combines the flow net Assam flow

1918
00:49:09,600 --> 00:49:09,610
show combines the flow net Assam flow
 

1919
00:49:09,610 --> 00:49:12,810
show combines the flow net Assam flow
net C and improves over the initial flow

1920
00:49:12,810 --> 00:49:12,820
net C and improves over the initial flow
 

1921
00:49:12,820 --> 00:49:15,140
net C and improves over the initial flow
net producing a smoother flow field

1922
00:49:15,140 --> 00:49:15,150
net producing a smoother flow field
 

1923
00:49:15,150 --> 00:49:18,660
net producing a smoother flow field
preserves the fine motion detail along

1924
00:49:18,660 --> 00:49:18,670
preserves the fine motion detail along
 

1925
00:49:18,670 --> 00:49:21,690
preserves the fine motion detail along
the edges of the objects and it runs

1926
00:49:21,690 --> 00:49:21,700
the edges of the objects and it runs
 

1927
00:49:21,700 --> 00:49:24,270
the edges of the objects and it runs
extremely efficiently depending on the

1928
00:49:24,270 --> 00:49:24,280
extremely efficiently depending on the
 

1929
00:49:24,280 --> 00:49:26,280
extremely efficiently depending on the
architecture there's a few variants

1930
00:49:26,280 --> 00:49:26,290
architecture there's a few variants
 

1931
00:49:26,290 --> 00:49:29,370
architecture there's a few variants
either eight to a hundred forty frames a

1932
00:49:29,370 --> 00:49:29,380
either eight to a hundred forty frames a
 

1933
00:49:29,380 --> 00:49:33,270
either eight to a hundred forty frames a
second and the process there is

1934
00:49:33,270 --> 00:49:33,280
second and the process there is
 

1935
00:49:33,280 --> 00:49:35,550
second and the process there is
essentially one that's common across

1936
00:49:35,550 --> 00:49:35,560
essentially one that's common across
 

1937
00:49:35,560 --> 00:49:37,500
essentially one that's common across
various applications deep learning is

1938
00:49:37,500 --> 00:49:37,510
various applications deep learning is
 

1939
00:49:37,510 --> 00:49:40,800
various applications deep learning is
stacking these networks together the

1940
00:49:40,800 --> 00:49:40,810
stacking these networks together the
 

1941
00:49:40,810 --> 00:49:45,990
stacking these networks together the
very interesting aspect here that we're

1942
00:49:45,990 --> 00:49:46,000
very interesting aspect here that we're
 

1943
00:49:46,000 --> 00:49:48,660
very interesting aspect here that we're
still exploring and again applicable in

1944
00:49:48,660 --> 00:49:48,670
still exploring and again applicable in
 

1945
00:49:48,670 --> 00:49:51,150
still exploring and again applicable in
all of deep learning in this case it

1946
00:49:51,150 --> 00:49:51,160
all of deep learning in this case it
 

1947
00:49:51,160 --> 00:49:53,340
all of deep learning in this case it
seemed that there was a strong effect in

1948
00:49:53,340 --> 00:49:53,350
seemed that there was a strong effect in
 

1949
00:49:53,350 --> 00:49:57,000
seemed that there was a strong effect in
taking sparse small multiple data set

1950
00:49:57,000 --> 00:49:57,010
taking sparse small multiple data set
 

1951
00:49:57,010 --> 00:49:58,920
taking sparse small multiple data set
and doing the training the order of

1952
00:49:58,920 --> 00:49:58,930
and doing the training the order of
 

1953
00:49:58,930 --> 00:50:01,140
and doing the training the order of
which those data sets were used for the

1954
00:50:01,140 --> 00:50:01,150
which those data sets were used for the
 

1955
00:50:01,150 --> 00:50:04,170
which those data sets were used for the
training process mattered a lot that's

1956
00:50:04,170 --> 00:50:04,180
training process mattered a lot that's
 

1957
00:50:04,180 --> 00:50:07,040
training process mattered a lot that's
very interesting

1958
00:50:07,040 --> 00:50:07,050

 

1959
00:50:07,050 --> 00:50:11,040

so using flow net 2.0 here's the data

1960
00:50:11,040 --> 00:50:11,050
so using flow net 2.0 here's the data
 

1961
00:50:11,050 --> 00:50:13,140
so using flow net 2.0 here's the data
set we're making available for psych

1962
00:50:13,140 --> 00:50:13,150
set we're making available for psych
 

1963
00:50:13,150 --> 00:50:16,830
set we're making available for psych
fuse the competition cars that mit.edu

1964
00:50:16,830 --> 00:50:16,840
fuse the competition cars that mit.edu
 

1965
00:50:16,840 --> 00:50:19,140
fuse the competition cars that mit.edu
slash psych fuse first the original

1966
00:50:19,140 --> 00:50:19,150
slash psych fuse first the original
 

1967
00:50:19,150 --> 00:50:24,860
slash psych fuse first the original
video us driving in high-definition

1968
00:50:24,860 --> 00:50:24,870
video us driving in high-definition
 

1969
00:50:24,870 --> 00:50:32,910
video us driving in high-definition
1080p and a 8k 360 video original video

1970
00:50:32,910 --> 00:50:32,920
1080p and a 8k 360 video original video
 

1971
00:50:32,920 --> 00:50:37,470
1080p and a 8k 360 video original video
driving around Cambridge

1972
00:50:37,470 --> 00:50:37,480

 

1973
00:50:37,480 --> 00:50:41,580

we're providing the ground truth for a

1974
00:50:41,580 --> 00:50:41,590
we're providing the ground truth for a
 

1975
00:50:41,590 --> 00:50:45,660
we're providing the ground truth for a
training set for that training set for

1976
00:50:45,660 --> 00:50:45,670
training set for that training set for
 

1977
00:50:45,670 --> 00:50:48,000
training set for that training set for
every single frame 30 frames a second

1978
00:50:48,000 --> 00:50:48,010
every single frame 30 frames a second
 

1979
00:50:48,010 --> 00:50:50,160
every single frame 30 frames a second
we're providing the segmentation frame

1980
00:50:50,160 --> 00:50:50,170
we're providing the segmentation frame
 

1981
00:50:50,170 --> 00:50:53,400
we're providing the segmentation frame
to frame to frame segmented on

1982
00:50:53,400 --> 00:50:53,410
to frame to frame segmented on
 

1983
00:50:53,410 --> 00:50:57,330
to frame to frame segmented on
Mechanical Turk we're also providing the

1984
00:50:57,330 --> 00:50:57,340
Mechanical Turk we're also providing the
 

1985
00:50:57,340 --> 00:51:01,680
Mechanical Turk we're also providing the
output of the network that I mentioned

1986
00:51:01,680 --> 00:51:01,690
output of the network that I mentioned
 

1987
00:51:01,690 --> 00:51:02,820
output of the network that I mentioned
the state of their our segmentation

1988
00:51:02,820 --> 00:51:02,830
the state of their our segmentation
 

1989
00:51:02,830 --> 00:51:05,760
the state of their our segmentation
network that's pretty damn close to the

1990
00:51:05,760 --> 00:51:05,770
network that's pretty damn close to the
 

1991
00:51:05,770 --> 00:51:10,800
network that's pretty damn close to the
ground truth but still not and our task

1992
00:51:10,800 --> 00:51:10,810
ground truth but still not and our task
 

1993
00:51:10,810 --> 00:51:15,390
ground truth but still not and our task
is this is the interesting thing is our

1994
00:51:15,390 --> 00:51:15,400
is this is the interesting thing is our
 

1995
00:51:15,400 --> 00:51:17,490
is this is the interesting thing is our
task is to take the output of this

1996
00:51:17,490 --> 00:51:17,500
task is to take the output of this
 

1997
00:51:17,500 --> 00:51:20,430
task is to take the output of this
network well there's two options one is

1998
00:51:20,430 --> 00:51:20,440
network well there's two options one is
 

1999
00:51:20,440 --> 00:51:23,450
network well there's two options one is
to take the output of this network and

2000
00:51:23,450 --> 00:51:23,460
to take the output of this network and
 

2001
00:51:23,460 --> 00:51:27,900
to take the output of this network and
use use other networks to help you

2002
00:51:27,900 --> 00:51:27,910
use use other networks to help you
 

2003
00:51:27,910 --> 00:51:30,570
use use other networks to help you
propagate the information better so what

2004
00:51:30,570 --> 00:51:30,580
propagate the information better so what
 

2005
00:51:30,580 --> 00:51:33,300
propagate the information better so what
this segmentation the output of this

2006
00:51:33,300 --> 00:51:33,310
this segmentation the output of this
 

2007
00:51:33,310 --> 00:51:37,380
this segmentation the output of this
network does is it only takes a frame by

2008
00:51:37,380 --> 00:51:37,390
network does is it only takes a frame by
 

2009
00:51:37,390 --> 00:51:39,450
network does is it only takes a frame by
frame by frame it's not using the

2010
00:51:39,450 --> 00:51:39,460
frame by frame it's not using the
 

2011
00:51:39,460 --> 00:51:41,700
frame by frame it's not using the
temporal information at all so the

2012
00:51:41,700 --> 00:51:41,710
temporal information at all so the
 

2013
00:51:41,710 --> 00:51:43,950
temporal information at all so the
question is can we figure out a way can

2014
00:51:43,950 --> 00:51:43,960
question is can we figure out a way can
 

2015
00:51:43,960 --> 00:51:46,170
question is can we figure out a way can
we figure out tricks to use temporal

2016
00:51:46,170 --> 00:51:46,180
we figure out tricks to use temporal
 

2017
00:51:46,180 --> 00:51:48,930
we figure out tricks to use temporal
information to improve this segmentation

2018
00:51:48,930 --> 00:51:48,940
information to improve this segmentation
 

2019
00:51:48,940 --> 00:51:54,320
information to improve this segmentation
so it looks more like this segmentation

2020
00:51:54,320 --> 00:51:54,330

 

2021
00:51:54,330 --> 00:51:56,010

and we're also

2022
00:51:56,010 --> 00:51:56,020
and we're also
 

2023
00:51:56,020 --> 00:51:57,870
and we're also
providing the optical flow from frame to

2024
00:51:57,870 --> 00:51:57,880
providing the optical flow from frame to
 

2025
00:51:57,880 --> 00:52:00,720
providing the optical flow from frame to
frame to frame so the optical flow based

2026
00:52:00,720 --> 00:52:00,730
frame to frame so the optical flow based
 

2027
00:52:00,730 --> 00:52:03,480
frame to frame so the optical flow based
on flowing at 2.00 of how each of the

2028
00:52:03,480 --> 00:52:03,490
on flowing at 2.00 of how each of the
 

2029
00:52:03,490 --> 00:52:10,560
on flowing at 2.00 of how each of the
pixels moved okay and that forms a seg

2030
00:52:10,560 --> 00:52:10,570
pixels moved okay and that forms a seg
 

2031
00:52:10,570 --> 00:52:14,550
pixels moved okay and that forms a seg
fuse competition 10,000 images and the

2032
00:52:14,550 --> 00:52:14,560
fuse competition 10,000 images and the
 

2033
00:52:14,560 --> 00:52:18,240
fuse competition 10,000 images and the
task is to submit code we have starter

2034
00:52:18,240 --> 00:52:18,250
task is to submit code we have starter
 

2035
00:52:18,250 --> 00:52:23,850
task is to submit code we have starter
code in Python and on github to take in

2036
00:52:23,850 --> 00:52:23,860
code in Python and on github to take in
 

2037
00:52:23,860 --> 00:52:26,460
code in Python and on github to take in
the original video take in for the

2038
00:52:26,460 --> 00:52:26,470
the original video take in for the
 

2039
00:52:26,470 --> 00:52:28,560
the original video take in for the
training set the ground truth the

2040
00:52:28,560 --> 00:52:28,570
training set the ground truth the
 

2041
00:52:28,570 --> 00:52:30,230
training set the ground truth the
segmentation from the state-of-the-art

2042
00:52:30,230 --> 00:52:30,240
segmentation from the state-of-the-art
 

2043
00:52:30,240 --> 00:52:32,700
segmentation from the state-of-the-art
segmentation Network the optical flow

2044
00:52:32,700 --> 00:52:32,710
segmentation Network the optical flow
 

2045
00:52:32,710 --> 00:52:34,910
segmentation Network the optical flow
from the state-of-the-art optical flow

2046
00:52:34,910 --> 00:52:34,920
from the state-of-the-art optical flow
 

2047
00:52:34,920 --> 00:52:38,280
from the state-of-the-art optical flow
Network and taking that together to

2048
00:52:38,280 --> 00:52:38,290
Network and taking that together to
 

2049
00:52:38,290 --> 00:52:40,560
Network and taking that together to
improve the the stuff on the bottom left

2050
00:52:40,560 --> 00:52:40,570
improve the the stuff on the bottom left
 

2051
00:52:40,570 --> 00:52:42,990
improve the the stuff on the bottom left
the segmentation to try to achieve the

2052
00:52:42,990 --> 00:52:43,000
the segmentation to try to achieve the
 

2053
00:52:43,000 --> 00:52:47,400
the segmentation to try to achieve the
ground truth and on the top right okay

2054
00:52:47,400 --> 00:52:47,410
ground truth and on the top right okay
 

2055
00:52:47,410 --> 00:52:50,460
ground truth and on the top right okay
with that I'd like to thank you tomorrow

2056
00:52:50,460 --> 00:52:50,470
with that I'd like to thank you tomorrow
 

2057
00:52:50,470 --> 00:52:56,370
with that I'd like to thank you tomorrow
at 1 p.m. is way mo in Stata 32 one two

2058
00:52:56,370 --> 00:52:56,380
at 1 p.m. is way mo in Stata 32 one two
 

2059
00:52:56,380 --> 00:52:59,550
at 1 p.m. is way mo in Stata 32 one two
three the next lecture next week will be

2060
00:52:59,550 --> 00:52:59,560
three the next lecture next week will be
 

2061
00:52:59,560 --> 00:53:01,080
three the next lecture next week will be
on deep learning for a sense in the

2062
00:53:01,080 --> 00:53:01,090
on deep learning for a sense in the
 

2063
00:53:01,090 --> 00:53:03,660
on deep learning for a sense in the
human understanding the human and we

2064
00:53:03,660 --> 00:53:03,670
human understanding the human and we
 

2065
00:53:03,670 --> 00:53:05,730
human understanding the human and we
will release online only lecture on

2066
00:53:05,730 --> 00:53:05,740
will release online only lecture on
 

2067
00:53:05,740 --> 00:53:08,790
will release online only lecture on
capsule networks and Gans general

2068
00:53:08,790 --> 00:53:08,800
capsule networks and Gans general
 

2069
00:53:08,800 --> 00:53:11,800
capsule networks and Gans general
adversarial networks thank you very much

2070
00:53:11,800 --> 00:53:11,810
adversarial networks thank you very much
 

2071
00:53:11,810 --> 00:53:14,980
adversarial networks thank you very much
[Applause]

