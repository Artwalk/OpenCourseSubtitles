1
00:00:00,000 --> 00:00:08,099
今天我们将深入讨论 

2
00:00:01,770 --> 00:00:11,070
强化学习我们的问题

3
00:00:08,099 --> 00:00:15,599
想探索这是 哪个 

4
00:00:11,070 --> 00:00:19,400
我们可以教导系统采取行动 

5
00:00:15,599 --> 00:00:22,650
从数据中感知并 在这个世界中 行动 

6
00:00:19,400 --> 00:00:25,890
让我们 退一步 想一想 

7
00:00:22,650 --> 00:00:27,570
那么什么是完整的任务 

8
00:00:25,890 --> 00:00:30,720
人工智能系统需要 

9
00:00:27,570 --> 00:00:34,829
完成这里的堆栈 从顶部到 

10
00:00:30,720 --> 00:00:36,690
底部输入底部输出 

11
00:00:34,829 --> 00:00:40,170
世界顶尖的环境

12
00:00:36,690 --> 00:00:43,379
代理人在感觉到的运作 

13
00:00:40,170 --> 00:00:45,270
传感器在外面和外面的世界

14
00:00:43,379 --> 00:00:50,160
将其转换为原始数据可解释 

15
00:00:45,270 --> 00:00:54,329
通过机器传感器数据和从那里 

16
00:00:50,160 --> 00:00:58,350
您提取的原始传感器数据为您提供 

17
00:00:54,329 --> 00:01:01,489
从该 数据 ，例如提取物结构

18
00:00:58,350 --> 00:01:04,830
您可以输入它来理解 它 

19
00:01:01,489 --> 00:01:10,500
歧视分开理解 

20
00:01:04,830 --> 00:01:12,479
数据和我们讨论过的你形成的更高 

21
00:01:10,500 --> 00:01:15,299
和更高阶的表示a 

22
00:01:12,479 --> 00:01:17,280
基于表示的层次

23
00:01:15,299 --> 00:01:23,759
哪台机器学习技巧 

24
00:01:17,280 --> 00:01:26,070
然后可以在机器上应用 

25
00:01:23,759 --> 00:01:28,950
学习技巧的理解为 

26
00:01:26,070 --> 00:01:30,689
我提到过将数据转换成 

27
00:01:28,950 --> 00:01:32,549
功能更高阶

28
00:01:30,689 --> 00:01:35,280
表示和简单

29
00:01:32,549 --> 00:01:36,900
可行的有用信息我们

30
00:01:35,280 --> 00:01:38,939
将该信息汇总到 

31
00:01:36,900 --> 00:01:40,640
知识，我们采取的碎片 

32
00:01:38,939 --> 00:01:42,810
从数据中提取的知识

33
00:01:40,640 --> 00:01:48,659
通过机器学习技巧 

34
00:01:42,810 --> 00:01:51,860
并建立一个分类法 的图书馆 

35
00:01:48,659 --> 00:01:57,570
知识和 我们的知识 

36
00:01:51,860 --> 00:02:01,950
推理老化的理由 

37
00:01:57,570 --> 00:02:04,409
聚合以连接它的 数据 

38
00:02:01,950 --> 00:02:06,780
在最近看到的还是遥远的 

39
00:02:04,409 --> 00:02:09,750
过去去了解那个 世界 

40
00:02:06,780 --> 00:02:11,910
经营并最终制定计划 

41
00:02:09,750 --> 00:02:13,940
如何在这个世界的基础上采取行动

42
00:02:11,910 --> 00:02:17,900
基于它想做的目标

43
00:02:13,940 --> 00:02:19,310
完成后 我提到了一个简单的但是 

44
00:02:17,900 --> 00:02:22,370
普遍接受 的 定义 

45
00:02:19,310 --> 00:02:25,580
智能是一个能够的系统 

46
00:02:22,370 --> 00:02:27,260
完成复杂的目标所以系统

47
00:02:25,580 --> 00:02:29,900
那是在环境中运作的 

48
00:02:27,260 --> 00:02:32,540
这个世界上一定有一个目标 必须有一个 

49
00:02:29,900 --> 00:02:34,820
目标函数奖励功能和 

50
00:02:32,540 --> 00:02:37,550
基于它形成一个计划并采取

51
00:02:34,820 --> 00:02:39,320
行动，因为有运作 

52
00:02:37,550 --> 00:02:43,130
在物理世界中的许多情况

53
00:02:39,320 --> 00:02:45,020
它必须有工具效应器

54
00:02:43,130 --> 00:02:47,510
它适用于改变的行动 

55
00:02:45,020 --> 00:02:50,780
关于这个世界的事情

56
00:02:47,510 --> 00:02:54,290
满满 的人工智能 

57
00:02:50,780 --> 00:02:58,790
在世界 和 世界上行动的制度 

58
00:02:54,290 --> 00:03:02,450
问题是这样的任务是什么样的 

59
00:02:58,790 --> 00:03:04,330
系统承担什么样的任务可以

60
00:03:02,450 --> 00:03:08,240
人工智能系统学习 为 

61
00:03:04,330 --> 00:03:11,180
我们今天理解AI，我们会谈

62
00:03:08,240 --> 00:03:13,220
关于 更深层次的进步 

63
00:03:11,180 --> 00:03:15,200
执法学习方法和一些

64
00:03:13,220 --> 00:03:18,739
能够 做到的迷人方式 

65
00:03:15,200 --> 00:03:21,890
占用大部分堆栈 并将 其 视为 

66
00:03:18,739 --> 00:03:24,380
一个端到端的学习问题，但我们

67
00:03:21,890 --> 00:03:26,600
看看我们看简单的游戏

68
00:03:24,380 --> 00:03:28,810
形式化的世界， 而它 仍然存在 

69
00:03:26,600 --> 00:03:32,120
令人印象深刻的美丽 

70
00:03:28,810 --> 00:03:36,860
成就是 正式 还是 

71
00:03:32,120 --> 00:03:40,459
我们可以将任务超越游戏

72
00:03:36,860 --> 00:03:45,769
进入医学诊断的专家任务

73
00:03:40,459 --> 00:03:49,209
设计和自然语言和 

74
00:03:45,769 --> 00:03:57,170
最后是人类层面的情感任务 

75
00:03:49,209 --> 00:03:59,750
想象力意识让我们一次

76
00:03:57,170 --> 00:04:03,980
再次审查堆栈的实用性 

77
00:03:59,750 --> 00:04:07,400
在我们输入的工具中 

78
00:04:03,980 --> 00:04:10,880
汽车在世界上运行的机器人 

79
00:04:07,400 --> 00:04:14,570
人形无人机 对我们来说 是无人机 

80
00:04:10,880 --> 00:04:17,600
相机雷达GPS立体声相机音频 

81
00:04:14,570 --> 00:04:19,100
用于通信的麦克风网络

82
00:04:17,600 --> 00:04:23,419
和各种方法来 衡量 

83
00:04:19,100 --> 00:04:27,409
IMU的运动学

84
00:04:23,419 --> 00:04:29,930
然后处理原始传感数据

85
00:04:27,409 --> 00:04:31,999
表征 形式的特征 是 

86
00:04:29,930 --> 00:04:33,289
形成多个越来越高 

87
00:04:31,999 --> 00:04:35,449
订单陈述

88
00:04:33,289 --> 00:04:38,050
这就是深度学习之前带给我们的东西

89
00:04:35,449 --> 00:04:41,029
神经 网络出现之前 

90
00:04:38,050 --> 00:04:43,219
在最近的神经成功之前

91
00:04:41,029 --> 00:04:45,740
网络更深入，因此 

92
00:04:43,219 --> 00:04:48,560
能够形成高阶表示 

93
00:04:45,740 --> 00:04:51,379
由专家完成的数据

94
00:04:48,560 --> 00:04:53,569
今天人类专家网络 能够 

95
00:04:51,379 --> 00:04:55,939
那就是代表作品 

96
00:04:53,569 --> 00:04:58,789
并在代表作品的顶部

97
00:04:55,939 --> 00:05:00,289
这些网络能够完成的最后一层 

98
00:04:58,789 --> 00:05:04,270
完成有监督的学习 

99
00:05:00,289 --> 00:05:08,150
任务生成任务和 

100
00:05:04,270 --> 00:05:10,550
无监督的聚类任务

101
00:05:08,150 --> 00:05:13,310
机器学习这就是我们所说的

102
00:05:10,550 --> 00:05:16,689
在讲座 一 上课 ，我们会 

103
00:05:13,310 --> 00:05:20,029
明天和周三继续

104
00:05:16,689 --> 00:05:21,919
这是有监督的学习，你可以 

105
00:05:20,029 --> 00:05:25,219
想想那些网络 的输出

106
00:05:21,919 --> 00:05:27,879
作为简单清洁有用的宝贵 

107
00:05:25,219 --> 00:05:31,729
信息 是 知识和 

108
00:05:27,879 --> 00:05:34,550
知识可以 是形式的 

109
00:05:31,729 --> 00:05:37,129
单个数字它可能是回归 

110
00:05:34,550 --> 00:05:39,169
连续变量可能是一个 

111
00:05:37,129 --> 00:05:45,409
它可以是图像的数字序列

112
00:05:39,169 --> 00:05:47,839
音频句子文本演讲一次 

113
00:05:45,409 --> 00:05:51,439
知识被提取和汇总

114
00:05:47,839 --> 00:05:54,439
我们如何以多分辨率连接它

115
00:05:51,439 --> 00:05:58,479
总是形成思想层次结构的连接 

116
00:05:54,439 --> 00:06:01,639
想法是琐碎的愚蠢的例子 

117
00:05:58,479 --> 00:06:04,849
连接图像活动识别

118
00:06:01,639 --> 00:06:07,219
和音频， 例如， 如果它看起来像 

119
00:06:04,849 --> 00:06:10,699
鸭叫声像鸭子 ，游泳像 

120
00:06:07,219 --> 00:06:11,870
鸭我们目前没有办法 

121
00:06:10,699 --> 00:06:14,960
有效地整合了这一点 

122
00:06:11,870 --> 00:06:17,689
信息，以 产生一个更高的 

123
00:06:14,960 --> 00:06:22,099
信心估计实际上 是 

124
00:06:17,689 --> 00:06:25,430
鸭子和规划片的任务 

125
00:06:22,099 --> 00:06:27,349
把感官信息融合起来 

126
00:06:25,430 --> 00:06:30,430
感官信息和制作

127
00:06:27,349 --> 00:06:33,800
行动控制和长期计划

128
00:06:30,430 --> 00:06:35,670
根据我们的信息

129
00:06:33,800 --> 00:06:38,160
今天讨论

130
00:06:35,670 --> 00:06:39,840
越来越顺从 了 

131
00:06:38,160 --> 00:06:42,240
学习深度学习的方法

132
00:06:39,840 --> 00:06:44,460
方法，但迄今为止最多 

133
00:06:42,240 --> 00:06:46,530
成功和非学习优化

134
00:06:44,460 --> 00:06:48,360
基于几种方法的基础方法

135
00:06:46,530 --> 00:06:51,330
我们 的演讲嘉宾 包括 

136
00:06:48,360 --> 00:06:56,460
这个机器人 Atlas 的创造者 

137
00:06:51,330 --> 00:06:58,440
波士顿动力学所以问题多少 

138
00:06:56,460 --> 00:07:01,080
堆栈的结构可以 学习并结束 

139
00:06:58,440 --> 00:07:03,630
从 输入到输出我们都知道 

140
00:07:01,080 --> 00:07:05,340
可以学习表示和 

141
00:07:03,630 --> 00:07:08,370
来自代表和知识的知识

142
00:07:05,340 --> 00:07:13,590
知识即使使用内核方法 

143
00:07:08,370 --> 00:07:17,010
SVM ，当然还有神经网络 

144
00:07:13,590 --> 00:07:21,060
网络从表示到映射

145
00:07:17,010 --> 00:07:22,680
信息一直是主要的

146
00:07:21,060 --> 00:07:26,340
机器学习的成功飞越 

147
00:07:22,680 --> 00:07:29,310
过去的三个十年里已经 从 映射 

148
00:07:26,340 --> 00:07:31,620
原始传感器数据到知识的 

149
00:07:29,310 --> 00:07:34,430
自动化成功的地方

150
00:07:31,620 --> 00:07:37,350
深度学习的表征学习 

151
00:07:34,430 --> 00:07:40,470
直接取得了成功 

152
00:07:37,350 --> 00:07:43,950
原始数据 知识的开放性 问题 

153
00:07:40,470 --> 00:07:46,140
如果可以的话， 今天及 以后 对我们而言 

154
00:07:43,950 --> 00:07:48,990
扩大红框那里什么都可以 

155
00:07:46,140 --> 00:07:51,350
从感官数据中学习并结束 

156
00:07:48,990 --> 00:07:53,550
推理如此聚合形成更高 

157
00:07:51,350 --> 00:07:57,510
提取的表示

158
00:07:53,550 --> 00:07:59,240
知识，形成计划和行动 

159
00:07:57,510 --> 00:08:02,370
在这个世界上从原始的感官数据 

160
00:07:59,240 --> 00:08:05,190
我们将展示 令人难以置信的事实 

161
00:08:02,370 --> 00:08:07,080
我们能够做到CERN究竟是什么 

162
00:08:05,190 --> 00:08:09,470
在这里显示并以更深的结尾 

163
00:08:07,080 --> 00:08:12,750
执法学习琐碎的任务

164
00:08:09,470 --> 00:08:15,000
一个普遍的问题是 

165
00:08:12,750 --> 00:08:17,790
那是否可以继续 

166
00:08:15,000 --> 00:08:23,880
自动 驾驶汽车的现实任务 

167
00:08:17,790 --> 00:08:26,340
类人机器人等等 

168
00:08:23,880 --> 00:08:28,290
这个悬而未决的问题今天让我们谈谈吧 

169
00:08:26,340 --> 00:08:31,400
关于加强学习 的问题 

170
00:08:28,290 --> 00:08:31,400
三种机器学习 

171
00:08:31,880 --> 00:08:37,440
监督

172
00:08:34,430 --> 00:08:40,529
无监督是类别在 

173
00:08:37,440 --> 00:08:42,630
极端相 对于金额 

174
00:08:40,529 --> 00:08:44,940
需要的人和人的输入 

175
00:08:42,630 --> 00:08:47,100
用于监督学习每一部分 

176
00:08:44,940 --> 00:08:50,339
用于教授这些数据的数据 

177
00:08:47,100 --> 00:08:52,440
系统首先 由人类标记 

178
00:08:50,339 --> 00:08:56,540
在右边无人监督学习 

179
00:08:52,440 --> 00:09:01,080
没有数据是由人类标记

180
00:08:56,540 --> 00:09:04,740
之间是来自人类的一些稀疏输入 

181
00:09:01,080 --> 00:09:07,110
半监督学习是唯一的时

182
00:09:04,740 --> 00:09:09,630
部分数据由人类提供

183
00:09:07,110 --> 00:09:11,700
事实上，其余的必须是 

184
00:09:09,630 --> 00:09:14,150
推断由系统推广和 

185
00:09:11,700 --> 00:09:17,220
那 是强化学习的结果 

186
00:09:14,150 --> 00:09:20,700
强化学习已经显示出来 

187
00:09:17,220 --> 00:09:24,180
正如我所说的每一次成功的猫

188
00:09:20,700 --> 00:09:28,830
演示必须包括他们的猫 

189
00:09:24,180 --> 00:09:30,450
应该是巴甫洛夫的猫和铃声 

190
00:09:28,830 --> 00:09:32,339
一个铃铛，每次响铃 

191
00:09:30,450 --> 00:09:36,420
他们得到食物， 他们学到了 这一点 

192
00:09:32,339 --> 00:09:41,490
处理 强化 的目标 

193
00:09:36,420 --> 00:09:44,430
学习是从稀疏的 奖励中 学习 

194
00:09:41,490 --> 00:09:47,130
来自稀疏监督的数据 

195
00:09:44,430 --> 00:09:49,920
数据并利用这一事实 

196
00:09:47,130 --> 00:09:51,839
在模拟中或在现实世界中 

197
00:09:49,920 --> 00:09:55,020
是世界的时间一致性

198
00:09:51,839 --> 00:09:56,970
有一种时间动态 

199
00:09:55,020 --> 00:09:59,520
从州到国家 

200
00:09:56,970 --> 00:10:02,040
通过时间，你可以 传播 

201
00:09:59,520 --> 00:10:04,110
即使信息即信息 

202
00:10:02,040 --> 00:10:06,150
你被收到了关于 

203
00:10:04,110 --> 00:10:08,610
监督地面的事实是稀疏的 

204
00:10:06,150 --> 00:10:11,430
你 可以 回复那些信息 

205
00:10:08,610 --> 00:10:13,470
通过时间来推断 一些事情 

206
00:10:11,430 --> 00:10:16,770
在此 之前 发生的事实 

207
00:10:13,470 --> 00:10:18,630
即使你的奖励信号很弱也是如此

208
00:10:16,770 --> 00:10:22,290
它使用的是物理的事实 

209
00:10:18,630 --> 00:10:25,230
世界随着时间的推移和一些人而演变 

210
00:10:22,290 --> 00:10:29,160
采取稀疏的可预测方式 

211
00:10:25,230 --> 00:10:31,200
信息并将其推广到

212
00:10:29,160 --> 00:10:33,779
完整的体验 

213
00:10:31,200 --> 00:10:36,120
我们学会了这两个 

214
00:10:33,779 --> 00:10:40,800
今天的问题我们会深入讨论 

215
00:10:36,120 --> 00:10:42,990
交通作为一种方法深入

216
00:10:40,800 --> 00:10:46,140
强化学习如此深入的交通 

217
00:10:42,990 --> 00:10:49,620
是我们去年举办的比赛 

218
00:10:46,140 --> 00:10:51,300
今年和今年显着扩大

219
00:10:49,620 --> 00:10:54,330
我将讨论一些 细节和 

220
00:10:51,300 --> 00:10:56,310
这个房间里的人怎么可以在你的房间里

221
00:10:54,330 --> 00:10:59,900
智能手机 今天或如果你有 

222
00:10:56,310 --> 00:11:01,940
我正在 谈论笔记本电脑培训代理 

223
00:10:59,900 --> 00:11:04,920
在浏览器中训练神经网络 

224
00:11:01,940 --> 00:11:07,710
我们已经添加了一些我们已经添加的东西 

225
00:11:04,920 --> 00:11:09,480
增加了我们现在 转向它的能力

226
00:11:07,710 --> 00:11:11,370
进入多代理更深层次的执法

227
00:11:09,480 --> 00:11:14,570
学习问题，你可以控制 

228
00:11:11,370 --> 00:11:18,120
您 网络中最多十辆汽车 

229
00:11:14,570 --> 00:11:21,330
也许不那么重要但非常酷 

230
00:11:18,120 --> 00:11:24,900
是自定义方式的能力 

231
00:11:21,330 --> 00:11:27,240
代理看起来所以你可以上传和人

232
00:11:24,900 --> 00:11:29,490
已经荒谬了 

233
00:11:27,240 --> 00:11:31,470
开始这样做上传不同 

234
00:11:29,490 --> 00:11:34,290
图像而不是显示的汽车 

235
00:11:31,470 --> 00:11:38,030
只要 它维持 

236
00:11:34,290 --> 00:11:41,750
这里显示的尺寸是SpaceX火箭 

237
00:11:38,030 --> 00:11:44,430
比赛在网站上进行 

238
00:11:41,750 --> 00:11:47,690
麻省理工学院ID 自动驾驶的汽车 

239
00:11:44,430 --> 00:11:51,030
深度流量将在稍后返回 

240
00:11:47,690 --> 00:11:54,450
代码在github 上还有更多 

241
00:11:51,030 --> 00:11:56,040
信息 一个入门代码 和一篇论文 

242
00:11:54,450 --> 00:12:00,060
描述一些基本的 

243
00:11:56,040 --> 00:12:03,410
有助于您 在此 获胜的见解

244
00:12:00,060 --> 00:12:06,330
竞争是一种档案 

245
00:12:03,410 --> 00:12:10,470
所以从讲座中的监督学习

246
00:12:06,330 --> 00:12:15,060
一到今天我们可以监督学习 

247
00:12:10,470 --> 00:12:17,130
想到 记住 基本事实 

248
00:12:15,060 --> 00:12:20,360
为了形成表示数据

249
00:12:17,130 --> 00:12:23,580
从那个基本事实 中推广 出来 

250
00:12:20,360 --> 00:12:26,130
强化学习是我们能想到的 

251
00:12:23,580 --> 00:12:30,410
作为暴力传播的一种方式

252
00:12:26,130 --> 00:12:37,890
那些信息稀疏的信息

253
00:12:30,410 --> 00:12:41,280
通过时间来分配质量奖励

254
00:12:37,890 --> 00:12:45,090
声明没有 直接的 

255
00:12:41,280 --> 00:12:47,940
奖励，以了解这个世界的 时候 

256
00:12:45,090 --> 00:12:50,340
奖励稀疏但是相互联系 

257
00:12:47,940 --> 00:12:55,080
通过时间， 你可以把它想象成 

258
00:12:50,340 --> 00:12:59,460
推理所以 

259
00:12:55,080 --> 00:13:02,280
通过时间模拟大多数 

260
00:12:59,460 --> 00:13:05,430
强化学习非常接近 

261
00:13:02,280 --> 00:13:07,800
只是说有一个代理人 参加了 

262
00:13:05,430 --> 00:13:10,380
在一个州采取行动并收到一点 

263
00:13:07,800 --> 00:13:13,320
奖励和代理经营 

264
00:13:10,380 --> 00:13:15,360
环境执行动作 接收 

265
00:13:13,320 --> 00:13:17,640
观察到的状态和新状态 

266
00:13:15,360 --> 00:13:22,980
在 这个过程中得到奖励 

267
00:13:17,640 --> 00:13:25,920
一遍又一遍地继续 

268
00:13:22,980 --> 00:13:27,540
我们可以想到任何一个例子

269
00:13:25,920 --> 00:13:31,530
视频游戏，其中一些 我们会谈 

270
00:13:27,540 --> 00:13:36,810
今天 像雅达利突围 作为 

271
00:13:31,530 --> 00:13:40,080
环境中，代理人是 每个人的桨 

272
00:13:36,810 --> 00:13:43,050
代理人采取的行动有一个 

273
00:13:40,080 --> 00:13:45,720
影响人类的 进化 

274
00:13:43,050 --> 00:13:48,930
环境和成功的衡量

275
00:13:45,720 --> 00:13:51,780
在这种情况下通过一些奖励机制 

276
00:13:48,930 --> 00:13:55,740
积分 由游戏和每个人给出 

277
00:13:51,780 --> 00:13:58,500
游戏有一个不同的点计划

278
00:13:55,740 --> 00:14:01,230
必须转换为规范化的方式 

279
00:13:58,500 --> 00:14:03,090
这是由系统解释 的 

280
00:14:01,230 --> 00:14:09,030
目标是 最大化这些点 

281
00:14:03,090 --> 00:14:11,040
最大化奖励连续

282
00:14:09,030 --> 00:14:12,660
卡杆的问题通过平衡 

283
00:14:11,040 --> 00:14:15,990
目标是平衡杆顶 

284
00:14:12,660 --> 00:14:18,330
移动车 状态是角度 

285
00:14:15,990 --> 00:14:21,210
角速度水平位置

286
00:14:18,330 --> 00:14:23,760
速度动作是水平的 

287
00:14:21,210 --> 00:14:26,010
力量应用于购物车 和奖励 

288
00:14:23,760 --> 00:14:28,700
如果杆是每个时间步是一个 

289
00:14:26,010 --> 00:14:31,160
还是挺直的 

290
00:14:28,700 --> 00:14:33,320
一切

291
00:14:31,160 --> 00:14:40,700
视频游戏的第一人称射击游戏 

292
00:14:33,320 --> 00:14:43,400
现在Starcraft的战略游戏以防万一 

293
00:14:40,700 --> 00:14:45,800
第一人称射击游戏 和 厄运是什么 

294
00:14:43,400 --> 00:14:47,960
环境的目标是游戏 

295
00:14:45,800 --> 00:14:50,120
目标是消除所有对手 

296
00:14:47,960 --> 00:14:53,240
状态是进入的原始游戏像素

297
00:14:50,120 --> 00:14:57,320
行动向左上方向上移动

298
00:14:53,240 --> 00:15:00,190
等等，奖励是积极的 

299
00:14:57,320 --> 00:15:04,900
当消灭对手和 

300
00:15:00,190 --> 00:15:04,900
当代理被淘汰时为负

301
00:15:05,380 --> 00:15:10,910
工业机器人一直在用 

302
00:15:09,020 --> 00:15:12,950
机器人手臂 的目标是 拿起 一个 

303
00:15:10,910 --> 00:15:15,620
装置从一个盒子里把 它放进去 

304
00:15:12,950 --> 00:15:17,660
容器状态是 原始像素 

305
00:15:15,620 --> 00:15:21,050
机器人观察的现实世界

306
00:15:17,660 --> 00:15:22,520
行动是可能的行动 

307
00:15:21,050 --> 00:15:23,900
机器人上的不同程度 的 

308
00:15:22,520 --> 00:15:25,910
自由正在 通过这些程度 

309
00:15:23,900 --> 00:15:28,820
将不同的执行器移动到 

310
00:15:25,910 --> 00:15:30,980
意识到手臂和手臂的位置 

311
00:15:28,820 --> 00:15:32,540
放置 时奖励是正

312
00:15:30,980 --> 00:15:36,380
设备成功和否定 

313
00:15:32,540 --> 00:15:39,890
否则一切都 可以建模 

314
00:15:36,380 --> 00:15:43,610
这种方式马尔可夫的决策过程就在那里

315
00:15:39,890 --> 00:15:46,640
零状态为零 和奖励的状态 

316
00:15:43,610 --> 00:15:48,890
收到新状态再次实现

317
00:15:46,640 --> 00:15:52,070
行动奖励国家行动奖励 

318
00:15:48,890 --> 00:15:55,510
状态，直到达到终端状态 

319
00:15:52,070 --> 00:15:59,390
和。的主要组成部分 

320
00:15:55,510 --> 00:16:01,010
强化学习是一种政策 

321
00:15:59,390 --> 00:16:02,990
那种 在每一个做 什么计划 

322
00:16:01,010 --> 00:16:08,540
单身状态 是什么样的动作 

323
00:16:02,990 --> 00:16:11,210
执行某种值函数 

324
00:16:08,540 --> 00:16:13,400
什么是 好状态的感觉

325
00:16:11,210 --> 00:16:19,640
什么是一个很好的行动 ，采取在 

326
00:16:13,400 --> 00:16:21,380
状态， 有时一个模型 ，它的 

327
00:16:19,640 --> 00:16:24,560
agent代表环境

328
00:16:21,380 --> 00:16:26,060
某种 环境 意识 

329
00:16:24,560 --> 00:16:28,520
其在该动态操作 

330
00:16:26,060 --> 00:16:31,310
对制作 有用的环境 

331
00:16:28,520 --> 00:16:34,450
关于行动的决定让我们采取行动 

332
00:16:31,310 --> 00:16:34,450
琐碎的例子 

333
00:16:35,410 --> 00:16:41,759
一个 三乘 四十二 的网格世界 

334
00:16:38,350 --> 00:16:44,889
我们开始在左下角 广场 

335
00:16:41,759 --> 00:16:48,579
他们走遍这个世界的任务

336
00:16:44,889 --> 00:16:51,129
最大限度地奖励他们获得的奖励 

337
00:16:48,579 --> 00:16:53,110
右上角是 加 1和1平方 

338
00:16:51,129 --> 00:16:56,350
下面是负1和 每个 

339
00:16:53,110 --> 00:17:00,610
你采取的步骤是惩罚或是一个 

340
00:16:56,350 --> 00:17:04,720
负面奖励0.04那么是什么 

341
00:17:00,610 --> 00:17:07,839
现在这个世界上的最佳政策

342
00:17:04,720 --> 00:17:11,079
一切都是确定性的也许这个 

343
00:17:07,839 --> 00:17:14,530
当你开始触底时是政策 

344
00:17:11,079 --> 00:17:15,939
离开很好，因为每一步都伤害了每一个 

345
00:17:14,530 --> 00:17:17,919
步骤有负面的 回报 

346
00:17:15,939 --> 00:17:20,169
那么你想走最短的路 

347
00:17:17,919 --> 00:17:22,959
到最大正方形最大值 

348
00:17:20,169 --> 00:17:27,459
奖励时态空间 

349
00:17:22,959 --> 00:17:31,059
如前所述的非确定性

350
00:17:27,459 --> 00:17:33,370
当你 的概率为 0.8 

351
00:17:31,059 --> 00:17:37,210
选择上升你 上去 但是 

352
00:17:33,370 --> 00:17:41,669
概率为0.1你离开并指向1 

353
00:17:37,210 --> 00:17:46,270
你再次变得不公平，就像生活一样 

354
00:17:41,669 --> 00:17:48,789
这将是最优政策 

355
00:17:46,270 --> 00:17:50,679
基思在这里观察到的每一个 

356
00:17:48,789 --> 00:17:54,760
空间中的 单一状态 必须具有 

357
00:17:50,679 --> 00:17:57,789
计划，因为你 不能因为那么一个 

358
00:17:54,760 --> 00:18:00,039
控制的非确定性方面

359
00:17:57,789 --> 00:18:01,720
你无法控制你去哪里 

360
00:18:00,039 --> 00:18:04,870
结束所以你必须为每个人制定计划 

361
00:18:01,720 --> 00:18:06,610
这是政策有行动的地方 

362
00:18:04,870 --> 00:18:09,490
最佳的行动， 采取在每 

363
00:18:06,610 --> 00:18:11,710
单身现在假设我们改变了

364
00:18:09,490 --> 00:18:14,650
奖励结构和我们的每一步

365
00:18:11,710 --> 00:18:17,230
拿一个负面的 奖励 是一个 

366
00:18:14,650 --> 00:18:19,000
负2所以真的很痛苦 有一个 

367
00:18:17,230 --> 00:18:22,630
我们每一步都要受到高度惩罚

368
00:18:19,000 --> 00:18:24,520
无论我们 总是采取 什么措施，这样 做 

369
00:18:22,630 --> 00:18:27,280
最优政策的最短路径 

370
00:18:24,520 --> 00:18:30,370
走最短路径去

371
00:18:27,280 --> 00:18:35,710
只有在没有的板上 

372
00:18:30,370 --> 00:18:39,970
如果我们减少 了， 就会导致惩罚 

373
00:18:35,710 --> 00:18:43,430
奖励每一步到负0.1 

374
00:18:39,970 --> 00:18:46,030
政策变化是否 

375
00:18:43,430 --> 00:18:50,000
一些额外的徘徊程度

376
00:18:46,030 --> 00:18:52,280
鼓励，随着我们走得更远 

377
00:18:50,000 --> 00:18:56,210
进一步 降低 惩罚作为 

378
00:18:52,280 --> 00:19:00,560
之前以负0.04 多徘徊 

379
00:18:56,210 --> 00:19:06,230
允许更多的游荡和什么时候 

380
00:19:00,560 --> 00:19:12,650
我们终于将 奖励 转为积极的 

381
00:19:06,230 --> 00:19:15,170
所以每走一步都会增加 

382
00:19:12,650 --> 00:19:18,170
奖励然后有一个重要的 

383
00:19:15,170 --> 00:19:21,370
鼓励留在董事会 

384
00:19:18,170 --> 00:19:21,370
没有到达目的地 

385
00:19:21,790 --> 00:19:26,050
有点像很多人的大学 

386
00:19:27,760 --> 00:19:34,340
所以价值就像我们思考的那样发挥作用

387
00:19:30,740 --> 00:19:38,630
关于 国家或价值的价值 

388
00:19:34,340 --> 00:19:41,360
在环境中任何事物是 

389
00:19:38,630 --> 00:19:44,510
奖励有可能 获得在 

390
00:19:41,360 --> 00:19:47,720
未来和 我们看待奖励的方式 

391
00:19:44,510 --> 00:19:51,140
很可能会收到我们的折扣 

392
00:19:47,720 --> 00:19:52,930
未来的奖项因为我们不能永远 

393
00:19:51,140 --> 00:19:56,600
指望它 

394
00:19:52,930 --> 00:19:59,180
Gama越走越远 了 

395
00:19:56,600 --> 00:20:02,450
未来 越来越 多的折扣 

396
00:19:59,180 --> 00:20:05,060
减少奖励的重要性 

397
00:20:02,450 --> 00:20:07,220
收到的奖励和好处 

398
00:20:05,060 --> 00:20:09,920
战略正在考虑这些的总和 

399
00:20:07,220 --> 00:20:11,240
奖励和最大化它最大化

400
00:20:09,920 --> 00:20:13,760
歹徒病房

401
00:20:11,240 --> 00:20:19,510
这就是强化学习所希望的 

402
00:20:13,760 --> 00:20:23,660
实现和 我们使用的提示学习 

403
00:20:19,510 --> 00:20:28,520
任何估算价值的政策

404
00:20:23,660 --> 00:20:32,300
在这样一个州采取行动 

405
00:20:28,520 --> 00:20:34,550
政策忘记我们的政策 

406
00:20:32,300 --> 00:20:37,250
世界并 在这里使用贝尔曼方程 

407
00:20:34,550 --> 00:20:39,650
在底部不断更新 我们的 

408
00:20:37,250 --> 00:20:45,020
估计某项 行动的 好坏程度 

409
00:20:39,650 --> 00:20:46,520
在某种状态，所以我们不需要这个 

410
00:20:45,020 --> 00:20:48,020
这使我们 能够在很多方面运作 

411
00:20:46,520 --> 00:20:50,600
更大的国家空间更大

412
00:20:48,020 --> 00:20:52,220
行动空间我们 在这个世界上 移动 

413
00:20:50,600 --> 00:20:55,010
通过模拟或在现实世界中 

414
00:20:52,220 --> 00:20:57,080
采取行动并更新我们的估计 

415
00:20:55,010 --> 00:21:01,880
某些行动有多好

416
00:20:57,080 --> 00:21:04,460
我是左边的新州 是 

417
00:21:01,880 --> 00:21:06,140
旧 状态的更新值 

418
00:21:04,460 --> 00:21:08,990
等式的起始值和我们 

419
00:21:06,140 --> 00:21:13,130
更新 旧的状态估计 

420
00:21:08,990 --> 00:21:18,340
服用 奖励的总和 

421
00:21:13,130 --> 00:21:22,580
行动的税收行动a并 声明我们和 

422
00:21:18,340 --> 00:21:25,390
可能的最大奖励 

423
00:21:22,580 --> 00:21:30,590
收到以下州 

424
00:21:25,390 --> 00:21:32,660
折扣更新 减少了 

425
00:21:30,590 --> 00:21:35,930
学习率越高，学习越高

426
00:21:32,660 --> 00:21:38,270
我们越快，价值就越高 

427
00:21:35,930 --> 00:21:41,000
将学习 我们赋予的更多价值 

428
00:21:38,270 --> 00:21:43,520
新的信息很 简单 就是这样 

429
00:21:41,000 --> 00:21:47,570
那是Q 学习 简单的更新规则 

430
00:21:43,520 --> 00:21:52,010
让我们探索世界和 

431
00:21:47,570 --> 00:21:53,600
我们探索获取越来越多的信息 

432
00:21:52,010 --> 00:21:56,150
关于在这个世界上做什么好事

433
00:21:53,600 --> 00:21:58,130
而且也总是 在 平衡 

434
00:21:56,150 --> 00:21:59,830
我们将讨论各种问题空间

435
00:21:58,130 --> 00:22:05,420
总之间有平衡 

436
00:21:59,830 --> 00:22:06,920
你形成的探索和利用 

437
00:22:05,420 --> 00:22:08,360
对 Q的更好和更好的估计 

438
00:22:06,920 --> 00:22:11,390
什么行动 的好处 

439
00:22:08,360 --> 00:22:14,150
带你开始了解 它是什么 

440
00:22:11,390 --> 00:22:15,530
采取的最佳行动，但不是 

441
00:22:14,150 --> 00:22:17,540
完美的感觉它仍然是一个 

442
00:22:15,530 --> 00:22:19,700
近似值， 因此有价值 

443
00:22:17,540 --> 00:22:21,620
探索但 越来越好 

444
00:22:19,700 --> 00:22:25,280
你的估计变得越来越少了

445
00:22:21,620 --> 00:22:26,540
探索有利于 我们 

446
00:22:25,280 --> 00:22:29,600
想在一开始就探索很多 

447
00:22:26,540 --> 00:22:31,910
并且越来越少到 最后 

448
00:22:29,600 --> 00:22:34,280
当我们最终释放出系统 时 

449
00:22:31,910 --> 00:22:38,150
进入世界并希望它运作 

450
00:22:34,280 --> 00:22:40,190
最好的， 然后我们把它作为一个 

451
00:22:38,150 --> 00:22:42,470
贪婪系统始终采取最佳 

452
00:22:40,190 --> 00:22:46,730
根据 q2键值进行操作 

453
00:22:42,470 --> 00:22:49,700
功能和 我正在谈论的 一切 

454
00:22:46,730 --> 00:22:53,570
现在是许可证上升 和我们的 

455
00:22:49,700 --> 00:22:56,500
这是 非常重要的参数

456
00:22:53,570 --> 00:22:59,420
赢得了深度交通竞争

457
00:22:56,500 --> 00:23:04,600
这是使用这个非常算法 

458
00:22:59,420 --> 00:23:07,090
一个神经网络的核心，所以罪恶 

459
00:23:04,600 --> 00:23:10,840
的 提示功能 的表表示

460
00:23:07,090 --> 00:23:14,430
其中，y轴是状态4个状态 s 

461
00:23:10,840 --> 00:23:18,460
一二三四， x轴是 

462
00:23:14,430 --> 00:23:20,770
行动一两三 四我们可以 

463
00:23:18,460 --> 00:23:23,440
把这个表当作随机的 

464
00:23:20,770 --> 00:23:25,750
启动或启动初始化 

465
00:23:23,440 --> 00:23:28,240
任何一种不是这样的方式

466
00:23:25,750 --> 00:23:29,740
代表实际 的 现实和 

467
00:23:28,240 --> 00:23:31,960
我们走动这个世界 ，我们采取 

468
00:23:29,740 --> 00:23:34,870
我们使用的更新此表的操作 

469
00:23:31,960 --> 00:23:37,270
贝尔曼方程显示在顶部和这里 

470
00:23:34,870 --> 00:23:40,120
幻灯片现在 在网上可以 看到 

471
00:23:37,270 --> 00:23:42,580
简单伪码算法如何 

472
00:23:40,120 --> 00:23:46,690
更新它如何运行这个门卫 

473
00:23:42,580 --> 00:23:49,000
方程式和随着时间的推移近似 

474
00:23:46,690 --> 00:23:51,880
成为最佳提示表 

475
00:23:49,000 --> 00:23:55,930
问题是那个cue表吧

476
00:23:51,880 --> 00:23:58,210
当我们采取时，变成指数大小

477
00:23:55,930 --> 00:24:01,780
像我们一样处理原始的感官信息

478
00:23:58,210 --> 00:24:04,210
深度撞击或深度撞击的相机

479
00:24:01,780 --> 00:24:07,660
它占据了整个网格空间 

480
00:24:04,210 --> 00:24:10,930
并把这些信息原始的 

481
00:24:07,660 --> 00:24:13,270
原始网格像素的深度流量和时间 

482
00:24:10,930 --> 00:24:17,350
你在这里参加街机游戏

483
00:24:13,270 --> 00:24:19,900
拍摄游戏的原始像素或 

484
00:24:17,350 --> 00:24:23,620
当我们开始游戏的时候 

485
00:24:19,900 --> 00:24:27,990
将单位作为原始单位 

486
00:24:23,620 --> 00:24:31,510
董事会的状态作为输入

487
00:24:27,990 --> 00:24:35,050
潜在的状态空间的数量 

488
00:24:31,510 --> 00:24:38,950
什么 陈述的可能组合 

489
00:24:35,050 --> 00:24:40,990
可能是非常大的比 

490
00:24:38,950 --> 00:24:43,960
我们当然可以保持 记忆 

491
00:24:40,990 --> 00:24:45,670
较大，我们可以永远能够 

492
00:24:43,960 --> 00:24:48,220
准确地 通过近似 

493
00:24:45,670 --> 00:24:52,000
贝尔曼方程随着 时间的推移 

494
00:24:48,220 --> 00:24:54,610
通过简单的更新模拟

495
00:24:52,000 --> 00:24:57,060
贝尔曼方程所以这就是这里

496
00:24:54,610 --> 00:24:59,560
深入的强化学习进来了

497
00:24:57,060 --> 00:25:01,600
神经网络非常好 

498
00:24:59,560 --> 00:25:04,420
近似于他们真正擅长的人

499
00:25:01,600 --> 00:25:07,410
正是这种学习这种任务的任务 

500
00:25:04,420 --> 00:25:07,410
提示表 

501
00:25:09,040 --> 00:25:13,370
所以我们从监督开始

502
00:25:11,210 --> 00:25:15,460
学习或神经网络 帮助了我们 

503
00:25:13,370 --> 00:25:17,990
使用监督记忆模式

504
00:25:15,460 --> 00:25:19,640
真实的数据，我们将转向 

505
00:25:17,990 --> 00:25:25,370
强化学习是希望 

506
00:25:19,640 --> 00:25:28,309
将结果传播到知识深处

507
00:25:25,370 --> 00:25:30,700
学习使我们能够做到这一点 

508
00:25:28,309 --> 00:25:34,790
较大的状态空间要大得多 

509
00:25:30,700 --> 00:25:38,390
动作空间意味着它

510
00:25:34,790 --> 00:25:43,130
可推广它更有能力

511
00:25:38,390 --> 00:25:46,190
处理感官数据的原始内容

512
00:25:43,130 --> 00:25:47,960
这意味着它更有能力 

513
00:25:46,190 --> 00:25:55,130
处理真实的广泛变化 

514
00:25:47,960 --> 00:25:58,040
世界应用程序，它就是这样 

515
00:25:55,130 --> 00:26:01,100
因为它能够学习 

516
00:25:58,040 --> 00:26:07,030
我们讨论过的陈述

517
00:26:01,100 --> 00:26:09,400
周一理解来自 

518
00:26:07,030 --> 00:26:12,830
转换原始感官信息 

519
00:26:09,400 --> 00:26:15,620
进入简单有用的信息 

520
00:26:12,830 --> 00:26:17,450
根据其中的行动 

521
00:26:15,620 --> 00:26:19,970
特殊的国家可以采取 

522
00:26:17,450 --> 00:26:22,460
同样的确切方式，而不是 提示 

523
00:26:19,970 --> 00:26:24,590
表而不是这个 cue函数我们 

524
00:26:22,460 --> 00:26:26,990
插入神经网络输入的地方 

525
00:26:24,590 --> 00:26:30,770
无论多么复杂，都是国家空间 

526
00:26:26,990 --> 00:26:35,510
并且输出是每个 的 值 

527
00:26:30,770 --> 00:26:38,300
你可以采取的行动是 

528
00:26:35,510 --> 00:26:43,220
状态输出是的值

529
00:26:38,300 --> 00:26:47,450
功能很简单，这 是 深Q 

530
00:26:43,220 --> 00:26:49,640
网络 DQ一个核心 

531
00:26:47,450 --> 00:26:51,800
深刻的心灵成功很多 很酷 

532
00:26:49,640 --> 00:26:54,670
你看到的关于视频游戏的东西 D. 

533
00:26:51,800 --> 00:26:57,530
排队或DQ和我们的游戏的变种 

534
00:26:54,670 --> 00:27:03,050
这是 水首先用自然 纸 

535
00:26:57,530 --> 00:27:05,350
一个深刻的 思想成功来自于玩 

536
00:27:03,050 --> 00:27:08,350
包括Atari在内的不同游戏 

537
00:27:05,350 --> 00:27:08,350
游戏

538
00:27:10,130 --> 00:27:18,860
这些 东西怎么训练得很好 

539
00:27:12,770 --> 00:27:20,210
类似于监督学习

540
00:27:18,860 --> 00:27:25,880
贝尔曼方程式顶部 

541
00:27:20,210 --> 00:27:32,480
它采用奖励和折扣 

542
00:27:25,880 --> 00:27:34,520
来自未来国家的预期奖励 

543
00:27:32,480 --> 00:27:36,230
神经网络的损失函数

544
00:27:34,520 --> 00:27:40,520
而你现在会让学习者失去理智 

545
00:27:36,230 --> 00:27:43,490
功能它需要收到的奖励 

546
00:27:40,520 --> 00:27:45,350
目前的状态是向前传球 

547
00:27:43,490 --> 00:27:49,420
通过神经网络 来估计 

548
00:27:45,350 --> 00:27:52,420
未来最佳状态的 价值 

549
00:27:49,420 --> 00:27:57,790
采取行动以 应对未来的国家和地区 

550
00:27:52,420 --> 00:27:59,840
然后从前 向传球中减去

551
00:27:57,790 --> 00:28:02,690
通过网络为当前 

552
00:27:59,840 --> 00:28:05,620
行动中的状态，所以你采取了 

553
00:28:02,690 --> 00:28:06,740
你的Q 之间的区别 

554
00:28:05,620 --> 00:28:09,530
估计

555
00:28:06,740 --> 00:28:14,060
然后你会 网络相信价值 

556
00:28:09,530 --> 00:28:17,960
当前状态是什么以及更多 

557
00:28:14,060 --> 00:28:19,880
很可能是基于 价值 

558
00:28:17,960 --> 00:28:26,870
可以到达的未来状态 

559
00:28:19,880 --> 00:28:31,640
基于你可以采取的行动

560
00:28:26,870 --> 00:28:33,710
算法输入是状态输出 

561
00:28:31,640 --> 00:28:35,810
是每个动作或的动作的Q值 

562
00:28:33,710 --> 00:28:38,080
此图输入是状态 

563
00:28:35,810 --> 00:28:41,540
动作和输出是Q值 

564
00:28:38,080 --> 00:28:47,590
这是非常相似的架构

565
00:28:41,540 --> 00:28:50,180
sa的过渡是 最重要的 

566
00:28:47,590 --> 00:28:56,370
当前状态采取行动接收 

567
00:28:50,180 --> 00:29:00,240
奖励和实现美国黄金州

568
00:28:56,370 --> 00:29:01,650
更新是前馈 传递 

569
00:29:00,240 --> 00:29:04,800
通过网络为当前 

570
00:29:01,650 --> 00:29:07,740
国家为每个人做一个前馈传球 

571
00:29:04,800 --> 00:29:09,900
下一步可能采取的行动 

572
00:29:07,740 --> 00:29:13,890
状态，这就是我们如何计算这两者 

573
00:29:09,900 --> 00:29:17,250
部分损失功能 和更新 

574
00:29:13,890 --> 00:29:19,140
再次使用反向传播的权重

575
00:29:17,250 --> 00:29:22,920
损失函数反向传播是如何 

576
00:29:19,140 --> 00:29:25,230
实际上 ，网络是训练有素的 

577
00:29:22,920 --> 00:29:30,270
出现 了 比长得 多 

578
00:29:25,230 --> 00:29:35,580
深刻的思想 使它成功 了 

579
00:29:30,270 --> 00:29:40,320
真正的工作经验重播 

580
00:29:35,580 --> 00:29:41,970
这是游戏中最大的一个

581
00:29:40,320 --> 00:29:45,690
通过模拟或如果它是物理的 

582
00:29:41,970 --> 00:29:49,200
系统，因为它在世界 上行动 

583
00:29:45,690 --> 00:29:52,500
实际收集观察结果 

584
00:29:49,200 --> 00:29:54,930
进入一个经验库和 那个 

585
00:29:52,500 --> 00:29:57,930
训练是随机进行的 

586
00:29:54,930 --> 00:30:00,420
通过采样库在过去

587
00:29:57,930 --> 00:30:03,720
随机抽样前一个 

588
00:30:00,420 --> 00:30:06,000
经验和批次所以你不是 

589
00:30:03,720 --> 00:30:07,530
总是训练自然 

590
00:30:06,000 --> 00:30:09,780
系统的不断发展

591
00:30:07,530 --> 00:30:12,330
你是随机挑选的训练 

592
00:30:09,780 --> 00:30:15,960
批次的那些经历就像 

593
00:30:12,330 --> 00:30:18,470
巨大的它是一个看起来像一个微妙的 

594
00:30:15,960 --> 00:30:23,160
伎俩，但它是一个非常重要的 一个，这样 

595
00:30:18,470 --> 00:30:27,240
系统不适合 特定的 

596
00:30:23,160 --> 00:30:33,390
这个游戏的演变

597
00:30:27,240 --> 00:30:35,430
模拟另一个重要的

598
00:30:33,390 --> 00:30:37,560
很多深刻的微妙技巧

599
00:30:35,430 --> 00:30:41,010
学习接近微妙的技巧

600
00:30:37,560 --> 00:30:44,550
让所有不同的是修复 

601
00:30:41,010 --> 00:30:47,610
目标网络的损失函数if 

602
00:30:44,550 --> 00:30:49,590
你注意到你 必须使用 神经 

603
00:30:47,610 --> 00:30:52,410
网络厚的单神经网络 

604
00:30:49,590 --> 00:30:57,870
所述GQI网络估计的值 

605
00:30:52,410 --> 00:31:03,360
当前的状态 和 行动对和 

606
00:30:57,870 --> 00:31:06,480
下一因此 使用它多次 和 

607
00:31:03,360 --> 00:31:08,820
你执行那项操作 

608
00:31:06,480 --> 00:31:10,740
更新网络意味着 

609
00:31:08,820 --> 00:31:13,410
失败中的 目标函数 

610
00:31:10,740 --> 00:31:15,900
功能总是在变化 所以你是 

611
00:31:13,410 --> 00:31:17,940
你的损失功能的本质

612
00:31:15,900 --> 00:31:20,390
在你 学习的过程中不断变化

613
00:31:17,940 --> 00:31:22,680
这 对稳定性 来说是个大问题 

614
00:31:20,390 --> 00:31:25,350
可以创建 的 大问题 

615
00:31:22,680 --> 00:31:28,410
学习过程所以这个小技巧是 

616
00:31:25,350 --> 00:31:33,929
修复网络，只更新它 

617
00:31:28,410 --> 00:31:37,290
每个安全的千 步，所以你 

618
00:31:33,929 --> 00:31:39,690
训练网络 是这样的 网络 

619
00:31:37,290 --> 00:31:43,470
用于计算 目标函数 

620
00:31:39,690 --> 00:31:45,840
里面的损失函数是固定的 

621
00:31:43,470 --> 00:31:48,710
在a上产生更稳定的计算

622
00:31:45,840 --> 00:31:51,630
损失功能所以地面没有 

623
00:31:48,710 --> 00:31:55,020
当你想要找到时，在你身下转移

624
00:31:51,630 --> 00:31:57,450
损失函数的最小损失 

625
00:31:55,020 --> 00:32:01,410
功能不会在不可预测的情况下改变 

626
00:31:57,450 --> 00:32:05,220
难以理解的方式和奖励

627
00:32:01,410 --> 00:32:09,000
剪裁它总是与 真实 

628
00:32:05,220 --> 00:32:10,770
正在运行它的一般系统

629
00:32:09,000 --> 00:32:14,600
寻求在广义上运作

630
00:32:10,770 --> 00:32:17,610
方式是非常适合这些各种游戏

631
00:32:14,600 --> 00:32:19,710
一些人的观点不同 

632
00:32:17,610 --> 00:32:21,390
分数很低，有些分数很高

633
00:32:19,710 --> 00:32:23,730
积极 和消极，他们都是 

634
00:32:21,390 --> 00:32:26,850
归一化为好的点

635
00:32:23,730 --> 00:32:29,730
积分或积分是1 

636
00:32:26,850 --> 00:32:32,309
和负点是负 1 

637
00:32:29,730 --> 00:32:35,550
那个奖励剪辑简化了

638
00:32:32,309 --> 00:32:38,730
奖励结构，因为很多 

639
00:32:35,550 --> 00:32:43,140
比赛是30 FPS或60 FPS 和 

640
00:32:38,730 --> 00:32:45,660
行动不是没有价值的 

641
00:32:43,140 --> 00:32:47,460
在 里面 以如此高的 速度 采取行动 

642
00:32:45,660 --> 00:32:49,740
其中特别是Atari游戏 

643
00:32:47,460 --> 00:32:52,200
那么你每四个只采取一次行动 

644
00:32:49,740 --> 00:32:54,090
步骤，同时仍然采取框架 

645
00:32:52,200 --> 00:32:57,179
作为时间窗口的一部分

646
00:32:54,090 --> 00:33:01,380
决定技巧但希望能给你

647
00:32:57,179 --> 00:33:05,580
一种必要的事物感

648
00:33:01,380 --> 00:33:07,620
对于像这样的两篇开创性论文 

649
00:33:05,580 --> 00:33:09,390
而对于更重要的 

650
00:33:07,620 --> 00:33:11,280
赢得深度交通的成就

651
00:33:09,390 --> 00:33:14,130
就是它

652
00:33:11,280 --> 00:33:19,410
这些技巧在这里有所不同 

653
00:33:14,130 --> 00:33:21,510
在底部是圆圈的时候 

654
00:33:19,410 --> 00:33:24,300
技术在x1中使用它不是 

655
00:33:21,510 --> 00:33:26,190
看重播和目标需要

656
00:33:24,300 --> 00:33:28,560
目标网络和 体验重播 

657
00:33:26,190 --> 00:33:31,830
当两者都用于游戏时 

658
00:33:28,560 --> 00:33:32,430
突围河突袭海上任务和空间 

659
00:33:31,830 --> 00:33:34,710
侵略者 

660
00:33:32,430 --> 00:33:38,760
数字越高越好 

661
00:33:34,710 --> 00:33:40,680
更多 的积分达到 ，所以当 它 

662
00:33:38,760 --> 00:33:42,600
让你感觉到重播和 

663
00:33:40,680 --> 00:33:44,760
目标都有重要意义 

664
00:33:42,600 --> 00:33:49,200
改善了表现 

665
00:33:44,760 --> 00:33:54,110
系统数量级的改进

666
00:33:49,200 --> 00:33:59,280
两个数量级的 解体和 

667
00:33:54,110 --> 00:34:03,900
这是实现 dq1的伪代码

668
00:33:59,280 --> 00:34:08,760
学习关键的事情要注意和 

669
00:34:03,900 --> 00:34:11,790
你可以看一下幻灯片 是 的 

670
00:34:08,760 --> 00:34:14,130
循环播放的while循环

671
00:34:11,790 --> 00:34:17,880
游戏和选择行动 

672
00:34:14,130 --> 00:34:22,050
游戏不是训练的一部分 

673
00:34:17,880 --> 00:34:25,250
这是保存观察的一部分 

674
00:34:22,050 --> 00:34:27,720
国家行动 奖励 下一个州 

675
00:34:25,250 --> 00:34:30,090
观察是将它们保存 为重播 

676
00:34:27,720 --> 00:34:32,909
记忆到那个库 然后你 

677
00:34:30,090 --> 00:34:36,179
从该重放内存中随机抽样 

678
00:34:32,909 --> 00:34:40,560
然后根据网络训练网络 

679
00:34:36,179 --> 00:34:43,169
损失函数和概率上升 

680
00:34:40,560 --> 00:34:46,399
顶部有概率 epsilon选择 

681
00:34:43,169 --> 00:34:49,290
epsilon的随机动作

682
00:34:46,399 --> 00:34:51,810
探索的概率 

683
00:34:49,290 --> 00:34:55,470
减少那些你会看到的东西 

684
00:34:51,810 --> 00:34:57,630
深交通以及处于率 

685
00:34:55,470 --> 00:34:59,520
那次探索减少了

686
00:34:57,630 --> 00:35:02,310
通过培训过程的时间你 

687
00:34:59,520 --> 00:35:05,190
想先探索一下 ，少 探索一下 

688
00:35:02,310 --> 00:35:08,360
少随着时间的推移所以这种 算法 

689
00:35:05,190 --> 00:35:12,450
能够在2015年完成 ，并 

690
00:35:08,360 --> 00:35:18,770
因为很多 不可思议的事情的东西 

691
00:35:12,450 --> 00:35:22,829
这让AI世界认为我们 

692
00:35:18,770 --> 00:35:26,640
人到 的东西， 

693
00:35:22,829 --> 00:35:28,979
一般AI是第一个可达到的 

694
00:35:26,640 --> 00:35:31,529
原始传感器信息的时间 

695
00:35:28,979 --> 00:35:33,599
用于创建一个行为和 

696
00:35:31,529 --> 00:35:35,430
让世界 变得 有意义 

697
00:35:33,599 --> 00:35:37,529
世界的物理学就足够了 

698
00:35:35,430 --> 00:35:42,349
能够从很小的角度取得成功 

699
00:35:37,529 --> 00:35:42,349
信息， 但 这些游戏 是 微不足道的 

700
00:35:42,829 --> 00:35:50,459
即使他们中有很多这个 

701
00:35:48,539 --> 00:35:52,650
dqn方法已经 超越了 

702
00:35:50,459 --> 00:35:54,319
很多Atari游戏 

703
00:35:52,650 --> 00:35:56,599
这就是报道的 内容 

704
00:35:54,319 --> 00:36:00,989
超越人类的表现 

705
00:35:56,599 --> 00:36:05,219
但是这些游戏 对我而言 都是微不足道的 

706
00:36:00,989 --> 00:36:06,869
思考，也许有偏见我有偏见 但是 

707
00:36:05,219 --> 00:36:08,430
最伟大的成就之一 

708
00:36:06,869 --> 00:36:12,509
人工智能 在最后 

709
00:36:08,430 --> 00:36:19,069
十年至少从 哲学角度 来看 

710
00:36:12,509 --> 00:36:24,390
或者研究视角是alphago 0 

711
00:36:19,069 --> 00:36:27,479
首先是alphago， 然后是alphago 0 

712
00:36:24,390 --> 00:36:30,390
深度 打击最好的 系统 

713
00:36:27,479 --> 00:36:35,400
在这场比赛 中的世界是 如此 

714
00:36:30,390 --> 00:36:38,219
去 它的游戏很简单，我不会进入 

715
00:36:35,400 --> 00:36:42,049
规则，但基本上它是一个19乘19 

716
00:36:38,219 --> 00:36:45,660
幻灯片底部显示的板

717
00:36:42,049 --> 00:36:50,369
为表 的 最下面一行 

718
00:36:45,660 --> 00:36:53,549
董事会19人19人的合法人数 

719
00:36:50,369 --> 00:36:57,719
比赛位置 是10倍的10倍 

720
00:36:53,549 --> 00:37:00,359
170的力量是一个非常大的 数字 

721
00:36:57,719 --> 00:37:03,150
可能的立场考虑任何一个 

722
00:37:00,359 --> 00:37:07,349
时间特别是游戏 进化了 

723
00:37:03,150 --> 00:37:12,199
可能的动作数量非常多 

724
00:37:07,349 --> 00:37:14,640
比国际象棋更大，这就是人工智能的原因 

725
00:37:12,199 --> 00:37:21,799
社区认为这个游戏是 

726
00:37:14,640 --> 00:37:26,749
直到2016年alphago时才能解决 

727
00:37:21,799 --> 00:37:30,019
使用这个使用人类专家的位置发挥 

728
00:37:26,749 --> 00:37:32,940
以有监督的方式种子 

729
00:37:30,019 --> 00:37:35,489
强化学习方法，我会 

730
00:37:32,940 --> 00:37:36,860
用一点点细节描述 一下 

731
00:37:35,489 --> 00:37:42,660
几个 幻灯片 在这里 

732
00:37:36,860 --> 00:37:47,940
然后打败世界上 最好 的 

733
00:37:42,660 --> 00:37:52,860
alphago 0即 成就 

734
00:37:47,940 --> 00:37:59,370
人工智能的十年是能够的 

735
00:37:52,860 --> 00:38:04,710
没有关于人类的训练数据 

736
00:37:59,370 --> 00:38:07,110
专家游戏并击败了最好的 

737
00:38:04,710 --> 00:38:12,140
这是一场极其复杂的比赛

738
00:38:07,110 --> 00:38:17,040
这不是Atari，这 是一个很大的问题 

739
00:38:12,140 --> 00:38:18,690
高阶难度游戏和那个

740
00:38:17,040 --> 00:38:21,240
和玩家的素质是 

741
00:38:18,690 --> 00:38:24,270
竞争对手要高得多 

742
00:38:21,240 --> 00:38:26,010
能够非常迅速地到这里来 

743
00:38:24,270 --> 00:38:29,700
达到更好的评级 

744
00:38:26,010 --> 00:38:32,160
alphago 和优于 不同的 

745
00:38:29,700 --> 00:38:35,250
alphago的变种， 当然更好 

746
00:38:32,160 --> 00:38:39,800
比21中 最好 的 人类球员 

747
00:38:35,250 --> 00:38:43,650
自我玩耍的日子，它是如何运作的 

748
00:38:39,800 --> 00:38:45,630
所有这些方法非常类似 

749
00:38:43,650 --> 00:38:49,490
以前的传统的 

750
00:38:45,630 --> 00:38:53,600
不是基于深度 学习的 

751
00:38:49,490 --> 00:38:57,270
使用蒙特卡罗树 搜索 MCTS 

752
00:38:53,600 --> 00:39:00,510
那是你有这么大的时候 

753
00:38:57,270 --> 00:39:06,410
你从董事会 和 你开始的国家空间

754
00:39:00,510 --> 00:39:10,550
玩，你 选择一些动作 

755
00:39:06,410 --> 00:39:12,720
开发探索平衡

756
00:39:10,550 --> 00:39:15,060
选择全新探索 

757
00:39:12,720 --> 00:39:17,010
职位或深入职位

758
00:39:15,060 --> 00:39:19,140
你知道直到底部都很好

759
00:39:17,010 --> 00:39:20,880
到达比赛直到最后 

760
00:39:19,140 --> 00:39:24,720
到达状态，然后你回来 

761
00:39:20,880 --> 00:39:26,250
宣传您的选择质量

762
00:39:24,720 --> 00:39:28,850
导致这个位置 

763
00:39:26,250 --> 00:39:34,320
并以这种方式你学习的价值 

764
00:39:28,850 --> 00:39:37,130
董事会职位和比赛的情况 

765
00:39:34,320 --> 00:39:42,000
用于最成功的去玩 

766
00:39:37,130 --> 00:39:43,770
之前的引擎和alphago ，但是你 

767
00:39:42,000 --> 00:39:45,690
也许可以 猜测有什么 

768
00:39:43,770 --> 00:39:48,870
与alphago 诗歌的区别

769
00:39:45,690 --> 00:39:50,250
以前的方法他们使用神经

770
00:39:48,870 --> 00:39:55,230
网络 

771
00:39:50,250 --> 00:39:57,630
作为直觉引用 - 不 引用 - 什么 

772
00:39:55,230 --> 00:40:06,240
好的状态 是好的 

773
00:39:57,630 --> 00:40:09,330
下一个董事会的位置，以探索和

774
00:40:06,240 --> 00:40:13,770
关键的事情再次 伎俩让所有的 

775
00:40:09,330 --> 00:40:16,470
使 alphago无效的差异

776
00:40:13,770 --> 00:40:19,370
并且工作比alphago好多了 

777
00:40:16,470 --> 00:40:23,120
首先是因为没有专家比赛 

778
00:40:19,370 --> 00:40:23,120
而不是人类游戏 

779
00:40:23,240 --> 00:40:32,160
alphago 使用了 同样的蒙特卡罗 

780
00:40:28,980 --> 00:40:34,800
树搜索算法 MCTS做了一个 

781
00:40:32,160 --> 00:40:36,720
智能前瞻基于 

782
00:40:34,800 --> 00:40:40,260
神经网络预测鸽子的位置 

783
00:40:36,720 --> 00:40:42,690
好的国家接受它检查 

784
00:40:40,260 --> 00:40:47,310
而不是人类专家发挥它检查 

785
00:40:42,690 --> 00:40:50,550
这些国家确实有多好 

786
00:40:47,310 --> 00:40:52,790
简单的预见行动，做到了

787
00:40:50,550 --> 00:40:54,870
实现目标的基本事实 

788
00:40:52,790 --> 00:40:56,760
产生损失的修正 

789
00:40:54,870 --> 00:40:59,100
功能第二部分是

790
00:40:56,760 --> 00:41:01,910
多任务学习现在称之为什么 

791
00:40:59,100 --> 00:41:04,830
多任务学习是网络工作者 

792
00:41:01,910 --> 00:41:06,810
从某种意义上来说， 引用 - 不引用 双头 

793
00:41:04,830 --> 00:41:09,030
首先它输出概率

794
00:41:06,810 --> 00:41:11,070
采取明显的事情和移动 

795
00:41:09,030 --> 00:41:13,530
它也产生了概率 

796
00:41:11,070 --> 00:41:15,170
获奖并有一些方法 来 

797
00:41:13,530 --> 00:41:18,450
结合这些信息和 

798
00:41:15,170 --> 00:41:21,450
不断培养的两个部分

799
00:41:18,450 --> 00:41:23,280
网络取决于 所采取的选择 

800
00:41:21,450 --> 00:41:26,040
你想在这里做出最好的选择

801
00:41:23,280 --> 00:41:28,260
短期并实现立场 

802
00:41:26,040 --> 00:41:30,720
这是一个非常轻微的引擎盖 

803
00:41:28,260 --> 00:41:37,230
赢得了轮到谁 的球员 

804
00:41:30,720 --> 00:41:40,260
它是另一个重要的一步就是他们 

805
00:41:37,230 --> 00:41:42,240
从2015年更新了 最新的 

806
00:41:40,260 --> 00:41:44,730
最先进的 建筑 

807
00:41:42,240 --> 00:41:48,180
现在是一个 imagenet 的架构 

808
00:41:44,730 --> 00:41:50,760
作为残差网络 RESNET 为 

809
00:41:48,180 --> 00:41:53,670
imagenet那就是它 

810
00:41:50,760 --> 00:41:56,700
而那些微小的变化使所有人 

811
00:41:53,670 --> 00:41:58,860
差异使我们 带到 深 

812
00:41:56,700 --> 00:42:01,490
交通和 八十亿小时 

813
00:41:58,860 --> 00:42:01,490
堵在路上

814
00:42:02,220 --> 00:42:08,790
美国的消遣， 所以我们试图 

815
00:42:04,200 --> 00:42:12,359
模拟驾驶那个行为层 

816
00:42:08,790 --> 00:42:14,880
驾驶所以不是直接控制不是 

817
00:42:12,359 --> 00:42:17,970
运动规划，但超越 了 

818
00:42:14,880 --> 00:42:21,330
在 这些控制决策 之上 

819
00:42:17,970 --> 00:42:22,740
人类可解释的决定 

820
00:42:21,330 --> 00:42:26,550
改变加速减速的车道

821
00:42:22,740 --> 00:42:28,170
在微流量中建模 

822
00:42:26,550 --> 00:42:30,480
在中国流行的模拟框架

823
00:42:28,170 --> 00:42:34,349
交通工程所示的那种 

824
00:42:30,480 --> 00:42:36,480
在这里我们应用深层加固

825
00:42:34,349 --> 00:42:39,150
学习， 我会称之为 深刻 

826
00:42:36,480 --> 00:42:41,460
交通的目标是 实现 目标 

827
00:42:39,150 --> 00:42:44,180
长期 最高的 平均速度 

828
00:42:41,460 --> 00:42:47,670
时间编织进出交通 

829
00:42:44,180 --> 00:42:49,650
这里学生的要求是 

830
00:42:47,670 --> 00:42:55,680
按照教程并达到速度

831
00:42:49,650 --> 00:42:58,890
每小时65英里，如果你真的 

832
00:42:55,680 --> 00:43:02,390
希望达到超过70英里的速度 

833
00:42:58,890 --> 00:43:06,390
小时，这是 赢得和获得的 

834
00:43:02,390 --> 00:43:10,230
也许是上传自己的图片 ，使 

835
00:43:06,390 --> 00:43:13,170
确定你看起来很好，做到了你 

836
00:43:10,230 --> 00:43:17,550
应该做清楚的竞争指示

837
00:43:13,170 --> 00:43:19,230
阅读您 可以更改的教程

838
00:43:17,550 --> 00:43:21,900
在该代码框中参数

839
00:43:19,230 --> 00:43:24,450
网站汽车完成 你的大小强大的爸爸 

840
00:43:21,900 --> 00:43:26,970
深度流量点击白色按钮 

841
00:43:24,450 --> 00:43:28,410
说应用代码应用代码

842
00:43:26,970 --> 00:43:31,470
你写这些是参数 

843
00:43:28,410 --> 00:43:34,140
您指定 的 ，那么你就网络信息技术 

844
00:43:31,470 --> 00:43:36,300
应用那些参数创建

845
00:43:34,140 --> 00:43:39,000
你指定的架构，现在你 

846
00:43:36,300 --> 00:43:40,380
有一个用JavaScript编写的网络 

847
00:43:39,000 --> 00:43:43,619
生活在浏览器中准备好了 

848
00:43:40,380 --> 00:43:46,770
训练然后你点击蓝色 按钮 

849
00:43:43,619 --> 00:43:50,310
那说 训练和训练 

850
00:43:46,770 --> 00:43:52,290
网络比 一个人 快得多 

851
00:43:50,310 --> 00:43:56,369
实际上是在浏览器中可视化 

852
00:43:52,290 --> 00:43:58,470
通过 不断发展的 快 1000 倍 

853
00:43:56,369 --> 00:44:00,180
游戏制作决策参与网格 

854
00:43:58,470 --> 00:44:02,550
我将在 这里谈论空间

855
00:44:00,180 --> 00:44:05,640
第二，限速是80英里 

856
00:44:02,550 --> 00:44:08,220
小时根据各种调整 

857
00:44:05,640 --> 00:44:09,930
这场比赛 达到了80英里 

858
00:44:08,220 --> 00:44:13,109
一个小时肯定是不可能的 

859
00:44:09,930 --> 00:44:15,210
平均并达到一些速度 

860
00:44:13,109 --> 00:44:18,050
我们去年取得的成就 

861
00:44:15,210 --> 00:44:20,460
这要困难得多

862
00:44:18,050 --> 00:44:24,060
终于，当你快乐 的时候 

863
00:44:20,460 --> 00:44:28,109
培训完成后 提交 模型 

864
00:44:24,060 --> 00:44:30,390
竞争那些超级渴望的人

865
00:44:28,109 --> 00:44:34,410
敬业的学生， 你可以做到每一个 

866
00:44:30,390 --> 00:44:39,089
五分钟， 并想象 你的 

867
00:44:34,410 --> 00:44:41,310
提交您可以 单击该请求 

868
00:44:39,089 --> 00:44:45,540
可视化指定自定义 

869
00:44:41,310 --> 00:44:47,880
图像和颜色没关系 

870
00:44:45,540 --> 00:44:48,890
所以这是模拟速度限制 80 

871
00:44:47,880 --> 00:44:52,589
一小时一英里

872
00:44:48,890 --> 00:44:54,900
屏幕上的汽车20其中一个 是 

873
00:44:52,589 --> 00:44:57,359
在这种情况下，红色的是那个 

874
00:44:54,900 --> 00:44:59,220
由 神经网络控制其速度 

875
00:44:57,359 --> 00:45:03,240
它允许加速的行动 

876
00:44:59,220 --> 00:45:10,190
减慢变化车道 左右或 

877
00:45:03,240 --> 00:45:13,950
和其他车完全一样 

878
00:45:10,190 --> 00:45:16,200
相当愚蠢， 他们加速减速转弯 

879
00:45:13,950 --> 00:45:19,640
向左， 但他们没有目的 

880
00:45:16,200 --> 00:45:21,869
在他们的存在中 他们是随机的 

881
00:45:19,640 --> 00:45:25,080
或者至少目的还没有

882
00:45:21,869 --> 00:45:28,950
发现了汽车的速度之路 

883
00:45:25,080 --> 00:45:32,520
这条路是 一个占用 的网格空间 

884
00:45:28,950 --> 00:45:40,410
指定何时为空的网格

885
00:45:32,520 --> 00:45:43,619
它被设置为B意味着 

886
00:45:40,410 --> 00:45:45,930
网格值是什么速度 

887
00:45:43,619 --> 00:45:48,270
如果你 在那个 网格 里面就可以实现

888
00:45:45,930 --> 00:45:51,030
当有其他汽车的时候 

889
00:45:48,270 --> 00:45:53,430
这个网格中的值变慢了 

890
00:45:51,030 --> 00:45:55,560
这辆车的速度就是国家空间 

891
00:45:53,430 --> 00:45:57,990
这是国家代表和你 

892
00:45:55,560 --> 00:46:00,060
可以选择多少切片 

893
00:45:57,990 --> 00:46:06,480
你接受的状态空间就是 输入 

894
00:46:00,060 --> 00:46:08,700
到神经网络的 视觉 

895
00:46:06,480 --> 00:46:11,700
亚洲目的你可以选择 正常 

896
00:46:08,700 --> 00:46:15,840
观看速度或速度快

897
00:46:11,700 --> 00:46:18,060
网络运行，并有显示 

898
00:46:15,840 --> 00:46:19,950
帮助您建立直觉的选项

899
00:46:18,060 --> 00:46:22,170
关于网络发生在什么 

900
00:46:19,950 --> 00:46:25,050
空间车在运行

901
00:46:22,170 --> 00:46:27,810
默认是没有添加额外信息 

902
00:46:25,050 --> 00:46:30,330
再有就是学习用输入 其 

903
00:46:27,810 --> 00:46:32,970
准确地想象出哪个部分

904
00:46:30,330 --> 00:46:36,090
道路是作为输入 

905
00:46:32,970 --> 00:46:37,710
网络然后有 安全系统 

906
00:46:36,090 --> 00:46:39,990
我将在 一点点 描述 

907
00:46:37,710 --> 00:46:42,150
这是道路的所有部分 

908
00:46:39,990 --> 00:46:44,040
汽车是不允许 进入的，因为它 

909
00:46:42,150 --> 00:46:45,390
会导致碰撞 

910
00:46:44,040 --> 00:46:50,430
使用JavaScript会非常困难 

911
00:46:45,390 --> 00:46:52,260
动画和完整的地图这里是一个 

912
00:46:50,430 --> 00:46:57,030
安全系统， 你可以 想到这一点 

913
00:46:52,260 --> 00:46:59,540
系统作为 CC基本雷达超声 

914
00:46:57,030 --> 00:47:02,220
传感器帮助您避免明显的 

915
00:46:59,540 --> 00:47:04,530
碰撞到 明显检测 

916
00:47:02,220 --> 00:47:06,780
你周围的物体和它 的 任务 

917
00:47:04,530 --> 00:47:11,369
钢网的红色车是移动 

918
00:47:06,780 --> 00:47:13,530
这个空间是 将 有关 

919
00:47:11,369 --> 00:47:17,760
在约束下的空间

920
00:47:13,530 --> 00:47:19,470
安全系统 红色显示所有 

921
00:47:17,760 --> 00:47:23,369
网格的部件是无法 移动 

922
00:47:19,470 --> 00:47:26,670
因此， 汽车 的目标 是不 

923
00:47:23,369 --> 00:47:32,090
陷入交通堵塞它变大了 

924
00:47:26,670 --> 00:47:32,090
清扫运动，以避免汽车的人群

925
00:47:32,119 --> 00:47:38,220
像DQ n这样的输入是状态空间 

926
00:47:35,940 --> 00:47:41,190
输出是不同的值 

927
00:47:38,220 --> 00:47:44,420
行动并基于epsilon 

928
00:47:41,190 --> 00:47:47,970
参数通过培训和通过 

929
00:47:44,420 --> 00:47:50,130
您选择的推理评估过程

930
00:47:47,970 --> 00:47:53,040
你想做多少探索 

931
00:47:50,130 --> 00:47:57,890
这些都是学习的参数 

932
00:47:53,040 --> 00:47:57,890
在 您 自己的计算机 上的浏览器中完成

933
00:47:58,670 --> 00:48:06,180
仅利用CPU的动作空间 

934
00:48:03,480 --> 00:48:07,950
有五个给你一些 

935
00:48:06,180 --> 00:48:10,980
这里的变量也许你会回去 

936
00:48:07,950 --> 00:48:14,400
幻灯片来 看看它的大脑报价 

937
00:48:10,980 --> 00:48:18,170
unquote 是接受的东西 

938
00:48:14,400 --> 00:48:19,730
国家和奖励需要四个 

939
00:48:18,170 --> 00:48:22,730
通过 国家 并生产 到 

940
00:48:19,730 --> 00:48:25,070
大脑的下一步行动在哪里 

941
00:48:22,730 --> 00:48:28,250
神经网络包含两者 

942
00:48:25,070 --> 00:48:32,600
培训和评估 学习 

943
00:48:28,250 --> 00:48:35,090
输入可以向前控制​​宽度

944
00:48:32,600 --> 00:48:37,220
长度和后退长度 车道侧 

945
00:48:35,090 --> 00:48:39,980
你看到的 那条 车道的数量

946
00:48:37,220 --> 00:48:42,440
前面的补丁作为前面的补丁

947
00:48:39,980 --> 00:48:47,180
你看到补丁背后的补丁 

948
00:48:42,440 --> 00:48:50,000
你看到mu 今年可以控制了 

949
00:48:47,180 --> 00:48:53,890
受控制 的代理数量 

950
00:48:50,000 --> 00:49:00,080
神经网络从一个到另一个 

951
00:48:53,890 --> 00:49:02,210
十是 进行 评价 

952
00:49:00,080 --> 00:49:05,110
完全一样， 你必须达到 

953
00:49:02,210 --> 00:49:08,450
最高平均速度为 代理商 

954
00:49:05,110 --> 00:49:12,470
这里非常关键的是 

955
00:49:08,450 --> 00:49:17,120
代理人不知道对方如此 

956
00:49:12,470 --> 00:49:20,530
他们没有联合策划

957
00:49:17,120 --> 00:49:23,600
网络是在联合下训练的

958
00:49:20,530 --> 00:49:24,530
达到平均速度的目标 

959
00:49:23,600 --> 00:49:27,350
对于 他们所有人 

960
00:49:24,530 --> 00:49:30,800
但行动贪婪 

961
00:49:27,350 --> 00:49:33,320
每一路这是非常有趣的东西 

962
00:49:30,800 --> 00:49:35,810
因为这可以用这种方式 学习 

963
00:49:33,320 --> 00:49:38,180
各种方法可以扩展到 

964
00:49:35,810 --> 00:49:41,240
任意数量的汽车，你可以 

965
00:49:38,180 --> 00:49:44,840
想象我们把最好的汽车扔下来

966
00:49:41,240 --> 00:49:47,960
从这个班级一起并拥有它们 

967
00:49:44,840 --> 00:49:52,190
以这种方式竞争最好的神经 

968
00:49:47,960 --> 00:49:55,550
网络，因为他们在他们的 

969
00:49:52,190 --> 00:49:57,920
贪吃操作的网络数量

970
00:49:55,550 --> 00:50:01,070
可以 同时运作的是完全的 

971
00:49:57,920 --> 00:50:07,910
可扩展的有很多 参数 

972
00:50:01,070 --> 00:50:10,250
所述 时间窗口 中的层中的许多 

973
00:50:07,910 --> 00:50:11,960
可以在 这里 添加的图层类型是 a 

974
00:50:10,250 --> 00:50:14,240
完全连接的图层与任期

975
00:50:11,960 --> 00:50:16,820
激活功能所有这些 

976
00:50:14,240 --> 00:50:20,360
事物可以按照规定进行定制 

977
00:50:16,820 --> 00:50:23,800
教程最后一层完全 

978
00:50:20,360 --> 00:50:26,570
连接层与输出 五 

979
00:50:23,800 --> 00:50:29,720
回归给出的每一个的值 

980
00:50:26,570 --> 00:50:31,760
五个动作，还有很多 

981
00:50:29,720 --> 00:50:32,260
更具体的参数 其中的 一些 

982
00:50:31,760 --> 00:50:39,400
拥有这个 

983
00:50:32,260 --> 00:50:43,540
只是从伽马到 epsilon体验 

984
00:50:39,400 --> 00:50:47,950
在时间上重放大小到学习率 

985
00:50:43,540 --> 00:50:51,430
窗口优化器的学习率 

986
00:50:47,950 --> 00:50:54,130
动量批量大小l2 l1到K为 

987
00:50:51,430 --> 00:50:56,140
正规化等等有一个很大的问题 

988
00:50:54,130 --> 00:50:58,630
应用代码的白色按钮

989
00:50:56,140 --> 00:51:00,370
你按下它会杀死你所有的工作

990
00:50:58,630 --> 00:51:02,680
做到这一点 所以 要小心 

991
00:51:00,370 --> 00:51:06,220
这样做应该 只在 

992
00:51:02,680 --> 00:51:08,230
如果你碰巧，那一开始就是 

993
00:51:06,220 --> 00:51:10,360
让你的电脑在 训练中 运行 

994
00:51:08,230 --> 00:51:13,810
人们做 了 几天 

995
00:51:10,360 --> 00:51:15,490
你按下的蓝色训练按钮 

996
00:51:13,810 --> 00:51:18,550
它根据您的参数进行训练

997
00:51:15,490 --> 00:51:20,290
指定和网络状态 

998
00:51:18,550 --> 00:51:22,270
从时间运到主模拟

999
00:51:20,290 --> 00:51:24,280
时间所以你 看到 的东西 

1000
00:51:22,270 --> 00:51:26,410
浏览器，因为你打开了网站是 

1001
00:51:24,280 --> 00:51:29,080
然后运行相同的网络 

1002
00:51:26,410 --> 00:51:30,610
经过培训并定期更新 

1003
00:51:29,080 --> 00:51:32,590
那个网络 所以它变得 更好了 

1004
00:51:30,610 --> 00:51:35,440
即使 培训需要数周， 也会更好

1005
00:51:32,590 --> 00:51:37,510
为你 而不断更新 

1006
00:51:35,440 --> 00:51:39,550
你在左边看到的网络如果是的话 

1007
00:51:37,510 --> 00:51:41,850
你正在训练的网络汽车

1008
00:51:39,550 --> 00:51:46,450
只是站在 原地而不动 

1009
00:51:41,850 --> 00:51:48,880
这可能 是重新开始和改变的时候了 

1010
00:51:46,450 --> 00:51:52,210
参数可能会添加 几层 

1011
00:51:48,880 --> 00:51:54,340
你的网络迭代次数 是 

1012
00:51:52,210 --> 00:51:58,090
肯定是一个重要的参数

1013
00:51:54,340 --> 00:52:00,190
控制和评估 是一回事 

1014
00:51:58,090 --> 00:52:02,860
自从 上次 以来我们已经做了很多工作 

1015
00:52:00,190 --> 00:52:07,270
一年去除随机性 程度 

1016
00:52:02,860 --> 00:52:09,160
删除提交的动机 

1017
00:52:07,270 --> 00:52:11,890
相同的代码一遍又一遍 ，以 

1018
00:52:09,160 --> 00:52:15,790
希望能产生较高的回报更高 

1019
00:52:11,890 --> 00:52:17,770
评价得分的方法 

1020
00:52:15,790 --> 00:52:23,410
评估是我们收集的平均值

1021
00:52:17,770 --> 00:52:27,580
速度超过 10次运行 约45 秒 

1022
00:52:23,410 --> 00:52:30,640
游戏每个不是分钟45模拟 

1023
00:52:27,580 --> 00:52:32,380
秒，有五百 

1024
00:52:30,640 --> 00:52:35,680
那些和我们采取的中间速度

1025
00:52:32,380 --> 00:52:37,930
500个运行它做服务器端 ，从而 

1026
00:52:35,680 --> 00:52:42,100
我非常难以作弊 

1027
00:52:37,930 --> 00:52:43,930
尝试你可以尝试在当地有一个 

1028
00:52:42,100 --> 00:52:45,460
开始评估运行，但那一个 

1029
00:52:43,930 --> 00:52:46,040
不计算 那 只是为了你 

1030
00:52:45,460 --> 00:52:49,310
感觉好多了 

1031
00:52:46,040 --> 00:52:51,080
通过你的网络那是应该的 

1032
00:52:49,310 --> 00:52:52,810
产生结果那是非常相似 的 

1033
00:52:51,080 --> 00:52:56,810
我们在服务器上生成的那个 

1034
00:52:52,810 --> 00:52:58,340
这是建立自己的直觉 和 

1035
00:52:56,810 --> 00:53:01,730
我说我们大大减少了 

1036
00:52:58,340 --> 00:53:03,770
随机性的影响因此 得分 

1037
00:53:01,730 --> 00:53:06,770
你获得网络的速度

1038
00:53:03,770 --> 00:53:10,880
设计应该与每个人非常相似 

1039
00:53:06,770 --> 00:53:12,710
如果是，则估值加载正在保存

1040
00:53:10,880 --> 00:53:14,510
网络很大，你想切换 

1041
00:53:12,710 --> 00:53:16,040
电脑你可以保存网络吧 

1042
00:53:14,510 --> 00:53:18,260
保存了两者的架构

1043
00:53:16,040 --> 00:53:22,540
网络和权重和 

1044
00:53:18,260 --> 00:53:26,560
网络， 你可以加载它 

1045
00:53:22,540 --> 00:53:28,760
显然， 当你 加载它不是 

1046
00:53:26,560 --> 00:53:31,280
保存您已经拥有的 任何 数据 

1047
00:53:28,760 --> 00:53:35,060
做完你不能做转移学习 

1048
00:53:31,280 --> 00:53:37,400
浏览器中的javascript 还在提交 

1049
00:53:35,060 --> 00:53:39,440
您的网络提交竞争模型

1050
00:53:37,400 --> 00:53:42,740
并确保你先进行训练 

1051
00:53:39,440 --> 00:53:44,570
否则它将开始通往 

1052
00:53:42,740 --> 00:53:44,930
随机 启动 ， 并 会不会这样做 

1053
00:53:44,570 --> 00:53:47,510
好

1054
00:53:44,930 --> 00:53:50,210
你可以重新提交我们， 你喜欢和 

1055
00:53:47,510 --> 00:53:52,160
最高分是重要的 

1056
00:53:50,210 --> 00:53:55,310
最酷的部分是你可以加载你的自定义 

1057
00:53:52,160 --> 00:53:59,840
图像指定颜色并请求

1058
00:53:55,310 --> 00:54:01,460
可视化，我们还没有表现出对 

1059
00:53:59,840 --> 00:54:04,220
可视化， 但我向你保证 

1060
00:54:01,460 --> 00:54:06,170
会再次看起来很棒了 

1061
00:54:04,220 --> 00:54:08,620
教程更改了参数 

1062
00:54:06,170 --> 00:54:11,060
代码框点击应用代码运行培训 

1063
00:54:08,620 --> 00:54:13,910
在回家的路上，这个房间里的每个人

1064
00:54:11,060 --> 00:54:15,500
在火车上希望 不在你的车里 

1065
00:54:13,910 --> 00:54:18,050
应该能够在浏览器中执行此操作 

1066
00:54:15,500 --> 00:54:19,490
然后 你可以看到 请求 

1067
00:54:18,050 --> 00:54:22,550
可视化，因为它是昂贵的 

1068
00:54:19,490 --> 00:54:27,310
过程 你必须要 我们这样做 

1069
00:54:22,550 --> 00:54:27,310
因为我们必须在服务器端运行 

1070
00:54:27,760 --> 00:54:34,190
竞争环节是github的首发 

1071
00:54:31,820 --> 00:54:35,810
代码是存在的，细节的那些 

1072
00:54:34,190 --> 00:54:39,950
真正 想赢 是在存档 

1073
00:54:35,810 --> 00:54:41,600
纸这样的问题 就会出现 

1074
00:54:39,950 --> 00:54:44,120
始终是这些

1075
00:54:41,600 --> 00:54:46,940
强化学习方法是在

1076
00:54:44,120 --> 00:54:51,560
全部或更确切地说是行动计划控制

1077
00:54:46,940 --> 00:54:54,500
很容易在学习中学习 

1078
00:54:51,560 --> 00:54:56,920
开车的情况我们不能做到这一点 

1079
00:54:54,500 --> 00:54:59,560
我们能做到零 

1080
00:54:56,920 --> 00:55:02,650
从自我发挥中学习

1081
00:54:59,560 --> 00:55:07,210
因为这 将导致 数百万 

1082
00:55:02,650 --> 00:55:09,850
崩溃是为了学会避免 

1083
00:55:07,210 --> 00:55:12,250
除非我们像 我们 一样工作，否则会崩溃

1084
00:55:09,850 --> 00:55:15,040
RC车上的严重撞击或我们是

1085
00:55:12,250 --> 00:55:16,870
我们可以在模拟中工作

1086
00:55:15,040 --> 00:55:18,490
在出口数据我们可以看看 司机 

1087
00:55:16,870 --> 00:55:20,410
我们有很多并且学习的数据 

1088
00:55:18,490 --> 00:55:24,730
这是一个悬而未决的问题 

1089
00:55:20,410 --> 00:55:26,260
适用于 约会，我会提出 来 

1090
00:55:24,730 --> 00:55:30,250
两家公司，因为他们都是客人 

1091
00:55:26,260 --> 00:55:32,560
扬声器深IRL不参与 

1092
00:55:30,250 --> 00:55:36,700
最成功的机器人在 

1093
00:55:32,560 --> 00:55:42,280
波士顿的现实世界

1094
00:55:36,700 --> 00:55:45,910
动力学大部分是感知控制 

1095
00:55:42,280 --> 00:55:49,000
并且像这个机器人那样的计划没有 

1096
00:55:45,910 --> 00:55:52,260
涉及学习方法，除了 

1097
00:55:49,000 --> 00:55:56,320
感知方面的最小增加 

1098
00:55:52,260 --> 00:55:59,050
最好的知识，当然 

1099
00:55:56,320 --> 00:56:01,360
作为演讲者， 魏薇也是如此 

1100
00:55:59,050 --> 00:56:03,310
周五将谈论深度学习 

1101
00:56:01,360 --> 00:56:06,100
在感知上使用了一点点

1102
00:56:03,310 --> 00:56:10,000
顶部但大部分工作都是从中完成的 

1103
00:56:06,100 --> 00:56:12,520
传感器和优化 基础 

1104
00:56:10,000 --> 00:56:14,410
基于模型的方法轨迹 

1105
00:56:12,520 --> 00:56:17,440
生成和优化哪个 

1106
00:56:14,410 --> 00:56:23,100
轨迹轨迹最好 避免 

1107
00:56:17,440 --> 00:56:25,140
碰撞深度IRL没有涉及和 

1108
00:56:23,100 --> 00:56:27,760
又回来了 

1109
00:56:25,140 --> 00:56:30,010
意想不到的本地POC很高 

1110
00:56:27,760 --> 00:56:31,860
在所有这些中出现的 奖励 

1111
00:56:30,010 --> 00:56:35,680
情况并适用于现实 世界 

1112
00:56:31,860 --> 00:56:37,630
所以对于非常短的猫视频 

1113
00:56:35,680 --> 00:56:39,880
猫在敲钟的地方 

1114
00:56:37,630 --> 00:56:46,060
他们正在学习那个戒指

1115
00:56:39,880 --> 00:56:48,190
贝尔正在寻求食物，我恳请 你 

1116
00:56:46,060 --> 00:56:51,220
想想如何发展过来 

1117
00:56:48,190 --> 00:56:54,040
他们可能不会 以意想不到的方式 来 

1118
00:56:51,220 --> 00:56:58,060
在决赛中有一个理想的效果 

1119
00:56:54,040 --> 00:57:01,920
奖励是以食物和食物的形式 

1120
00:56:58,060 --> 00:57:01,920
预期的效果是响铃

1121
00:57:02,580 --> 00:57:06,720
那是

1122
00:57:03,810 --> 00:57:08,550
ASAT为人工将军提供服务

1123
00:57:06,720 --> 00:57:11,670
在两周的情报课程， 

1124
00:57:08,550 --> 00:57:15,620
有些东西会广泛探索它 

1125
00:57:11,670 --> 00:57:18,930
这些强化学习如何

1126
00:57:15,620 --> 00:57:22,260
规划算法将以 各种方式 发展 

1127
00:57:18,930 --> 00:57:24,840
他们没有被期待 ，我们怎么可能 

1128
00:57:22,260 --> 00:57:28,640
限制他们如何设计奖励 

1129
00:57:24,840 --> 00:57:31,590
导致安全操作的功能

1130
00:57:28,640 --> 00:57:35,100
所以我鼓励你来 谈谈 

1131
00:57:31,590 --> 00:57:38,400
星期五下午 1点作为 提醒 

1132
00:57:35,100 --> 00:57:40,980
Stata 32中的 下午1:00而非下午7:00 

1133
00:57:38,400 --> 00:57:43,860
两个三和两个令人敬畏的谈话 

1134
00:57:40,980 --> 00:57:47,430
两周从波士顿动力学 到雷 

1135
00:57:43,860 --> 00:57:49,830
Kurzweil 等 明天就AGI而来 

1136
00:57:47,430 --> 00:57:52,780
我们将谈论计算机 视觉和 

1137
00:57:49,830 --> 00:57:56,589
psyche fuse 谢谢大家 

1138
00:57:52,780 --> 00:57:56,589
[掌声] 

