1
00:00:00,000 --> 00:00:02,700
行。所以，我们已经谈过了

2
00:00:04,520 --> 00:00:06,140
常规神经网络， 

3
00:00:06,380 --> 00:00:08,240
完全连接的神经网络， 

4
00:00:08,280 --> 00:00:11,120
我们已经讨论过卷积神经网络

5
00:00:11,160 --> 00:00:12,640
使用图像， 

6
00:00:12,700 --> 00:00:14,580
我们谈到过强化， 

7
00:00:14,620 --> 00:00:16,360
更深入的强化学习， 

8
00:00:16,480 --> 00:00:18,860
我们插入神经网络的地方

9
00:00:18,940 --> 00:00:22,060
到强化学习算法， 

10
00:00:23,380 --> 00:00:25,500
当一个系统不仅要

11
00:00:25,560 --> 00:00:27,720
感知世界，但也在其中行动， 

12
00:00:27,860 --> 00:00:29,460
并收取奖励。 

13
00:00:29,500 --> 00:00:31,080
今天我们来谈谈， 

14
00:00:32,880 --> 00:00:34,620
也许是最不了解的

15
00:00:34,660 --> 00:00:37,880
但那里最激动人心的神经网络， 

16
00:00:38,880 --> 00:00:42,460
神经网络的味道，是递归神经网络。 

17
00:00:44,300 --> 00:00:46,460
但首先，对于行政人员， 

18
00:00:48,180 --> 00:00:50,840
有一个网站。我不知道你是否听说过， 

19
00:00:50,840 --> 00:00:52,240
cars.mit.edu， 

20
00:00:52,280 --> 00:00:55,450
如果您是注册学生，您应该在哪里创建一个帐户， 

21
00:00:55,470 --> 00:00:57,160
这是要求之一。 

22
00:00:57,160 --> 00:00:58,540
你需要一个帐户

23
00:00:58,540 --> 00:01:00,540
如果你想得到这个， 

24
00:01:00,540 --> 00:01:02,540
你需要提交代码

25
00:01:02,540 --> 00:01:06,240
对于DeepTrafficJS和DeepTeslaJS， 

26
00:01:06,240 --> 00:01:08,020
对于DeepTraffic， 

27
00:01:08,020 --> 00:01:11,920
你必须有一个驱动速度超过65英里每小时的神经网络。 

28
00:01:12,360 --> 00:01:14,980
如果您需要帮助来达到这个速度

29
00:01:14,980 --> 00:01:16,220
请发电子邮件给我们。 

30
00:01:17,840 --> 00:01:19,660
我们可以给你一些提示。 

31
00:01:20,640 --> 00:01:23,020
对于那些是老派SNL粉丝的人， 

32
00:01:23,020 --> 00:01:25,840
现在有深度思考部分， 

33
00:01:27,380 --> 00:01:29,100
在个人资料页面中， 

34
00:01:29,100 --> 00:01:31,340
我们鼓励你谈谈的地方

35
00:01:31,340 --> 00:01:34,160
您在DeepTraffic中尝试过的各种事物

36
00:01:34,160 --> 00:01:35,360
或任何其他的

37
00:01:35,700 --> 00:01:39,140
DeepTesla或您完成的任何工作

38
00:01:39,540 --> 00:01:42,020
作为DeepLearning课程的一部分。 

39
00:01:43,080 --> 00:01:43,780
好的， 

40
00:01:45,100 --> 00:01:46,820
我们已经谈过了

41
00:01:46,820 --> 00:01:49,560
左边的香草神经网络。 

42
00:01:49,560 --> 00:01:51,320
香草神经网络

43
00:01:51,320 --> 00:01:54,000
是它计算的那个

44
00:01:54,000 --> 00:01:56,820
近似于从一个输入映射的函数

45
00:01:57,180 --> 00:01:59,170
到一个输出。 

46
00:01:59,220 --> 00:02:01,720
一个例子是映射图像

47
00:02:01,750 --> 00:02:03,830
到图像中显示的数字。 

48
00:02:03,940 --> 00:02:05,020
对于ImageNet 

49
00:02:05,020 --> 00:02:06,480
正在映射图像

50
00:02:06,480 --> 00:02:08,480
什么是图像中的对象。 

51
00:02:08,480 --> 00:02:10,020
它可以是任何东西。 

52
00:02:10,020 --> 00:02:10,780
事实上， 

53
00:02:11,020 --> 00:02:13,940
卷积神经网络可以对音频进行操作， 

54
00:02:13,950 --> 00:02:17,210
你可以给它一大块音频，一个五秒音频剪辑， 

55
00:02:17,660 --> 00:02:19,800
仍然算作一个输入

56
00:02:19,800 --> 00:02:21,060
因为它是固定大小的。 

57
00:02:21,060 --> 00:02:23,400
只要输入的大小是固定的， 

58
00:02:23,400 --> 00:02:27,240
这是一大块投入

59
00:02:27,600 --> 00:02:29,600
只要你有基本的事实

60
00:02:29,600 --> 00:02:32,320
将该输入块映射到某个输出

61
00:02:32,320 --> 00:02:33,400
事实上， 

62
00:02:33,990 --> 00:02:36,030
那就是香草神经网络。 

63
00:02:36,100 --> 00:02:38,820
是否有完全连接的神经网络

64
00:02:38,840 --> 00:02:40,900
或卷积神经网络。 

65
00:02:41,200 --> 00:02:44,760
今天我们将谈论惊人的， 

66
00:02:45,260 --> 00:02:47,720
神秘的回归神经网络。 

67
00:02:47,720 --> 00:02:49,720
他们计算功能

68
00:02:50,140 --> 00:02:52,080
从一个到多个， 

69
00:02:52,400 --> 00:02:53,840
从多到一， 

70
00:02:53,840 --> 00:02:55,200
从很多到很多。 

71
00:02:59,320 --> 00:03:00,640
也是双向的。 

72
00:03:01,360 --> 00:03:02,440
那是什么意思？ 

73
00:03:02,720 --> 00:03:05,280
他们接受输入序列， 

74
00:03:05,700 --> 00:03:07,360
时间序列， 

75
00:03:07,360 --> 00:03:08,260
音频， 

76
00:03:09,120 --> 00:03:10,260
视频， 

77
00:03:10,260 --> 00:03:12,480
只要有一系列数据， 

78
00:03:12,480 --> 00:03:14,480
和时间动态

79
00:03:14,580 --> 00:03:16,000
连接数据

80
00:03:16,000 --> 00:03:18,280
比空间更重要

81
00:03:20,060 --> 00:03:22,260
每个框架的内容。 

82
00:03:22,340 --> 00:03:24,500
所以，每当有很多信息时

83
00:03:24,500 --> 00:03:26,640
按顺序传达， 

84
00:03:27,300 --> 00:03:29,000
在时间变化

85
00:03:29,000 --> 00:03:30,660
无论那种类型的数据是什么， 

86
00:03:30,660 --> 00:03:33,600
那是你想要使用回归神经网络的时候

87
00:03:33,600 --> 00:03:35,600
喜欢说话， 

88
00:03:35,600 --> 00:03:36,980
自然语言， 

89
00:03:37,460 --> 00:03:38,240
音频

90
00:03:38,680 --> 00:03:40,060
和这个的力量

91
00:03:40,060 --> 00:03:41,740
对于他们中的许多人来说， 

92
00:03:41,740 --> 00:03:43,680
对于递归神经网络， 

93
00:03:43,680 --> 00:03:44,980
他们真正发光的地方， 

94
00:03:44,980 --> 00:03:49,200
当输入的大小是可变的， 

95
00:03:49,320 --> 00:03:51,200
所以你没有固定的数据块

96
00:03:51,200 --> 00:03:53,520
你输入的是可变输入。 

97
00:03:53,720 --> 00:03:55,980
输出也一样， 

98
00:03:56,620 --> 00:04:00,540
所以你可以给它一系列的演讲， 

99
00:04:01,520 --> 00:04:02,940
几秒钟的演讲

100
00:04:03,700 --> 00:04:05,160
然后输出是

101
00:04:06,380 --> 00:04:10,700
扬声器是男性还是女性的单一标签。 

102
00:04:11,340 --> 00:04:13,500
这是多对一的。 

103
00:04:14,960 --> 00:04:16,780
你也可以这样做

104
00:04:17,380 --> 00:04:18,660
很多很多。 

105
00:04:19,220 --> 00:04:20,220
翻译。 

106
00:04:20,740 --> 00:04:22,460
你可以拥有自然语言

107
00:04:22,460 --> 00:04:23,540
投入网络

108
00:04:25,400 --> 00:04:27,100
在西班牙语中

109
00:04:27,100 --> 00:04:29,180
输出是英文的。 

110
00:04:30,320 --> 00:04:31,520
机器翻译。 

111
00:04:31,520 --> 00:04:32,640
那是多对多的。 

112
00:04:33,100 --> 00:04:35,860
并且许多人不必这样做

113
00:04:35,860 --> 00:04:37,520
直接映射

114
00:04:37,520 --> 00:04:39,800
分成相同大小的序列。 

115
00:04:40,020 --> 00:04:42,660
对于视频，序列大小可能相同

116
00:04:42,680 --> 00:04:45,240
你放入了每一帧的标签

117
00:04:45,700 --> 00:04:47,340
一个五秒钟的剪辑

118
00:04:49,030 --> 00:04:50,810
打篮球的人

119
00:04:50,840 --> 00:04:52,860
你可以标记每一帧

120
00:04:52,860 --> 00:04:55,220
计算每一帧中的人数。 

121
00:04:55,220 --> 00:04:56,180
那是多对多的

122
00:04:56,180 --> 00:04:59,520
当输入的大小和输出的大小相同时

123
00:04:59,540 --> 00:05:00,460
是的，问题？ 

124
00:05:00,840 --> 00:05:01,720
问题是， 

125
00:05:01,800 --> 00:05:05,380
是否有任何型号的输出和输入有反馈？ 

126
00:05:05,380 --> 00:05:09,020
这正是回归神经网络的意义所在。 

127
00:05:09,880 --> 00:05:11,640
它产生输出， 

128
00:05:11,640 --> 00:05:13,900
并复制该输出

129
00:05:13,900 --> 00:05:15,140
并将其循环回来。 

130
00:05:18,160 --> 00:05:21,500
这几乎就是回归神经网络的定义。 

131
00:05:21,500 --> 00:05:24,360
那里有一个产生输出的循环

132
00:05:24,400 --> 00:05:27,320
并再次将该输出作为输入。 

133
00:05:29,560 --> 00:05:33,060
还有许多序列不对齐的地方。 

134
00:05:33,060 --> 00:05:34,680
像机器翻译一样

135
00:05:34,700 --> 00:05:36,770
输出序列的大小

136
00:05:36,780 --> 00:05:39,400
可能与输入序列完全不同。 

137
00:05:39,400 --> 00:05:41,910
我们将介绍很多很酷的应用程序; 

138
00:05:44,240 --> 00:05:45,600
你可以开一首歌

139
00:05:45,600 --> 00:05:48,960
学习特定歌曲的音频

140
00:05:48,960 --> 00:05:51,240
有回归神经网络

141
00:05:51,240 --> 00:05:55,120
在一段时间后继续播放这首歌。 

142
00:05:55,440 --> 00:05:57,840
因此它可以学习生成序列

143
00:05:57,900 --> 00:06:00,940
音频，自然语言，视频。 

144
00:06:01,120 --> 00:06:01,940
好的。 

145
00:06:04,120 --> 00:06:06,720
我知道我答应的方程式不多， 

146
00:06:06,720 --> 00:06:07,900
但这是

147
00:06:08,600 --> 00:06:11,080
非常简单

148
00:06:11,080 --> 00:06:13,340
我们必须涵盖反向传播。 

149
00:06:13,960 --> 00:06:15,900
这也是事情

150
00:06:15,900 --> 00:06:18,200
那，如果你有点懒

151
00:06:18,200 --> 00:06:19,860
你去互联网

152
00:06:19,860 --> 00:06:23,020
并开始使用TensorFlow的基本教程， 

153
00:06:23,020 --> 00:06:25,700
你忽略了反向传播的工作原理。 

154
00:06:25,700 --> 00:06:27,000
在你身上危险

155
00:06:27,380 --> 00:06:29,070
你认为它只是有效。 

156
00:06:29,100 --> 00:06:31,030
我给它一些输入，一些输出， 

157
00:06:31,050 --> 00:06:33,450
它就像我可以装配它们的乐高积木

158
00:06:33,460 --> 00:06:34,970
就像你可能已经完成了DeepTraffic 

159
00:06:34,980 --> 00:06:36,710
一堆层放在一起

160
00:06:36,740 --> 00:06:39,110
然后按下火车。 

161
00:06:39,430 --> 00:06:41,260
反向传播是机制

162
00:06:41,260 --> 00:06:43,140
那个神经网络目前 - 

163
00:06:43,140 --> 00:06:46,260
我们所知道的最佳机制用于培训。 

164
00:06:46,290 --> 00:06:47,480
所以你需要了解

165
00:06:49,460 --> 00:06:52,480
反向传播的简单力量， 

166
00:06:52,480 --> 00:06:54,480
但也有危险。 

167
00:06:57,500 --> 00:06:58,360
摘要， 

168
00:06:59,320 --> 00:07:02,460
我放在幻灯片的顶部，有一个输入

169
00:07:02,460 --> 00:07:04,380
对于一个图像的网络， 

170
00:07:04,380 --> 00:07:07,400
有一堆神经元， 

171
00:07:07,400 --> 00:07:09,180
一切都差别化

172
00:07:09,180 --> 00:07:12,620
每个神经元上的平滑激活功能， 

173
00:07:12,620 --> 00:07:13,520
然后， 

174
00:07:14,620 --> 00:07:18,280
当你通过这些激活函数时， 

175
00:07:18,280 --> 00:07:20,280
接受输入，传递它

176
00:07:20,280 --> 00:07:25,520
这个可区分的计算节点网， 

177
00:07:25,520 --> 00:07:27,040
你产生了一个输出。 

178
00:07:27,040 --> 00:07:28,460
在那个输出中

179
00:07:28,900 --> 00:07:30,760
你也有一个基本的事实， 

180
00:07:30,760 --> 00:07:32,900
正确的，真相

181
00:07:32,900 --> 00:07:33,920
你希望

182
00:07:33,920 --> 00:07:35,870
或者你期望网络生产。 

183
00:07:35,920 --> 00:07:38,280
你可以看看它们之间的差异

184
00:07:38,280 --> 00:07:40,160
网络实际产生了什么

185
00:07:40,180 --> 00:07:42,300
你希望它会产生什么， 

186
00:07:42,300 --> 00:07:43,460
这是一个错误。 

187
00:07:43,460 --> 00:07:46,280
然后你向后传播那个错误， 

188
00:07:46,280 --> 00:07:48,000
惩罚或奖励

189
00:07:50,680 --> 00:07:54,400
导致该输出的网络参数

190
00:07:57,160 --> 00:07:59,920
让我们从一个非常简单的例子开始吧。 

191
00:08:03,060 --> 00:08:04,200
有一个功能

192
00:08:04,960 --> 00:08:07,240
需要它的输入

193
00:08:07,240 --> 00:08:08,860
在顶部， 

194
00:08:09,200 --> 00:08:11,520
三个变量，X，Y和Z. 

195
00:08:12,300 --> 00:08:14,100
该功能做两件事： 

196
00:08:14,100 --> 00:08:15,940
它增加了X和Y. 

197
00:08:15,940 --> 00:08:17,300
然后它成倍增加

198
00:08:17,300 --> 00:08:19,900
Z的总和。 

199
00:08:20,420 --> 00:08:23,020
然后我们可以将其表述为电路， 

200
00:08:23,440 --> 00:08:24,720
门电路， 

201
00:08:25,680 --> 00:08:27,120
哪里有一个加号门， 

202
00:08:27,120 --> 00:08:29,120
和乘法门。 

203
00:08:36,140 --> 00:08:38,040
我们来一些投入， 

204
00:08:38,040 --> 00:08:39,680
以蓝色显示。 

205
00:08:39,680 --> 00:08:41,860
让我们说它的X是负二， 

206
00:08:41,860 --> 00:08:44,460
Y为5，Z为负4。 

207
00:08:45,760 --> 00:08:47,420
让我们做一个前锋传球

208
00:08:47,420 --> 00:08:48,600
通过电路

209
00:08:48,900 --> 00:08:50,300
产生输出。 

210
00:08:52,080 --> 00:08:55,020
负二加五等于三

211
00:08:55,860 --> 00:08:57,900
q是那个中间值， 

212
00:08:58,660 --> 00:08:59,280
三。 

213
00:09:01,300 --> 00:09:03,140
这很简单， 

214
00:09:03,420 --> 00:09:05,240
了解如此重要

215
00:09:05,240 --> 00:09:07,350
我只是想花时间来做这件事

216
00:09:07,370 --> 00:09:11,810
因为神经网络的其他一切都建立在这些概念之上

217
00:09:13,880 --> 00:09:15,960
添加门产生q， 

218
00:09:15,960 --> 00:09:17,160
在这种情况下，是三， 

219
00:09:17,290 --> 00:09:19,660
三次负四是十二。 

220
00:09:19,660 --> 00:09:20,700
这是输出。 

221
00:09:20,700 --> 00:09:24,480
该网络电路的输出， 

222
00:09:24,880 --> 00:09:26,740
如果你这样想， 

223
00:09:26,740 --> 00:09:28,240
是负十二。 

224
00:09:29,820 --> 00:09:31,570
前向传球以蓝色显示

225
00:09:31,580 --> 00:09:33,640
向后传球将以红色显示

226
00:09:33,640 --> 00:09:34,780
在这里的第二个

227
00:09:34,780 --> 00:09:36,400
我们想做什么

228
00:09:36,400 --> 00:09:37,720
是什么让我们开心， 

229
00:09:37,720 --> 00:09:39,260
什么让快乐

230
00:09:39,260 --> 00:09:42,020
是为了尽可能高的输出。 

231
00:09:42,020 --> 00:09:44,400
负十二，一般，它可能会更好。 

232
00:09:44,400 --> 00:09:45,740
我们如何教它

233
00:09:46,620 --> 00:09:48,640
我们如何调整X，Y和Z， 

234
00:09:49,540 --> 00:09:54,040
确保它产生更高的f 

235
00:09:55,420 --> 00:09:56,940
让我更快乐。 

236
00:09:56,940 --> 00:09:59,340
让我们开始向后， 

237
00:10:00,660 --> 00:10:01,960
向后传球。 

238
00:10:02,920 --> 00:10:05,840
我们将在输出上设置渐变， 

239
00:10:05,840 --> 00:10:07,920
意思是我们希望这个增加。 

240
00:10:07,920 --> 00:10:09,060
我们希望f增加。 

241
00:10:09,060 --> 00:10:11,140
这就是我们如何编码我们的幸福。 

242
00:10:12,100 --> 00:10:14,540
我们希望它一个接一个。 

243
00:10:16,400 --> 00:10:19,260
然后再传播

244
00:10:21,140 --> 00:10:23,180
我们想要的那个事实

245
00:10:23,180 --> 00:10:25,340
f上升一， 

246
00:10:25,340 --> 00:10:27,760
我们要看一下

247
00:10:27,760 --> 00:10:30,660
每个门上的梯度。 

248
00:10:30,660 --> 00:10:32,480
什么是梯度？ 

249
00:10:34,000 --> 00:10:35,380
它是

250
00:10:37,220 --> 00:10:38,740
偏导数

251
00:10:40,560 --> 00:10:42,640
就其投入而言。 

252
00:10:42,640 --> 00:10:45,340
门输出的偏导数

253
00:10:45,340 --> 00:10:47,260
就其投入而言， 

254
00:10:47,260 --> 00:10:49,260
如果你不知道这意味着什么， 

255
00:10:49,320 --> 00:10:50,180
只是

256
00:10:54,040 --> 00:10:56,200
输出变化多少

257
00:10:56,760 --> 00:10:58,940
当我稍微改变输入时。 

258
00:10:58,940 --> 00:11:02,280
如果我增加X，那个变化的斜率是多少

259
00:11:02,340 --> 00:11:04,600
对于第一个添加功能， 

260
00:11:04,600 --> 00:11:07,200
X的Y，Y等于X加Y. 

261
00:11:07,200 --> 00:11:09,040
如果我稍微增加X， 

262
00:11:09,040 --> 00:11:10,200
怎么回事？ 

263
00:11:10,200 --> 00:11:13,160
如果我稍微增加Y，f会发生什么？ 

264
00:11:13,160 --> 00:11:15,160
采取那些的偏导数

265
00:11:15,160 --> 00:11:17,160
关于X和Y. 

266
00:11:17,160 --> 00:11:19,260
你得到一个斜坡

267
00:11:19,260 --> 00:11:20,640
当你增加X时， 

268
00:11:20,640 --> 00:11:22,720
f线性增加。 

269
00:11:23,100 --> 00:11:24,160
与Y相同

270
00:11:24,780 --> 00:11:26,760
乘法有点棘手。 

271
00:11:28,740 --> 00:11:30,700
当你增加X时， 

272
00:11:31,980 --> 00:11:33,640
f增加Y. 

273
00:11:34,500 --> 00:11:38,500
f相对于X的偏导数是Y， 

274
00:11:38,500 --> 00:11:41,700
f相对于Y的偏导数是X. 

275
00:11:44,500 --> 00:11:47,460
如果你考虑一下，会发生什么

276
00:11:47,800 --> 00:11:50,400
渐变，当你改变X时， 

277
00:11:51,020 --> 00:11:52,660
变化的梯度

278
00:11:53,000 --> 00:11:54,480
不关心X. 

279
00:11:54,940 --> 00:11:57,080
它关心Y. 

280
00:11:58,140 --> 00:11:59,140
它被翻转了。 

281
00:11:59,760 --> 00:12:01,960
所以我们可以反过来传播那一个， 

282
00:12:01,960 --> 00:12:05,580
是什么让X快乐落后的迹象。 

283
00:12:07,440 --> 00:12:09,240
这完成了

284
00:12:09,680 --> 00:12:11,340
计算局部梯度。 

285
00:12:15,020 --> 00:12:16,340
对于q， 

286
00:12:18,240 --> 00:12:21,300
f的相对于q的偏导数， 

287
00:12:21,300 --> 00:12:23,300
那中间值， 

288
00:12:24,360 --> 00:12:26,820
那个梯度将是负四。 

289
00:12:26,820 --> 00:12:28,700
它将取Z的值

290
00:12:28,700 --> 00:12:30,700
正如我所说的那样是乘法门， 

291
00:12:30,700 --> 00:12:32,460
它将取Z的值

292
00:12:35,260 --> 00:12:36,900
并将其分配给渐变。 

293
00:12:37,300 --> 00:12:38,800
同样的

294
00:12:38,800 --> 00:12:41,940
f相对于Z的偏导数， 

295
00:12:41,940 --> 00:12:43,600
它会将其分配给q。 

296
00:12:43,600 --> 00:12:45,880
q的正向传递值。 

297
00:12:45,880 --> 00:12:47,500
有三个

298
00:12:47,500 --> 00:12:50,280
蓝色前进传球中的负四

299
00:12:50,280 --> 00:12:51,600
那就是翻了。 

300
00:12:51,600 --> 00:12:52,960
负四和三

301
00:12:53,640 --> 00:12:54,840
在落后的传球。 

302
00:12:54,840 --> 00:12:56,020
那是渐变。 

303
00:12:56,660 --> 00:12:59,820
然后我们继续进行相同的过程。 

304
00:13:00,600 --> 00:13:01,820
可是等等。 

305
00:13:03,580 --> 00:13:06,160
是什么让所有这些工作， 

306
00:13:07,220 --> 00:13:09,020
是连锁规则。 

307
00:13:09,480 --> 00:13:10,520
这太神奇了。 

308
00:13:12,720 --> 00:13:14,240
它允许我们做什么

309
00:13:14,240 --> 00:13:16,680
是计算梯度， 

310
00:13:20,100 --> 00:13:23,020
相对于输入X，Y，Z的f的梯度。 

311
00:13:23,420 --> 00:13:25,480
我们不需要构建

312
00:13:27,160 --> 00:13:29,160
这个巨大的功能

313
00:13:31,260 --> 00:13:35,860
f的相对于X，Y和Z的偏导数

314
00:13:35,860 --> 00:13:37,200
解析。 

315
00:13:37,200 --> 00:13:38,500
我们可以一步一步做到这一点

316
00:13:38,500 --> 00:13:40,320
反向传播渐变。 

317
00:13:40,320 --> 00:13:42,490
我们可以将梯度相乘

318
00:13:42,510 --> 00:13:44,840
而不是做偏导数

319
00:13:44,880 --> 00:13:46,150
f相对于X. 

320
00:13:46,620 --> 00:13:48,700
我们只有中级， 

321
00:13:48,700 --> 00:13:49,900
局部渐变

322
00:13:49,900 --> 00:13:53,980
f相对于q，q相对于X， 

323
00:13:53,980 --> 00:13:55,980
并将它们相乘。 

324
00:13:58,360 --> 00:13:59,940
所以，而不是计算

325
00:14:00,560 --> 00:14:02,780
那个渐变

326
00:14:03,120 --> 00:14:06,700
巨功能X加Y倍Z， 

327
00:14:06,700 --> 00:14:08,400
在这种情况下不是那么巨大， 

328
00:14:08,400 --> 00:14:10,900
但它与神经网络相比变得非常巨大， 

329
00:14:10,930 --> 00:14:12,200
我们一步一步走。 

330
00:14:12,200 --> 00:14:14,020
看看第一个函数， 

331
00:14:14,020 --> 00:14:17,060
简单加法，q等于X加Y， 

332
00:14:17,060 --> 00:14:20,160
和第二个函数，乘法， 

333
00:14:20,160 --> 00:14:22,160
f等于q乘以Z. 

334
00:14:25,420 --> 00:14:28,840
X和Y上的渐变， 

335
00:14:30,280 --> 00:14:32,240
偏导数

336
00:14:32,780 --> 00:14:35,740
f对X和Y的关系

337
00:14:35,740 --> 00:14:37,840
通过乘以计算

338
00:14:38,260 --> 00:14:41,020
输出上的梯度，负四， 

339
00:14:41,620 --> 00:14:44,740
乘以输入的渐变， 

340
00:14:44,740 --> 00:14:46,260
正如我们所谈到的， 

341
00:14:46,260 --> 00:14:48,640
当操作添加时， 

342
00:14:48,640 --> 00:14:50,080
那只是一个。 

343
00:14:50,080 --> 00:14:52,080
这是负四倍。 

344
00:14:56,480 --> 00:14:57,620
那是什么意思？ 

345
00:14:57,620 --> 00:15:00,060
让我们解释这些数字。 

346
00:15:00,940 --> 00:15:04,200
您现在在X，Y和Z上有渐变

347
00:15:05,840 --> 00:15:08,980
F的偏导数相对于X，Y，Z。 

348
00:15:08,980 --> 00:15:10,080
那意味着， 

349
00:15:11,360 --> 00:15:14,200
对于X和Y是负四，因为Z是三。 

350
00:15:14,200 --> 00:15:17,680
这意味着，为了让快乐， 

351
00:15:17,680 --> 00:15:19,100
我们必须减少

352
00:15:22,140 --> 00:15:25,360
具有负梯度的输入

353
00:15:25,360 --> 00:15:28,740
并增加具有正梯度的输入。 

354
00:15:28,740 --> 00:15:30,400
负面的是X和Y， 

355
00:15:30,400 --> 00:15:31,850
积极的是Z. 

356
00:15:35,680 --> 00:15:36,890
希望，我不说

357
00:15:36,920 --> 00:15:39,800
在这个演讲中，“美丽”这个词太多次了

358
00:15:39,820 --> 00:15:42,240
这很简单。非常简单。 

359
00:15:44,780 --> 00:15:47,580
因为这个梯度是本地工人， 

360
00:15:48,160 --> 00:15:50,160
它为你传播; 

361
00:15:50,160 --> 00:15:53,340
它不了解更广泛的

362
00:15:53,900 --> 00:15:55,800
f的快乐

363
00:15:58,120 --> 00:16:01,180
它计算输出和输入之间的较大值。 

364
00:16:01,180 --> 00:16:03,700
它可以传播这个渐变

365
00:16:03,700 --> 00:16:05,100
基于， 

366
00:16:05,500 --> 00:16:07,120
在这种情况下f， 

367
00:16:07,120 --> 00:16:09,840
一个渐变，但也错误。 

368
00:16:10,420 --> 00:16:13,340
而不是我们可以在输出上有错误

369
00:16:13,340 --> 00:16:14,860
作为幸福的衡量标准。 

370
00:16:14,860 --> 00:16:17,320
然后我们可以向后传播该错误。 

371
00:16:17,340 --> 00:16:20,230
这些门是重要的，因为我们可以打破

372
00:16:20,250 --> 00:16:22,450
几乎我们能想到的每一项操作

373
00:16:22,460 --> 00:16:24,420
我们在神经网络中工作

374
00:16:24,450 --> 00:16:27,180
进入这样的一个或几个门。 

375
00:16:28,260 --> 00:16:30,300
最受欢迎的是三个， 

376
00:16:30,300 --> 00:16:32,320
这是加法，乘法

377
00:16:32,400 --> 00:16:34,080
和Max操作。 

378
00:16:34,080 --> 00:16:35,300
另外， 

379
00:16:39,340 --> 00:16:40,260
这个过程是

380
00:16:40,260 --> 00:16:42,720
你通过网络前进， 

381
00:16:43,140 --> 00:16:45,780
所以我们每个门都有一个价值， 

382
00:16:47,040 --> 00:16:49,040
然后你采取向后传球。 

383
00:16:49,700 --> 00:16:52,920
通过向后传递，您可以计算出这些渐变。 

384
00:16:53,520 --> 00:16:55,320
对于添加门， 

385
00:16:55,540 --> 00:16:57,640
你平均分配渐变

386
00:16:57,640 --> 00:16:58,960
在输出到输入， 

387
00:16:58,960 --> 00:17:01,570
当输出的梯度为负四时， 

388
00:17:01,590 --> 00:17:03,910
你平均分配它的四种情感。 

389
00:17:06,700 --> 00:17:09,160
你忽略了前向传递值。 

390
00:17:09,620 --> 00:17:12,480
当你反向传播时，这三个被忽略了。 

391
00:17:15,940 --> 00:17:17,580
在乘法门上， 

392
00:17:18,200 --> 00:17:19,300
这比较棘手。 

393
00:17:19,760 --> 00:17:22,740
你切换正向传递值， 

394
00:17:23,560 --> 00:17:27,680
如果你看f，那是一个乘法门， 

395
00:17:29,720 --> 00:17:31,760
切换正向传递值

396
00:17:32,540 --> 00:17:36,620
并乘以输出中的渐变值。 

397
00:17:37,960 --> 00:17:41,220
如果它令人困惑，请慢慢浏览幻灯片。 

398
00:17:41,600 --> 00:17:43,760
它会更有意义。 

399
00:17:44,160 --> 00:17:45,020
希望。 

400
00:17:45,540 --> 00:17:47,740
还有一个门。有Max门， 

401
00:17:47,740 --> 00:17:50,820
它接受输入

402
00:17:50,820 --> 00:17:52,820
并作为输出产生

403
00:17:53,280 --> 00:17:55,720
更大的价值。 

404
00:17:56,360 --> 00:17:59,820
在计算Max门的梯度时， 

405
00:18:01,260 --> 00:18:03,740
它分配渐变

406
00:18:04,260 --> 00:18:13,640
类似于添加门，但只有一个，只有一个输入; 

407
00:18:13,980 --> 00:18:14,920
最大的一个。 

408
00:18:16,600 --> 00:18:19,160
与添加门不同，注意输入

409
00:18:19,180 --> 00:18:22,300
正向传递的输入值。 

410
00:18:22,560 --> 00:18:23,540
行。 

411
00:18:24,780 --> 00:18:29,140
很多数字，但这里的重点是， 

412
00:18:29,140 --> 00:18:30,500
它真的很简单; 

413
00:18:33,160 --> 00:18:36,760
神经网络只是这些门的简单集合。 

414
00:18:37,380 --> 00:18:39,560
你前往传球， 

415
00:18:40,080 --> 00:18:41,960
你计算某种功能

416
00:18:41,960 --> 00:18:43,960
最后，最后的渐变， 

417
00:18:43,960 --> 00:18:45,960
然后你传播回来。 

418
00:18:45,960 --> 00:18:49,280
通常，对于神经网络，这是一个错误函数。 

419
00:18:49,280 --> 00:18:51,820
损失函数，目标函数， 

420
00:18:52,480 --> 00:18:55,480
成本函数。所有相同的词。 

421
00:18:57,040 --> 00:19:00,140
这就是那里的Sigmoid功能

422
00:19:00,140 --> 00:19:01,960
当你有三个重量

423
00:19:01,960 --> 00:19:04,740
W零，W一，W两

424
00:19:04,740 --> 00:19:09,430
和X，两个输入，X0，X1， 

425
00:19:09,440 --> 00:19:11,480
这将是Sigmoid功能。 

426
00:19:11,480 --> 00:19:13,320
这就是你计算输出的方式

427
00:19:18,500 --> 00:19:19,780
神经元

428
00:19:19,780 --> 00:19:22,360
但是你可以分解那个神经元

429
00:19:22,360 --> 00:19:24,680
你可以将它全部分开

430
00:19:24,680 --> 00:19:26,540
只是这样一套门

431
00:19:26,540 --> 00:19:28,540
加法，乘法， 

432
00:19:28,720 --> 00:19:31,620
在那里和分裂有一个指数

433
00:19:31,620 --> 00:19:32,900
但都非常相似。 

434
00:19:32,900 --> 00:19:35,120
你重复完全相同的过程。 

435
00:19:38,820 --> 00:19:40,140
有五个输入， 

436
00:19:40,140 --> 00:19:41,320
有三个重量

437
00:19:41,580 --> 00:19:44,140
和两个输入。 X零，X一。 

438
00:19:46,280 --> 00:19:50,860
你通过这个赛道前进， 

439
00:19:52,340 --> 00:19:53,960
在这种情况下，再次， 

440
00:19:54,300 --> 00:19:58,940
你希望它增加，使输出的梯度为1 

441
00:19:58,940 --> 00:20:01,420
然后你反向传播那个渐变

442
00:20:01,680 --> 00:20:03,420
一，投入。 

443
00:20:04,120 --> 00:20:05,520
现在在神经网络中， 

444
00:20:05,720 --> 00:20:07,260
有一堆参数

445
00:20:07,260 --> 00:20:09,900
您正在尝试完成此过程，请进行修改。 

446
00:20:09,900 --> 00:20:12,320
而且你无法修改输入

447
00:20:12,320 --> 00:20:15,060
你可以沿途修改重量， 

448
00:20:15,060 --> 00:20:16,340
和偏见。 

449
00:20:16,340 --> 00:20:17,520
输入是固定的， 

450
00:20:17,520 --> 00:20:18,860
输出是固定的， 

451
00:20:18,860 --> 00:20:20,860
你希望的产出

452
00:20:21,600 --> 00:20:23,740
网络将产生。 

453
00:20:23,740 --> 00:20:25,740
你要修改的是权重。 

454
00:20:25,740 --> 00:20:28,040
所以我试着调整那些重量

455
00:20:29,400 --> 00:20:31,780
在梯度的方向。 

456
00:20:34,840 --> 00:20:37,440
这是反向传播的任务。 

457
00:20:37,780 --> 00:20:41,040
神经网络学习的主要方式。 

458
00:20:41,560 --> 00:20:44,400
当我们更新权重和偏见时

459
00:20:44,400 --> 00:20:46,400
减少损失功能。 

460
00:20:47,020 --> 00:20:49,780
损失函数越低越好。 

461
00:20:50,480 --> 00:20:52,340
在这种情况下，你有

462
00:20:52,660 --> 00:20:55,220
左上角有三个输入。 

463
00:20:55,220 --> 00:20:57,220
一个简单的网络，三个输入。 

464
00:20:58,760 --> 00:21:00,980
每个输入上有三个权重。 

465
00:21:00,980 --> 00:21:03,260
节点上存在偏差， 

466
00:21:03,700 --> 00:21:06,280
b并产生输出

467
00:21:06,960 --> 00:21:13,280
a，该小符号表示Sigmoid功能。 

468
00:21:15,480 --> 00:21:17,240
和损失

469
00:21:17,760 --> 00:21:21,220
计算为Y减去A平方， 

470
00:21:22,660 --> 00:21:24,460
除以2， 

471
00:21:25,520 --> 00:21:27,700
Y是基本事实， 

472
00:21:27,960 --> 00:21:31,780
您希望网络生成的输出。 

473
00:21:32,340 --> 00:21:34,980
而且这种损失函数正在反向传播

474
00:21:34,980 --> 00:21:37,500
与我们之前描述的方式完全相同。 

475
00:21:37,840 --> 00:21:39,100
子任务

476
00:21:39,580 --> 00:21:42,560
参与权重和偏见的更新

477
00:21:42,560 --> 00:21:44,560
是前进传球计算

478
00:21:44,560 --> 00:21:47,180
每个神经元的网络输出， 

479
00:21:47,180 --> 00:21:49,760
最后，输出层， 

480
00:21:50,180 --> 00:21:53,520
计算错误，a和b之间的差异， 

481
00:21:54,040 --> 00:21:55,120
然后

482
00:21:55,660 --> 00:21:57,880
向后传播渐变。 

483
00:21:58,820 --> 00:22:00,260
而不是输出上的一个， 

484
00:22:00,260 --> 00:22:03,240
这将是输出上的错误，你反向传播。 

485
00:22:03,410 --> 00:22:05,240
然后，一旦你知道了渐变， 

486
00:22:05,240 --> 00:22:07,560
你调整权重和偏见

487
00:22:07,560 --> 00:22:09,560
在梯度的方向。 

488
00:22:09,580 --> 00:22:12,590
实际上，与梯度方向相反， 

489
00:22:12,600 --> 00:22:14,980
因为你希望损失减少。 

490
00:22:14,980 --> 00:22:18,440
以及进行调整的金额

491
00:22:18,440 --> 00:22:20,100
被称为学习率。 

492
00:22:20,130 --> 00:22:23,230
整个网络的学习率可以相同

493
00:22:23,240 --> 00:22:25,550
或者可以通过每个重量来个体化。 

494
00:22:32,500 --> 00:22:34,080
而这个过程

495
00:22:34,520 --> 00:22:36,880
调整重量和偏差

496
00:22:36,880 --> 00:22:38,880
只是优化。 

497
00:22:39,120 --> 00:22:41,540
学习是一个优化问题。 

498
00:22:41,560 --> 00:22:44,850
你有一个客观的功能，你正试图最小化它。 

499
00:22:44,880 --> 00:22:48,150
而你的变量是参数，权重和偏差。 

500
00:22:49,480 --> 00:22:51,360
神经网络恰巧有

501
00:22:51,360 --> 00:22:54,060
数十万，数十万，数百万

502
00:22:54,060 --> 00:22:56,060
那些参数。 

503
00:22:57,080 --> 00:23:00,460
因此，您尝试最小化的功能是高度非线性的。 

504
00:23:00,460 --> 00:23:03,200
但它归结为类似的东西，你有

505
00:23:03,200 --> 00:23:09,240
两个重量，两个地块 - 或实际上一个重量

506
00:23:09,240 --> 00:23:11,960
当你调整它，成本

507
00:23:14,720 --> 00:23:18,320
您可以通过最小化输出成本的方式进行调整。 

508
00:23:20,860 --> 00:23:24,080
还有一堆优化方法可以做到这一点。 

509
00:23:25,800 --> 00:23:28,040
这是一个凸函数， 

510
00:23:28,040 --> 00:23:30,540
你可以找到当地的最低要求。 

511
00:23:31,020 --> 00:23:33,780
如果您了解这些术语， 

512
00:23:33,780 --> 00:23:36,500
当地最小值与全局最小值相同， 

513
00:23:36,520 --> 00:23:39,340
这不是一个奇怪的丘陵地形

514
00:23:39,340 --> 00:23:41,760
你可以陷入困境的地方。 

515
00:23:41,760 --> 00:23:44,180
你的目标是深入了解这件事

516
00:23:44,180 --> 00:23:46,440
如果这是非常复杂的地形， 

517
00:23:46,520 --> 00:23:49,000
很难找到它的底部。 

518
00:23:52,660 --> 00:23:55,960
这种一般方法是梯度下降， 

519
00:23:55,960 --> 00:23:59,220
并且有许多不同的方法来进行梯度下降。 

520
00:24:00,320 --> 00:24:03,860
将随机性添加到流程中的各种方法， 

521
00:24:03,860 --> 00:24:06,420
所以你不要陷入奇怪的境地

522
00:24:06,420 --> 00:24:09,380
地形的裂缝。 

523
00:24:10,140 --> 00:24:11,480
但它很乱。 

524
00:24:11,780 --> 00:24:13,300
你必须非常小心。 

525
00:24:13,300 --> 00:24:15,540
这是你必须要注意的部分， 

526
00:24:15,920 --> 00:24:18,700
为DeepTraffic设计网络时

527
00:24:18,700 --> 00:24:20,200
没有任何事情发生

528
00:24:20,740 --> 00:24:22,860
这可能是正在发生的事情： 

529
00:24:24,180 --> 00:24:25,700
消失的渐变

530
00:24:25,960 --> 00:24:27,680
或爆炸渐变。 

531
00:24:29,740 --> 00:24:31,760
当偏导数

532
00:24:31,760 --> 00:24:34,920
很小，所以你采取Sigmoid函数， 

533
00:24:35,360 --> 00:24:36,840
最受欢迎

534
00:24:37,340 --> 00:24:40,040
一段时间，激活功能， 

535
00:24:40,040 --> 00:24:43,040
尾部的导数为零。 

536
00:24:43,040 --> 00:24:45,040
当输入到

537
00:24:45,040 --> 00:24:48,340
Sigmoid函数非常高或非常低， 

538
00:24:48,860 --> 00:24:51,360
该衍生品将为零。 

539
00:24:55,060 --> 00:24:57,940
渐变告诉我要调整多少权重。 

540
00:24:57,940 --> 00:25:00,120
渐变可能为零， 

541
00:25:00,120 --> 00:25:02,480
所以你反向传播零， 

542
00:25:02,480 --> 00:25:04,140
数量很少， 

543
00:25:04,140 --> 00:25:06,140
它变得越来越少

544
00:25:06,140 --> 00:25:08,020
当你反向传播

545
00:25:08,020 --> 00:25:10,920
结果就是这样

546
00:25:12,680 --> 00:25:15,310
你认为你根本不需要调整重量。 

547
00:25:15,340 --> 00:25:17,550
而当网络的很大一部分

548
00:25:17,560 --> 00:25:19,630
重量不需要调整， 

549
00:25:19,830 --> 00:25:21,460
他们不调整重量。 

550
00:25:21,460 --> 00:25:23,280
你没有做任何学习

551
00:25:23,300 --> 00:25:25,040
所以学习很慢。 

552
00:25:28,280 --> 00:25:30,140
这有一些修复， 

553
00:25:31,640 --> 00:25:33,860
有不同类型的功能。 

554
00:25:33,860 --> 00:25:35,700
有一块， 

555
00:25:35,700 --> 00:25:39,840
ReLUs功能是最受欢迎的激活功能。 

556
00:25:40,500 --> 00:25:42,300
但是， 

557
00:25:43,620 --> 00:25:46,940
如果神经元初始化不良， 

558
00:25:49,100 --> 00:25:51,700
这个功能可能不会触发。 

559
00:25:51,700 --> 00:25:54,560
它可能是零梯度

560
00:25:54,560 --> 00:25:56,560
对于整个数据集。 

561
00:25:57,700 --> 00:26:00,940
没有你作为输入产生， 

562
00:26:01,260 --> 00:26:04,720
你运行所有成千上万的猫图像， 

563
00:26:04,720 --> 00:26:06,720
而且他们都没有开枪。 

564
00:26:06,720 --> 00:26:08,720
这就是危险。 

565
00:26:10,780 --> 00:26:12,380
所以你必须选择

566
00:26:13,680 --> 00:26:17,960
两个优化引擎， 

567
00:26:17,960 --> 00:26:19,640
你使用的解算器

568
00:26:19,640 --> 00:26:21,880
并且激活功能小心。 

569
00:26:21,880 --> 00:26:25,780
你不能只是像Lego那样即插即用

570
00:26:25,780 --> 00:26:28,020
你必须要知道这个功能。 

571
00:26:29,760 --> 00:26:32,920
SGD，随机梯度下降， 

572
00:26:38,760 --> 00:26:42,380
这是Vanilla优化算法

573
00:26:42,380 --> 00:26:43,980
用于梯度下降。 

574
00:26:44,620 --> 00:26:48,500
用于优化梯度上的损耗函数

575
00:26:48,500 --> 00:26:50,660
这里可视化的是， 

576
00:26:50,660 --> 00:26:54,500
再次，如果你做了任何数值优化， 

577
00:26:54,500 --> 00:26:56,340
和非线性优化， 

578
00:26:56,340 --> 00:26:58,360
有着名的马鞍点， 

579
00:26:59,020 --> 00:27:02,240
对于这些算法来说，这很棘手。 

580
00:27:02,700 --> 00:27:06,120
会发生什么，它们很容易振荡， 

581
00:27:06,120 --> 00:27:09,220
卡在那个马鞍上来回摆动

582
00:27:09,220 --> 00:27:12,180
而不是他们想做的事情

583
00:27:12,180 --> 00:27:14,100
进入 - 

584
00:27:14,100 --> 00:27:17,180
你很高兴找到了这个

585
00:27:18,900 --> 00:27:19,840
低点

586
00:27:20,340 --> 00:27:22,860
你忘了那里有一个低得多的点。 

587
00:27:22,920 --> 00:27:25,100
所以你会陷入渐变。 

588
00:27:25,100 --> 00:27:26,520
渐变的势头

589
00:27:26,520 --> 00:27:29,320
在你不去的情况下来回摇摆

590
00:27:29,740 --> 00:27:32,060
全球最低限度。 

591
00:27:32,060 --> 00:27:34,900
并且有很多聪明的方法来解决这个问题， 

592
00:27:34,900 --> 00:27:38,080
Atom优化器就是其中之一。 

593
00:27:40,460 --> 00:27:45,340
但在这种情况下，只要渐变不消失

594
00:27:46,180 --> 00:27:48,430
SGD，随机梯度下降， 

595
00:27:48,450 --> 00:27:50,720
其中一种算法可以帮助你

596
00:27:50,720 --> 00:27:53,690
可能需要一段时间，但它会让你到那里

597
00:27:54,300 --> 00:27:55,600
是的，问题。 

598
00:27:57,040 --> 00:27:59,160
问题是， 

599
00:28:00,260 --> 00:28:03,100
你正在处理一个非凸的函数， 

600
00:28:03,100 --> 00:28:05,760
我们如何确保任何事情

601
00:28:06,160 --> 00:28:07,900
收敛到任何东西

602
00:28:07,900 --> 00:28:09,300
相当不错， 

603
00:28:09,300 --> 00:28:12,100
局部最优收敛于 - 

604
00:28:12,100 --> 00:28:15,000
答案是，你做不到。 

605
00:28:16,740 --> 00:28:19,400
这不仅是非线性函数

606
00:28:19,700 --> 00:28:22,260
这是一个高度无功能的

607
00:28:22,260 --> 00:28:24,920
神经网络的力量和美丽

608
00:28:25,460 --> 00:28:32,420
是它可以代表这些任意复杂的功能。 

609
00:28:32,420 --> 00:28:33,800
这太不可思议了。 

610
00:28:33,800 --> 00:28:37,040
它可以从数据中学习这些功能

611
00:28:38,000 --> 00:28:43,000
但人们将神经网络训练称为艺术的原因

612
00:28:43,040 --> 00:28:46,180
你正试图玩参数吗？ 

613
00:28:46,180 --> 00:28:48,480
不要陷入这些局部最优。 

614
00:28:48,480 --> 00:28:50,760
出于愚蠢的原因和聪明的原因。 

615
00:28:50,760 --> 00:28:51,660
是的，问题。 

616
00:28:53,780 --> 00:28:57,700
该问题继续在同一个主题上。 

617
00:29:00,340 --> 00:29:03,650
问题是，我们正在处理功能

618
00:29:03,660 --> 00:29:06,260
我们不知道全局最优的是什么。 

619
00:29:06,260 --> 00:29:08,160
这就是它的鳄鱼。 

620
00:29:10,880 --> 00:29:12,440
我们谈论的一切， 

621
00:29:13,100 --> 00:29:14,440
解释文字， 

622
00:29:14,860 --> 00:29:16,440
解读视频， 

623
00:29:17,280 --> 00:29:18,560
甚至开车。 

624
00:29:18,560 --> 00:29:20,720
什么是驾驶的最佳选择？ 

625
00:29:21,740 --> 00:29:22,880
永远不会崩溃？ 

626
00:29:25,260 --> 00:29:27,320
听起来很容易， 

627
00:29:27,320 --> 00:29:29,630
你实际上必须制定世界

628
00:29:29,660 --> 00:29:33,020
在它下面定义所有这些东西，它变成了一个真正的东西

629
00:29:33,040 --> 00:29:34,900
非线性目标函数

630
00:29:34,900 --> 00:29:37,240
你不知道最佳的是什么。 

631
00:29:41,160 --> 00:29:42,900
这就是你继续努力的原因

632
00:29:42,900 --> 00:29:45,390
并且每次变好都会给人留下深刻的印象。 

633
00:29:45,430 --> 00:29:47,100
这基本上就是这个过程。 

634
00:29:47,460 --> 00:29:49,800
你也可以比较， 

635
00:29:49,830 --> 00:29:52,450
你可以与人类的表现进行比较。 

636
00:29:52,460 --> 00:29:53,360
对于ImageNet， 

637
00:29:53,370 --> 00:29:56,040
谁能分辨出猫狗之间的区别， 

638
00:29:56,040 --> 00:29:58,660
和前五个类别， 

639
00:30:00,500 --> 00:30:03,060
96％的时间准确度， 

640
00:30:03,060 --> 00:30:04,750
然后你会留下深刻的印象

641
00:30:04,770 --> 00:30:06,600
一台机器可以做得更好。 

642
00:30:06,660 --> 00:30:08,460
但你不知道什么是最好的。 

643
00:30:15,210 --> 00:30:17,180
这些视频可以观看几个小时， 

644
00:30:17,180 --> 00:30:19,600
在我解释这张幻灯片之前我不会播放它。 

645
00:30:21,070 --> 00:30:23,700
让我们暂停以反思反向传播

646
00:30:23,700 --> 00:30:26,880
在我继续回归神经网络之前。是的，问题。 

647
00:30:26,880 --> 00:30:29,380
以这种实际的方式，你怎么知道什么时候

648
00:30:29,380 --> 00:30:31,820
无论你是谁，你实际上是在创造一个网络

649
00:30:31,860 --> 00:30:34,140
面对管理梯度问题

650
00:30:34,140 --> 00:30:37,780
或者您需要更改优化程序

651
00:30:39,780 --> 00:30:42,400
或者你达到了当地的最低标准？ 

652
00:30:43,460 --> 00:30:45,060
问题是， 

653
00:30:45,060 --> 00:30:46,940
你怎么知道的

654
00:30:47,280 --> 00:30:51,240
当你遇到消失的梯度问题？ 

655
00:30:51,600 --> 00:30:53,300
消失的梯度可能是 - 

656
00:31:00,760 --> 00:31:03,420
梯度上的导数为零， 

657
00:31:03,420 --> 00:31:07,240
当激活爆炸时发生， 

658
00:31:07,240 --> 00:31:09,040
喜欢真正的高价值

659
00:31:09,040 --> 00:31:10,220
而且价值非常低。 

660
00:31:10,220 --> 00:31:12,100
真正的高价值很容易。 

661
00:31:12,100 --> 00:31:14,660
你的网络刚刚疯了。 

662
00:31:14,660 --> 00:31:17,440
它产生非常大的值。 

663
00:31:17,440 --> 00:31:23,280
你可以通过限制激活来解决很多这些问题。 

664
00:31:25,920 --> 00:31:28,240
价值真的很低， 

665
00:31:28,820 --> 00:31:32,240
导致渐变的消失，真的很难被发现

666
00:31:34,660 --> 00:31:37,880
试图弄清楚有很多研究

667
00:31:37,880 --> 00:31:39,880
如何检测这些东西。 

668
00:31:40,380 --> 00:31:43,020
如果你经常不小心的话

669
00:31:43,860 --> 00:31:46,080
你可以找到， 

670
00:31:48,660 --> 00:31:50,240
这不难做到

671
00:31:50,240 --> 00:31:53,280
我们就像40％或50％的网络， 

672
00:31:53,280 --> 00:31:54,560
神经元， 

673
00:31:55,580 --> 00:31:57,220
是死的。 

674
00:31:57,720 --> 00:31:59,420
我们称之为ReLU， 

675
00:31:59,420 --> 00:32:00,800
他们已经死了ReLU 

676
00:32:00,800 --> 00:32:02,560
他们根本没有开火。 

677
00:32:03,220 --> 00:32:04,380
你怎么发现的？ 

678
00:32:05,590 --> 00:32:06,810
这是学习的一部分

679
00:32:06,840 --> 00:32:08,730
如果他们从未开火，你可以发现它

680
00:32:08,770 --> 00:32:11,250
通过在整个训练集中运行它。 

681
00:32:11,260 --> 00:32:14,150
有很多技巧。但那就是问题所在。 

682
00:32:14,460 --> 00:32:15,720
你试着学习

683
00:32:15,900 --> 00:32:19,100
然后你看一下损失函数

684
00:32:19,100 --> 00:32:20,260
而事实并非如此

685
00:32:20,960 --> 00:32:22,890
融合到任何合理的东西。 

686
00:32:22,920 --> 00:32:26,470
他们到处都是，或者只是非常慢地融合。 

687
00:32:26,490 --> 00:32:28,960
这表明出现了问题

688
00:32:28,970 --> 00:32:31,350
那个东西可能是损失函数很糟糕， 

689
00:32:31,390 --> 00:32:33,410
你可能已经找到了最佳的东西， 

690
00:32:33,420 --> 00:32:36,200
或者某种东西可能是消失的梯度。 

691
00:32:36,210 --> 00:32:37,920
再次，这就是为什么它是一门艺术。 

692
00:32:41,100 --> 00:32:42,500
当然， 

693
00:32:44,560 --> 00:32:47,960
至少有一部分神经元需要射击。 

694
00:32:49,120 --> 00:32:52,340
否则，初始化真的很糟糕。 

695
00:32:52,780 --> 00:32:54,840
好的，要反思一下

696
00:32:55,180 --> 00:32:57,520
简单的反向传播

697
00:32:58,800 --> 00:33:00,280
和它的力量， 

698
00:33:02,080 --> 00:33:03,320
这种步骤

699
00:33:03,340 --> 00:33:06,910
将损失函数反向传播到局部的梯度， 

700
00:33:08,120 --> 00:33:10,060
是神经网络学习的方式。 

701
00:33:11,900 --> 00:33:13,700
这是唯一的方法

702
00:33:13,700 --> 00:33:16,640
我们已经有效地做到了

703
00:33:16,640 --> 00:33:19,640
训练神经网络

704
00:33:19,640 --> 00:33:21,140
网络学习功能。 

705
00:33:21,140 --> 00:33:23,140
要调整重量和偏差， 

706
00:33:23,140 --> 00:33:26,440
大量的权重和偏差，参数

707
00:33:26,440 --> 00:33:28,260
这只是通过这种优化。 

708
00:33:28,260 --> 00:33:30,580
它反向传播错误， 

709
00:33:31,320 --> 00:33:34,500
你有监督的基本事实。 

710
00:33:34,580 --> 00:33:36,000
问题是

711
00:33:36,000 --> 00:33:38,980
是否适合这个过程， 

712
00:33:41,800 --> 00:33:44,220
调整参数

713
00:33:45,000 --> 00:33:49,700
高度非线性函数，以最小化单个目标， 

714
00:33:49,700 --> 00:33:55,040
是你实现智慧的方式。 

715
00:33:55,040 --> 00:33:56,440
人类智慧。 

716
00:33:56,440 --> 00:33:58,100
这是值得思考的问题。 

717
00:33:58,320 --> 00:34:00,950
出于驾驶目的，你必须考虑一下

718
00:34:00,980 --> 00:34:03,600
这种方法的局限性是什么？ 

719
00:34:04,140 --> 00:34:05,680
什么没发生？ 

720
00:34:05,680 --> 00:34:08,660
神经网络设计，架构

721
00:34:08,690 --> 00:34:10,180
没有调整。 

722
00:34:12,830 --> 00:34:16,370
任何边缘，层，没有任何东西进化

723
00:34:18,340 --> 00:34:21,880
还有其他优化方法

724
00:34:22,480 --> 00:34:24,080
我认为更多

725
00:34:27,200 --> 00:34:30,780
有趣和鼓舞人心而不是有效。 

726
00:34:30,780 --> 00:34:32,560
例如，这是

727
00:34:33,700 --> 00:34:36,400
使用软立方体 - 

728
00:34:36,400 --> 00:34:40,000
这已经失控了

729
00:34:40,000 --> 00:34:42,800
进化机器人学。 

730
00:34:43,120 --> 00:34:45,280
你进化的地方

731
00:34:45,580 --> 00:34:47,340
机器人的动力学

732
00:34:47,340 --> 00:34:49,160
使用遗传算法

733
00:34:49,840 --> 00:34:50,960
那是

734
00:35:00,020 --> 00:35:02,740
这些机器人已被教导， 

735
00:35:03,120 --> 00:35:04,640
在模拟中，显然， 

736
00:35:05,520 --> 00:35:07,660
走路和游泳。 

737
00:35:07,980 --> 00:35:09,460
那是游泳。 

738
00:35:14,640 --> 00:35:17,320
这里的好处是动态

739
00:35:17,720 --> 00:35:19,740
非常非线性的空间， 

740
00:35:19,740 --> 00:35:23,700
控制这个奇怪形状的机器人的动态

741
00:35:24,200 --> 00:35:25,980
有很多自由度， 

742
00:35:25,980 --> 00:35:28,540
它与神经网络是一回事。 

743
00:35:28,540 --> 00:35:31,400
事实上，人们已经应用了通用算法， 

744
00:35:32,090 --> 00:35:36,040
蚁群优化，各种自然激发算法

745
00:35:36,060 --> 00:35:38,450
用于自动化权重和偏差

746
00:35:38,500 --> 00:35:41,050
但他们目前似乎并没有这么好。 

747
00:35:41,440 --> 00:35:43,660
这是一个很酷的想法

748
00:35:43,660 --> 00:35:46,640
自然型进化算法的演化

749
00:35:46,640 --> 00:35:50,000
已经被大自然启发的东西是神经网络。 

750
00:35:50,000 --> 00:35:52,000
但是，有待考虑的事情

751
00:35:54,980 --> 00:35:57,920
反向传播，虽然很简单

752
00:35:57,920 --> 00:36:00,420
这有点愚蠢，问题是，是否

753
00:36:00,420 --> 00:36:04,050
通过这个过程可以实现一般情报推理。 

754
00:36:04,360 --> 00:36:06,360
好的，回归神经网络， 

755
00:36:07,000 --> 00:36:10,600
在左边有一个输入X. 

756
00:36:10,980 --> 00:36:13,300
在输入上有权重，U， 

757
00:36:13,860 --> 00:36:15,900
有一个隐藏的状态， 

758
00:36:15,900 --> 00:36:17,900
隐藏层S， 

759
00:36:18,640 --> 00:36:21,060
有重量

760
00:36:25,260 --> 00:36:26,860
边缘连接

761
00:36:26,860 --> 00:36:28,860
相互隐藏的状态

762
00:36:28,860 --> 00:36:33,000
然后更多权重，V，输出O. 

763
00:36:33,680 --> 00:36:36,130
这是一个非常简单的网络，有输入， 

764
00:36:36,360 --> 00:36:38,520
有隐藏的状态， 

765
00:36:39,200 --> 00:36:41,260
这个网络的记忆

766
00:36:42,160 --> 00:36:43,760
并且有输出。 

767
00:36:46,260 --> 00:36:47,880
但事实就是这样

768
00:36:48,120 --> 00:36:50,020
这个循环

769
00:36:50,820 --> 00:36:53,600
隐藏状态彼此连接的地方

770
00:36:53,600 --> 00:36:57,180
意味着与产生单一输入相反， 

771
00:36:57,520 --> 00:37:00,820
网络采用任意数量的输入， 

772
00:37:00,820 --> 00:37:03,860
它只是一次一个地拿着X. 

773
00:37:03,860 --> 00:37:06,120
并产生一系列Xs 

774
00:37:06,480 --> 00:37:07,480
通过时间。 

775
00:37:10,100 --> 00:37:11,500
取决于

776
00:37:11,500 --> 00:37:14,140
您感兴趣的序列的持续时间， 

777
00:37:14,140 --> 00:37:16,140
你可以想到这个网络

778
00:37:16,140 --> 00:37:17,900
处于展开状态。 

779
00:37:18,860 --> 00:37:20,820
您可以展开此神经网络

780
00:37:20,820 --> 00:37:25,520
输入在底部，Xt-1，Xt，Xt + 1， 

781
00:37:25,780 --> 00:37:27,160
和输出一样

782
00:37:28,740 --> 00:37:31,460
Ot-1，Ot，Ot + 1， 

783
00:37:32,360 --> 00:37:34,920
它变得像一个普通的神经网络， 

784
00:37:34,920 --> 00:37:38,240
展开了一些任意次数。 

785
00:37:40,300 --> 00:37:42,220
参数，再次， 

786
00:37:42,220 --> 00:37:44,760
有重量，有偏见， 

787
00:37:44,760 --> 00:37:46,580
类似于CNN， 

788
00:37:46,580 --> 00:37:48,360
卷积神经网络

789
00:37:48,960 --> 00:37:51,400
就像卷积神经网络一样

790
00:37:51,400 --> 00:37:55,340
做出一定的空间一致性假设， 

791
00:37:55,620 --> 00:37:58,320
递归神经网络假设

792
00:37:58,320 --> 00:38:00,880
参数之间的时间一致性， 

793
00:38:00,880 --> 00:38:02,420
分享参数。 

794
00:38:03,380 --> 00:38:05,800
那个W，那个U，那个V， 

795
00:38:05,800 --> 00:38:08,100
每一个时间步都是一样的。 

796
00:38:08,100 --> 00:38:09,220
你正在学习

797
00:38:09,980 --> 00:38:11,580
相同的参数， 

798
00:38:11,580 --> 00:38:13,740
无论序列的持续时间

799
00:38:14,360 --> 00:38:15,780
这可以让你

800
00:38:15,780 --> 00:38:19,080
看看任意长序列

801
00:38:19,760 --> 00:38:22,860
没有参数爆炸。 

802
00:38:29,480 --> 00:38:32,480
这个过程与重复的过程完全相同

803
00:38:32,480 --> 00:38:35,260
基于我们之前谈到的不同变体， 

804
00:38:35,260 --> 00:38:36,850
在投入和产出方面， 

805
00:38:36,860 --> 00:38:39,180
一对多，多对一，多对多。 

806
00:38:40,380 --> 00:38:42,740
反向传播过程

807
00:38:42,740 --> 00:38:45,640
与常规神经网络完全相同。 

808
00:38:45,640 --> 00:38:48,660
这是一个时髦的反向传播的名字， 

809
00:38:49,200 --> 00:38:50,440
BPTT， 

810
00:38:51,020 --> 00:38:54,980
但它只是通过展开的反向传播

811
00:38:58,700 --> 00:39:00,540
递归神经网络

812
00:39:00,540 --> 00:39:03,820
错误在输出上计算的位置， 

813
00:39:04,600 --> 00:39:06,440
计算渐变， 

814
00:39:07,220 --> 00:39:09,280
backpropagated 

815
00:39:10,200 --> 00:39:12,060
并在输入上计算， 

816
00:39:12,600 --> 00:39:15,580
再次，痛苦同样的问题

817
00:39:16,460 --> 00:39:18,440
消失的渐变。 

818
00:39:18,440 --> 00:39:19,800
问题是

819
00:39:19,800 --> 00:39:23,840
这些网络的深度可以任意长

820
00:39:23,840 --> 00:39:26,700
如果在任何时候梯度击中

821
00:39:26,700 --> 00:39:28,700
较低的数字，零， 

822
00:39:29,460 --> 00:39:32,240
成为，神经变得饱和。 

823
00:39:32,300 --> 00:39:34,480
那个渐变，我们称之为饱和， 

824
00:39:34,540 --> 00:39:35,700
渐变得到 - 

825
00:39:37,680 --> 00:39:40,240
将所有较早的层驱动为零， 

826
00:39:41,240 --> 00:39:42,780
所以很容易遇到问题

827
00:39:42,780 --> 00:39:46,680
你真的忽略了大部分序列。 

828
00:39:47,880 --> 00:39:50,260
这只是另一个Python重量， 

829
00:39:51,180 --> 00:39:53,660
sudo称为重量来看待它。 

830
00:39:54,040 --> 00:39:55,520
你有同样的w， 

831
00:39:55,520 --> 00:39:58,120
记住你正在分享权重

832
00:39:58,160 --> 00:40:00,880
和所有参数，不时， 

833
00:40:01,340 --> 00:40:04,680
所以如果重量是这样的话

834
00:40:05,140 --> 00:40:07,340
WHH， 

835
00:40:07,400 --> 00:40:10,480
如果重量是他们产生的

836
00:40:10,540 --> 00:40:11,700
[难以理解] 

837
00:40:14,120 --> 00:40:18,160
它们具有负值

838
00:40:18,220 --> 00:40:21,360
在渐变到零， 

839
00:40:22,080 --> 00:40:24,400
通过其余的传播。 

840
00:40:24,500 --> 00:40:27,140
这是反向传播的sudo-call， 

841
00:40:27,140 --> 00:40:29,140
传递给RNN， 

842
00:40:29,660 --> 00:40:31,080
WHH 

843
00:40:31,920 --> 00:40:33,500
传播回来。 

844
00:40:35,580 --> 00:40:36,860
你得到这个东西

845
00:40:36,860 --> 00:40:39,140
随着爆炸和消失的渐变

846
00:40:40,660 --> 00:40:45,220
例如，单个隐藏单元RNN的错误表面， 

847
00:40:45,220 --> 00:40:47,680
这是可视化的渐变， 

848
00:40:47,680 --> 00:40:51,100
权重的价值，偏见的价值

849
00:40:51,100 --> 00:40:52,080
和错误， 

850
00:40:52,540 --> 00:40:55,500
错误可能非常平坦或可能爆炸， 

851
00:40:56,740 --> 00:40:58,700
两者都将领先

852
00:40:59,860 --> 00:41:02,080
你没有 - 

853
00:41:02,220 --> 00:41:04,420
要么采取过于渐进的步骤

854
00:41:04,420 --> 00:41:05,280
还是太大了

855
00:41:06,580 --> 00:41:08,680
这是几何解释。 

856
00:41:08,680 --> 00:41:11,800
好的。我们看一下其他什么变体？ 

857
00:41:11,950 --> 00:41:13,620
他们[难以理解00:41:13]？ 

858
00:41:13,620 --> 00:41:15,360
它不一定只有一种方式， 

859
00:41:15,380 --> 00:41:16,800
它可以是双向的， 

860
00:41:16,800 --> 00:41:20,420
这可能是前进的边缘和边缘返回

861
00:41:21,080 --> 00:41:22,760
需要什么

862
00:41:22,960 --> 00:41:25,060
是这样的

863
00:41:25,130 --> 00:41:27,770
填写遗失，无论数据是什么， 

864
00:41:27,780 --> 00:41:29,950
填写该数据的缺失元素， 

865
00:41:29,960 --> 00:41:33,730
无论是图像，文字还是音频。 

866
00:41:34,820 --> 00:41:37,540
通常，与神经网络一样， 

867
00:41:37,540 --> 00:41:39,400
它越深越好。 

868
00:41:40,740 --> 00:41:45,140
那深层指的是层数

869
00:41:45,140 --> 00:41:48,080
在一个时间实例中。 

870
00:41:48,080 --> 00:41:50,880
在幻灯片的右侧

871
00:41:50,940 --> 00:41:52,780
我们正在堆叠

872
00:41:54,700 --> 00:41:56,480
时域中的节点。 

873
00:41:58,680 --> 00:42:02,580
每个层都有自己的一组权重， 

874
00:42:02,640 --> 00:42:04,240
它自己的偏见。 

875
00:42:05,340 --> 00:42:06,940
这些都很棒

876
00:42:07,000 --> 00:42:08,540
但他们需要大量的数据

877
00:42:12,840 --> 00:42:15,520
当您以这种方式添加额外的图层时。 

878
00:42:17,440 --> 00:42:20,100
问题是，虽然递归神经网络， 

879
00:42:20,380 --> 00:42:21,300
理论上， 

880
00:42:21,360 --> 00:42:24,580
应该能够学习任何类型的序列， 

881
00:42:25,210 --> 00:42:28,310
现实是他们并不擅长记忆

882
00:42:28,360 --> 00:42:30,020
刚才发生的事， 

883
00:42:30,020 --> 00:42:31,640
长期依赖。 

884
00:42:31,700 --> 00:42:34,740
这是一个愚蠢的例子， 

885
00:42:35,040 --> 00:42:38,140
让我们想一个故事

886
00:42:38,600 --> 00:42:39,460
关于鲍勃， 

887
00:42:40,040 --> 00:42:41,540
鲍勃正在吃一个苹果。 

888
00:42:42,540 --> 00:42:44,540
苹果部分

889
00:42:44,640 --> 00:42:47,140
由递归神经网络生成。 

890
00:42:50,340 --> 00:42:53,600
你的反复神经网络可以学会生成“苹果” 

891
00:42:53,640 --> 00:42:56,400
因为它在很多句子中被看到，有“鲍勃”和“吃” 

892
00:42:56,420 --> 00:42:58,240
它可以生成单词apple。 

893
00:42:59,420 --> 00:43:01,720
对于更长的句子，比如

894
00:43:01,720 --> 00:43:03,040
“鲍勃喜欢苹果， 

895
00:43:03,140 --> 00:43:05,200
他饿了，决定吃零食， 

896
00:43:05,240 --> 00:43:06,860
所以现在他正在吃一个苹果“， 

897
00:43:07,440 --> 00:43:09,440
你必须保持状态

898
00:43:09,500 --> 00:43:11,140
我们在谈论鲍勃

899
00:43:11,440 --> 00:43:13,240
我们在谈论苹果， 

900
00:43:13,240 --> 00:43:14,960
通过几个

901
00:43:14,960 --> 00:43:17,480
谨慎的语义

902
00:43:18,780 --> 00:43:19,860
句子。 

903
00:43:20,760 --> 00:43:23,260
那种长期记忆

904
00:43:23,260 --> 00:43:24,460
不是 - 

905
00:43:25,520 --> 00:43:27,920
因为效果不同， 

906
00:43:27,920 --> 00:43:29,920
但消失的渐变， 

907
00:43:30,920 --> 00:43:32,760
传播很困难

908
00:43:32,760 --> 00:43:35,860
刚才发生的重要事情

909
00:43:35,860 --> 00:43:37,920
为了维持这种背景

910
00:43:37,920 --> 00:43:39,100
在生成“苹果”， 

911
00:43:39,160 --> 00:43:41,560
或者对发生的一些概念进行分类

912
00:43:41,760 --> 00:43:42,880
顺便说一句。 

913
00:43:45,260 --> 00:43:47,040
当人们谈论

914
00:43:49,300 --> 00:43:51,020
递归神经网络

915
00:43:51,240 --> 00:43:55,440
这些天，他们谈论的是LSTM， 

916
00:43:55,640 --> 00:43:59,240
长期短期记忆网络

917
00:44:00,400 --> 00:44:01,970
所有令人印象深刻的结果

918
00:44:02,010 --> 00:44:04,260
关于时间序列和音频和视频的结果

919
00:44:04,260 --> 00:44:07,380
而这一切，都需要LSTM。 

920
00:44:07,860 --> 00:44:09,580
同样，香草RNN 

921
00:44:09,680 --> 00:44:11,200
在幻灯片的顶部， 

922
00:44:13,680 --> 00:44:15,540
每个细胞都很简单， 

923
00:44:16,020 --> 00:44:17,720
有一些隐藏的单位， 

924
00:44:18,200 --> 00:44:20,520
有一个输入，有一个输出。 

925
00:44:21,760 --> 00:44:25,280
在这里，我们使用TANH作为激活函数， 

926
00:44:28,360 --> 00:44:32,880
它只是另一种流行的Sigmoid类型激活功能。 

927
00:44:35,100 --> 00:44:37,880
LSTM更复杂， 

928
00:44:38,620 --> 00:44:40,920
或者他们看起来更复杂但是

929
00:44:42,580 --> 00:44:45,580
在某些方面，它们对我们来说更直观。 

930
00:44:45,880 --> 00:44:48,580
每个单元格中都有一堆门， 

931
00:44:49,620 --> 00:44:50,960
我们将通过这些。 

932
00:44:50,980 --> 00:44:54,260
黄色是不同的神经网络层， 

933
00:44:54,840 --> 00:44:56,740
Sigmoid和TANH， 

934
00:44:56,740 --> 00:45:00,420
只是不同类型的激活功能。 

935
00:45:00,480 --> 00:45:03,220
TANH是一种激活功能

936
00:45:03,260 --> 00:45:07,700
将输入转移到负一对一的范围内。 

937
00:45:08,840 --> 00:45:10,680
Sigmoid功能

938
00:45:10,680 --> 00:45:13,400
在零和一之间搜索它

939
00:45:13,480 --> 00:45:15,540
这有不同的用途。 

940
00:45:15,920 --> 00:45:18,340
有一些有意义的操作， 

941
00:45:18,340 --> 00:45:21,060
加法，乘法， 

942
00:45:21,940 --> 00:45:24,860
并且有联系， 

943
00:45:25,400 --> 00:45:28,240
数据从一层传递到另一层， 

944
00:45:28,680 --> 00:45:30,120
箭头所示。 

945
00:45:31,480 --> 00:45:35,420
有连接，输出上有一个复制操作

946
00:45:35,420 --> 00:45:36,540
我们复制， 

947
00:45:37,580 --> 00:45:40,320
它被复制到下一个单元格的每个单元格的输出

948
00:45:40,380 --> 00:45:41,480
并输出。 

949
00:45:43,060 --> 00:45:45,760
让我试着澄清一下， 

950
00:45:50,480 --> 00:45:51,760
澄清一点。 

951
00:45:54,240 --> 00:45:56,840
有这个输送带

952
00:45:57,180 --> 00:46:00,460
穿过每个单独的细胞内部

953
00:46:00,460 --> 00:46:05,240
他们都有，输送带上真的有三个台阶。 

954
00:46:05,280 --> 00:46:06,740
首先是， 

955
00:46:07,300 --> 00:46:10,480
有一个Sigmoid函数

956
00:46:10,480 --> 00:46:13,260
那是负责决定的

957
00:46:15,820 --> 00:46:18,300
忘记什么，忽略什么， 

958
00:46:18,620 --> 00:46:21,280
它负责

959
00:46:22,340 --> 00:46:26,200
接受输入，新输入，x（t）， 

960
00:46:26,640 --> 00:46:30,700
采取以前的状态， 

961
00:46:31,660 --> 00:46:35,300
前一个单元格的输出，上一个时间步长

962
00:46:36,080 --> 00:46:40,000
并决定“我想把它留在我的记忆中吗？” 

963
00:46:40,000 --> 00:46:41,420
和“我想整合

964
00:46:41,420 --> 00:46:44,180
我的记忆中的新输入与否？“ 

965
00:46:44,480 --> 00:46:45,900
这可以让你

966
00:46:45,980 --> 00:46:49,680
选择性地了解您学到的信息。 

967
00:46:49,680 --> 00:46:50,940
例如， 

968
00:46:50,960 --> 00:46:53,740
那句话“鲍勃和爱丽丝正在吃午饭， 

969
00:46:53,820 --> 00:46:54,840
鲍勃喜欢苹果， 

970
00:46:55,700 --> 00:46:57,200
爱丽丝喜欢橘子， 

971
00:46:57,280 --> 00:46:58,480
她正在吃一个橘子“。 

972
00:47:02,880 --> 00:47:05,320
鲍勃和爱丽丝正在吃午饭， 

973
00:47:05,440 --> 00:47:06,640
鲍勃喜欢苹果， 

974
00:47:06,900 --> 00:47:10,120
现在，如果你说你有一个隐藏的状态， 

975
00:47:10,240 --> 00:47:13,560
跟踪我们所谈论的人的性别

976
00:47:16,000 --> 00:47:19,240
你可能会说第一句话有两种性别， 

977
00:47:19,240 --> 00:47:21,040
第二句中有男性， 

978
00:47:21,060 --> 00:47:23,160
女性在第三句话中， 

979
00:47:23,340 --> 00:47:24,380
那样

980
00:47:24,680 --> 00:47:27,940
当你必须生成一个关于谁在吃什么的句子时， 

981
00:47:27,940 --> 00:47:29,060
你会知道 - 

982
00:47:29,420 --> 00:47:31,920
你保留性别信息

983
00:47:32,720 --> 00:47:36,080
为了准确生成文本

984
00:47:36,120 --> 00:47:37,840
对应于

985
00:47:38,260 --> 00:47:39,560
适当的人。 

986
00:47:39,940 --> 00:47:41,880
你必须忘记某些事情， 

987
00:47:41,920 --> 00:47:44,640
就像忘了Bob那时存在的那样， 

988
00:47:45,800 --> 00:47:49,160
你必须忘记鲍勃喜欢苹果

989
00:47:49,560 --> 00:47:50,940
但你必须记住

990
00:47:51,300 --> 00:47:54,140
爱丽丝喜欢橘子

991
00:47:54,180 --> 00:47:57,500
所以你必须有选择地记住并忘记某些事情

992
00:47:57,500 --> 00:47:59,500
简而言之就是LSTM。 

993
00:47:59,500 --> 00:48:03,880
你决定忘记什么，决定要记住什么

994
00:48:03,940 --> 00:48:06,780
并决定在该单元格中输出什么。 

995
00:48:11,620 --> 00:48:14,680
放大一点，因为这很酷

996
00:48:15,640 --> 00:48:19,540
有一个州穿过牢房， 

997
00:48:20,240 --> 00:48:21,620
这个输送带， 

998
00:48:22,140 --> 00:48:25,140
以前的状态喜欢性别

999
00:48:25,920 --> 00:48:28,320
我们目前正在谈论的， 

1000
00:48:28,560 --> 00:48:31,100
这就是你要跟踪的状态

1001
00:48:31,180 --> 00:48:33,320
那是在细胞中运行的。 

1002
00:48:34,200 --> 00:48:36,580
然后有三个Sigmoid层

1003
00:48:37,180 --> 00:48:40,220
输出一个， 

1004
00:48:40,660 --> 00:48:42,480
零和一之间的数字， 

1005
00:48:42,480 --> 00:48:45,640
一个，当你想要的信息通过

1006
00:48:46,080 --> 00:48:49,660
当你不想让它通过时为零， 

1007
00:48:51,060 --> 00:48:54,260
保持状态的输送带。 

1008
00:48:56,100 --> 00:48:58,360
首先，Sigmoid功能是， 

1009
00:48:58,640 --> 00:49:02,060
我们决定忘记什么，忽略什么， 

1010
00:49:02,060 --> 00:49:03,400
那是第一个， 

1011
00:49:03,400 --> 00:49:06,620
我们从前一个时间步骤中获取输入， 

1012
00:49:06,620 --> 00:49:09,160
网络的输入

1013
00:49:09,180 --> 00:49:10,680
在当前时间步骤

1014
00:49:10,680 --> 00:49:14,420
并决定，我想忘记或者我想忽略那些？ 

1015
00:49:15,560 --> 00:49:18,060
然后我们决定了

1016
00:49:18,260 --> 00:49:20,820
要更新的州的哪一部分， 

1017
00:49:21,100 --> 00:49:24,500
我们需要用这些信息更新记忆的哪一部分

1018
00:49:24,500 --> 00:49:28,120
以及在该更新中插入的值。 

1019
00:49:30,360 --> 00:49:33,900
第三步，我们执行实际更新

1020
00:49:34,280 --> 00:49:36,540
并执行实际遗忘， 

1021
00:49:36,980 --> 00:49:39,900
这就是你有Sigmoid功能的原因， 

1022
00:49:39,900 --> 00:49:41,460
你只是乘以它， 

1023
00:49:42,480 --> 00:49:44,540
什么时候是零是遗忘， 

1024
00:49:44,640 --> 00:49:47,060
什么时候信息通过。 

1025
00:49:49,300 --> 00:49:50,360
最后， 

1026
00:49:50,840 --> 00:49:53,860
我们从细胞产生一个输出， 

1027
00:49:55,180 --> 00:49:58,020
如果它的翻译

1028
00:49:58,500 --> 00:50:01,300
用英语生成输出

1029
00:50:01,340 --> 00:50:03,540
输入是西班牙语的

1030
00:50:03,760 --> 00:50:06,020
然后是相同的输出

1031
00:50:06,460 --> 00:50:08,940
它被复制到下一个单元格。 

1032
00:50:14,060 --> 00:50:17,440
我们可以用这种方法做些什么？ 

1033
00:50:18,340 --> 00:50:20,420
我们可以看一下机器翻译。 

1034
00:50:20,460 --> 00:50:21,980
我想我正在努力 - 

1035
00:50:23,660 --> 00:50:24,740
题。 

1036
00:50:25,300 --> 00:50:27,600
你对这个州的代表是什么？ 

1037
00:50:27,600 --> 00:50:29,070
它是否像浮点

1038
00:50:29,110 --> 00:50:30,440
或者它就像一个矢量

1039
00:50:30,440 --> 00:50:32,440
或者究竟是什么？ 

1040
00:50:33,680 --> 00:50:35,040
国家

1041
00:50:35,540 --> 00:50:38,600
是激活

1042
00:50:40,240 --> 00:50:41,600
乘以重量， 

1043
00:50:41,640 --> 00:50:45,900
它是Sigmoid或TANH激活的输出。 

1044
00:50:47,320 --> 00:50:50,080
有一堆神经元，他们正在发射一个数字

1045
00:50:50,080 --> 00:50:52,960
在负一个或一个之间，或在零到一之间， 

1046
00:50:53,320 --> 00:50:54,960
整个都是一个国家。 

1047
00:50:55,020 --> 00:50:58,080
只是把它称之为一种简化的状态， 

1048
00:50:58,120 --> 00:50:59,520
但重点是，有

1049
00:50:59,520 --> 00:51:02,960
一堆数字经常被权重修改

1050
00:51:03,400 --> 00:51:04,480
和偏见， 

1051
00:51:05,420 --> 00:51:07,340
这些数字持有国家

1052
00:51:09,060 --> 00:51:11,240
并修改这些数字

1053
00:51:11,340 --> 00:51:13,380
由权重控制

1054
00:51:14,140 --> 00:51:16,260
一旦完成所有这一切， 

1055
00:51:16,600 --> 00:51:18,300
结果输出

1056
00:51:18,540 --> 00:51:20,440
复发神经网络

1057
00:51:20,520 --> 00:51:22,580
它与所需的输出进行比较

1058
00:51:22,760 --> 00:51:25,540
并将错误反向传播到权重。 

1059
00:51:27,840 --> 00:51:29,400
希望这是有道理的。 

1060
00:51:30,940 --> 00:51:34,300
因此，机器翻译是一种流行的应用程序

1061
00:51:37,200 --> 00:51:39,560
所有这一切都是一样的， 

1062
00:51:40,080 --> 00:51:42,520
我所谈到的所有这些网络， 

1063
00:51:42,560 --> 00:51:45,080
他们是非常相似的结构。 

1064
00:51:46,240 --> 00:51:48,460
你有一些投入， 

1065
00:51:48,980 --> 00:51:51,300
无论再说什么语言， 

1066
00:51:51,640 --> 00:51:55,660
德国人也许，我认为一切都是德国人， 

1067
00:51:58,680 --> 00:51:59,780
和输出。 

1068
00:52:00,020 --> 00:52:03,560
输入是一种语言， 

1069
00:52:03,600 --> 00:52:05,700
一组人物

1070
00:52:05,760 --> 00:52:08,100
用一种语言组成一个单词， 

1071
00:52:08,140 --> 00:52:09,980
有一个国家正在传播

1072
00:52:10,620 --> 00:52:12,300
一旦那句话结束， 

1073
00:52:12,340 --> 00:52:14,670
你开始，而不是收集投入， 

1074
00:52:14,680 --> 00:52:16,080
开始产生产出

1075
00:52:16,120 --> 00:52:18,600
你可以用英语输出。 

1076
00:52:19,820 --> 00:52:23,460
机器翻译有很多很棒的工作。 

1077
00:52:23,480 --> 00:52:26,680
这是谷歌应该用于他们的翻译， 

1078
00:52:26,680 --> 00:52:27,800
一样。 

1079
00:52:28,380 --> 00:52:30,260
我以前说过这个

1080
00:52:30,880 --> 00:52:32,680
但现在你们都知道它是如何工作的， 

1081
00:52:32,700 --> 00:52:34,860
同样的事情，LSTMs 

1082
00:52:35,080 --> 00:52:37,800
生成手写字符， 

1083
00:52:37,800 --> 00:52:40,040
任意风格的手写， 

1084
00:52:40,040 --> 00:52:42,060
控制绘图， 

1085
00:52:43,780 --> 00:52:47,100
输入是文本，输出是手写。 

1086
00:52:47,160 --> 00:52:49,060
再次，同样的

1087
00:52:50,220 --> 00:52:53,280
这里有一些深度的网络， 

1088
00:52:53,280 --> 00:52:55,280
输入是文字， 

1089
00:52:55,400 --> 00:52:58,200
输出是写作的控制。 

1090
00:53:00,020 --> 00:53:02,000
字符级文本生成， 

1091
00:53:02,960 --> 00:53:04,540
这是

1092
00:53:05,000 --> 00:53:06,960
告诉我们生活的东西， 

1093
00:53:07,180 --> 00:53:09,060
生命的意义， 

1094
00:53:09,120 --> 00:53:13,300
文学认同与古代人类繁衍的传统。 

1095
00:53:13,540 --> 00:53:16,340
那是同样的过程， 

1096
00:53:16,680 --> 00:53:19,300
当时输入一个字符， 

1097
00:53:19,700 --> 00:53:23,580
我们看到的是输入图层上的字符编码， 

1098
00:53:23,860 --> 00:53:25,440
有一个隐藏的状态， 

1099
00:53:25,740 --> 00:53:28,950
跟踪那些激活的隐藏层， 

1100
00:53:29,000 --> 00:53:30,340
产出

1101
00:53:31,160 --> 00:53:35,140
激活功能和每一个

1102
00:53:37,620 --> 00:53:40,300
它正在输出的时间

1103
00:53:40,580 --> 00:53:42,700
最好的预测

1104
00:53:42,780 --> 00:53:44,700
随后的下一个字符。 

1105
00:53:45,060 --> 00:53:47,060
现在，在很多这些应用程序上

1106
00:53:47,560 --> 00:53:49,220
你想忽略输出

1107
00:53:49,420 --> 00:53:53,000
直到输入句结束

1108
00:53:53,320 --> 00:53:55,860
然后你开始听输出， 

1109
00:53:55,860 --> 00:53:58,680
但关键是它只是不断生成文本， 

1110
00:53:58,680 --> 00:54:00,680
无论是否给予输入， 

1111
00:54:00,820 --> 00:54:02,280
所以你产生了输入

1112
00:54:02,360 --> 00:54:04,280
只是添加，转向

1113
00:54:04,680 --> 00:54:06,360
递归神经网络。 

1114
00:54:07,120 --> 00:54:08,800
你可以回答问题

1115
00:54:11,480 --> 00:54:12,980
关于图像， 

1116
00:54:13,600 --> 00:54:15,240
你到那里的输入， 

1117
00:54:15,300 --> 00:54:18,060
你几乎可以把任意东西叠在一起， 

1118
00:54:18,100 --> 00:54:21,220
你把图像作为输入，左下角， 

1119
00:54:21,920 --> 00:54:24,460
把它放在卷积神经网络中， 

1120
00:54:26,640 --> 00:54:29,600
并提出问题。 

1121
00:54:31,180 --> 00:54:33,400
有一些叫做嵌入字的东西， 

1122
00:54:33,460 --> 00:54:35,140
这是为了扩大

1123
00:54:35,400 --> 00:54:37,600
这些词的代表意义。 

1124
00:54:37,660 --> 00:54:40,160
“有多少本书？”是个问题。 

1125
00:54:40,220 --> 00:54:42,120
你想要嵌入这个词

1126
00:54:42,200 --> 00:54:43,380
和图像

1127
00:54:43,580 --> 00:54:44,600
并生产

1128
00:54:45,200 --> 00:54:46,920
你对答案的最佳估计。 

1129
00:54:46,980 --> 00:54:47,880
对于问题

1130
00:54:48,100 --> 00:54:49,660
“猫是什么颜色的？” 

1131
00:54:50,040 --> 00:54:51,720
它可能是灰色或黑色， 

1132
00:54:51,960 --> 00:54:54,300
这是不同的LSTM口味

1133
00:54:54,500 --> 00:54:55,800
产生这个答案。 

1134
00:54:56,180 --> 00:54:57,860
与计数椅子相同

1135
00:54:58,040 --> 00:55:00,100
你可以给一张椅子的图像

1136
00:55:00,780 --> 00:55:03,300
并且问题是“那里有多少把椅子？” 

1137
00:55:03,300 --> 00:55:06,540
它可以产生“三”的答案。 

1138
00:55:08,420 --> 00:55:11,060
我应该说这真的很难， 

1139
00:55:11,100 --> 00:55:12,820
任意问题

1140
00:55:12,880 --> 00:55:14,200
询问任意图像， 

1141
00:55:14,200 --> 00:55:15,650
你们都在诠释 - 

1142
00:55:15,680 --> 00:55:17,840
你正在做自然语言处理

1143
00:55:17,880 --> 00:55:20,580
而且你在一个网络中进行计算机视觉。 

1144
00:55:22,340 --> 00:55:25,640
与图像捕获生成相同， 

1145
00:55:26,180 --> 00:55:28,640
你可以检测到

1146
00:55:28,760 --> 00:55:30,660
场景中的不同对象， 

1147
00:55:31,300 --> 00:55:32,880
生成这些词， 

1148
00:55:33,580 --> 00:55:37,760
在语法正确的句子中将它们拼接在一起

1149
00:55:37,840 --> 00:55:39,940
并重新排列句子。 

1150
00:55:40,000 --> 00:55:41,900
所有这些都是LSTM， 

1151
00:55:41,940 --> 00:55:43,470
第二步和第三步， 

1152
00:55:43,490 --> 00:55:46,400
首先是检测物体的计算机视觉， 

1153
00:55:46,400 --> 00:55:48,690
分割图像和检测物体， 

1154
00:55:48,700 --> 00:55:51,010
这样你就可以生成一个标题

1155
00:55:51,020 --> 00:55:54,300
“一个男人坐在椅子上，狗在他的腿上”。 

1156
00:55:56,900 --> 00:55:59,180
再次，LSTMs视频。 

1157
00:56:00,740 --> 00:56:02,440
视频的标题生成， 

1158
00:56:03,200 --> 00:56:07,020
输入，每一帧都是一个图像

1159
00:56:07,100 --> 00:56:08,500
进入LSTM， 

1160
00:56:08,560 --> 00:56:09,720
输入是图像

1161
00:56:11,240 --> 00:56:13,660
输出是一组字符。 

1162
00:56:13,760 --> 00:56:15,160
首先，你加载视频， 

1163
00:56:15,440 --> 00:56:17,140
在这种情况下，输出在顶部， 

1164
00:56:18,040 --> 00:56:19,260
你编码

1165
00:56:19,880 --> 00:56:20,980
该视频

1166
00:56:21,640 --> 00:56:24,660
进入网络内部的表示

1167
00:56:24,660 --> 00:56:26,660
然后你开始生成单词

1168
00:56:26,720 --> 00:56:27,960
关于那个视频。 

1169
00:56:28,020 --> 00:56:31,600
首先是输入，编码阶段，然后是解码阶段。 

1170
00:56:32,700 --> 00:56:33,740
接收视频， 

1171
00:56:33,920 --> 00:56:35,540
说一个男人正在服用， 

1172
00:56:35,720 --> 00:56:37,140
说话，无论如何

1173
00:56:38,020 --> 00:56:41,500
因为输入和输出是任意的， 

1174
00:56:41,540 --> 00:56:44,600
还必须有开头的指标和

1175
00:56:44,680 --> 00:56:46,020
一句话的结尾， 

1176
00:56:46,360 --> 00:56:47,980
在这种情况下，句子结束。 

1177
00:56:48,680 --> 00:56:50,400
你想知道什么时候停下来

1178
00:56:51,520 --> 00:56:54,520
为了生成语法正确的句子。 

1179
00:56:54,520 --> 00:56:57,400
表示句子结束。您还希望能够生成句点

1180
00:57:01,500 --> 00:57:04,580
您还可以再次使用递归神经网络， 

1181
00:57:04,940 --> 00:57:06,260
LSTM在这里， 

1182
00:57:06,260 --> 00:57:07,400
控制

1183
00:57:07,800 --> 00:57:09,940
转向

1184
00:57:11,180 --> 00:57:14,460
图像上的滑动窗口

1185
00:57:15,360 --> 00:57:19,560
用于对该图像中包含的内容进行分类。 

1186
00:57:19,560 --> 00:57:23,560
这里，CNN由循环神经网络操纵

1187
00:57:25,080 --> 00:57:28,600
为了转换这个图像

1188
00:57:28,620 --> 00:57:32,000
到与门牌号码相关联的号码， 

1189
00:57:33,300 --> 00:57:35,000
它被称为视觉注意力。 

1190
00:57:35,000 --> 00:57:38,020
视觉注意可以用来引导

1191
00:57:38,040 --> 00:57:39,800
对于感知方面

1192
00:57:39,900 --> 00:57:43,080
它可以用来引导网络进行生成。 

1193
00:57:43,140 --> 00:57:43,820
在右边， 

1194
00:57:44,260 --> 00:57:47,120
我们可以生成一个图像 - 

1195
00:57:50,700 --> 00:57:52,180
所以网络的输出 - 

1196
00:57:52,240 --> 00:57:53,720
这是一个LSTM 

1197
00:57:54,320 --> 00:57:56,940
每个时间步的输出

1198
00:57:57,640 --> 00:57:58,780
是视觉的， 

1199
00:57:59,720 --> 00:58:02,260
这样你就可以绘制数字。 

1200
00:58:06,080 --> 00:58:07,200
这里， 

1201
00:58:09,840 --> 00:58:11,280
我之前提过这个， 

1202
00:58:12,780 --> 00:58:15,780
正在输入无声视频， 

1203
00:58:15,920 --> 00:58:17,240
图像序列

1204
00:58:19,580 --> 00:58:21,160
并产生音频。 

1205
00:58:22,500 --> 00:58:23,920
这是

1206
00:58:25,280 --> 00:58:26,620
LSTM 

1207
00:58:28,960 --> 00:58:31,840
每个帧都有卷积层， 

1208
00:58:32,760 --> 00:58:34,740
将图像作为输入

1209
00:58:35,620 --> 00:58:36,940
并生产

1210
00:58:37,820 --> 00:58:40,460
频谱图，音频作为输出。 

1211
00:58:45,780 --> 00:58:49,880
训练集是用鼓槌敲击物体的人

1212
00:58:49,880 --> 00:58:53,580
你的任务是生成一个无声的视频， 

1213
00:58:53,740 --> 00:58:57,500
产生鼓槌会发出的声音

1214
00:58:57,500 --> 00:59:00,260
与该物体接触时。 

1215
00:59:03,040 --> 00:59:04,700
医学诊断， 

1216
00:59:06,520 --> 00:59:07,360
那实际上是 - 

1217
00:59:07,380 --> 00:59:10,470
我已经列出了一些非常成功的地方

1218
00:59:10,500 --> 00:59:11,340
非常酷， 

1219
00:59:11,370 --> 00:59:13,300
但它也开始应用

1220
00:59:13,340 --> 00:59:15,580
在哪里的地方

1221
00:59:16,660 --> 00:59:18,020
实际上可以

1222
00:59:19,500 --> 00:59:21,000
真有帮助

1223
00:59:22,320 --> 00:59:25,840
文明，在医疗应用中。 

1224
00:59:25,920 --> 00:59:27,840
用于医疗诊断

1225
00:59:28,940 --> 00:59:30,840
有

1226
00:59:31,220 --> 00:59:33,900
高度的桅杆和

1227
00:59:35,780 --> 00:59:37,220
可变长度

1228
00:59:38,160 --> 00:59:41,020
形式的信息序列， 

1229
00:59:41,080 --> 00:59:43,500
例如，患者电子健康记录。 

1230
00:59:43,500 --> 00:59:45,420
所以，每次去看医生， 

1231
00:59:45,500 --> 00:59:48,340
正在进行测试，信息就在那里

1232
00:59:48,400 --> 00:59:51,780
你可以把它看作一段时间的序列

1233
00:59:51,780 --> 00:59:54,680
然后给出数据，这是输入， 

1234
00:59:55,240 --> 00:59:57,400
输出是诊断， 

1235
00:59:58,980 --> 01:00:00,240
医学诊断， 

1236
01:00:00,300 --> 01:00:03,780
在这种情况下，我们可以看看预测糖尿病， 

1237
01:00:04,640 --> 01:00:07,280
脊柱侧凸，哮喘等， 

1238
01:00:09,140 --> 01:00:10,700
具有非常好的准确性。 

1239
01:00:13,740 --> 01:00:14,960
有一些东西

1240
01:00:15,740 --> 01:00:17,260
我们所有人都希望我们能做到， 

1241
01:00:20,220 --> 01:00:22,780
是股市预测。 

1242
01:00:25,580 --> 01:00:27,100
你可以输入， 

1243
01:00:27,180 --> 01:00:30,680
例如，首先，您可以输入原始库存数据， 

1244
01:00:30,680 --> 01:00:33,920
[难以理解的01:00:30]书籍等，财务数据， 

1245
01:00:33,940 --> 01:00:37,300
但您也可以查看来自网络的新闻报道

1246
01:00:38,240 --> 01:00:40,600
并将这些作为输入，如此处所示， 

1247
01:00:40,680 --> 01:00:42,280
在X轴上是时间， 

1248
01:00:42,280 --> 01:00:44,120
来自不同日子的文章， 

1249
01:00:45,620 --> 01:00:47,420
LSTM，再一次， 

1250
01:00:48,920 --> 01:00:51,740
并产生预测的输出， 

1251
01:00:52,040 --> 01:00:55,140
二元预测，股票是上涨还是下跌。 

1252
01:00:56,870 --> 01:00:59,500
没有人能够真正成功地做到这一点

1253
01:00:59,540 --> 01:01:02,220
但是有很多结果

1254
01:01:02,880 --> 01:01:04,760
并尝试随机执行

1255
01:01:06,840 --> 01:01:09,600
这是你赚钱的方式， 

1256
01:01:10,500 --> 01:01:12,220
显着高于随机

1257
01:01:12,340 --> 01:01:14,540
在预测它上升或下降？ 

1258
01:01:14,600 --> 01:01:15,900
所以你可以买卖

1259
01:01:16,680 --> 01:01:18,020
特别是

1260
01:01:18,280 --> 01:01:19,080
当有 - 

1261
01:01:19,120 --> 01:01:22,560
在发生崩溃的情况下，更容易预测， 

1262
01:01:23,380 --> 01:01:25,760
所以你可以预测一次闯入的崩溃。 

1263
01:01:26,080 --> 01:01:27,800
这些显示在表中， 

1264
01:01:27,980 --> 01:01:30,400
不同股票的错误率， 

1265
01:01:31,740 --> 01:01:33,100
汽车股。 

1266
01:01:35,680 --> 01:01:38,340
你也可以生成音频， 

1267
01:01:38,340 --> 01:01:41,200
是与生成语言完全相同的过程， 

1268
01:01:41,200 --> 01:01:42,520
你生成音频。 

1269
01:01:42,580 --> 01:01:44,280
这是受过训练的

1270
01:01:45,880 --> 01:01:47,580
一个扬声器， 

1271
01:01:47,860 --> 01:01:50,920
几个小时的史诗

1272
01:01:50,980 --> 01:01:52,080
他们说话

1273
01:01:52,600 --> 01:01:57,060
而你只是学习，这是扬声器的原始音频

1274
01:01:58,920 --> 01:02:03,480
它正在慢慢地学习生成

1275
01:02:03,840 --> 01:02:07,940
[音频] 

1276
01:02:19,860 --> 01:02:22,660
显然，他们正在阅读数字。 

1277
01:02:26,140 --> 01:02:28,720
这是不可思议的，这是训练有素的

1278
01:02:28,880 --> 01:02:34,120
在音频，原始音频的压缩频谱图上

1279
01:02:35,760 --> 01:02:38,240
并正在生产一些东西

1280
01:02:38,780 --> 01:02:43,340
只有少数史诗正在产生听起来像单词的东西， 

1281
01:02:43,340 --> 01:02:45,860
我希望它可以为我做这个讲座。 

1282
01:02:59,840 --> 01:03:00,880
这真太了不起了， 

1283
01:03:02,000 --> 01:03:03,980
这是原始输入， 

1284
01:03:04,340 --> 01:03:05,840
原始输出， 

1285
01:03:06,100 --> 01:03:07,860
再一次，LSTMs， 

1286
01:03:10,500 --> 01:03:13,080
在语音识别方面有很多工作， 

1287
01:03:13,120 --> 01:03:15,760
音频识别。你在映射 - 

1288
01:03:20,660 --> 01:03:22,200
让我把它打开。 

1289
01:03:22,340 --> 01:03:25,540
您正在将任何类型的音频映射到分类， 

1290
01:03:29,640 --> 01:03:32,200
你可以采取道路的音频

1291
01:03:35,680 --> 01:03:39,520
这是显示在底部的频谱图

1292
01:03:39,580 --> 01:03:42,240
你可以检测到道路是否潮湿

1293
01:03:42,300 --> 01:03:44,020
潮湿或道路干燥。 

1294
01:03:47,640 --> 01:03:50,240
你可以做同样的事情

1295
01:03:51,400 --> 01:03:54,620
认识到发言者的性别

1296
01:03:54,700 --> 01:03:58,040
或识别多对多的地图

1297
01:03:58,080 --> 01:03:59,940
所说的实际单词， 

1298
01:04:00,200 --> 01:04:01,760
语音识别。 

1299
01:04:02,560 --> 01:04:04,220
这是关于驾驶， 

1300
01:04:04,280 --> 01:04:07,800
所以让我们看看复发神经的位置网络适​​用于驾驶。 

1301
01:04:08,580 --> 01:04:11,860
我们谈到了NVIDIA的方法， 

1302
01:04:12,200 --> 01:04:16,260
实际上是DeepTeslaJS的东西， 

1303
01:04:16,280 --> 01:04:19,000
它是一个简单的卷积神经网络， 

1304
01:04:19,040 --> 01:04:21,460
有五个卷积层

1305
01:04:21,500 --> 01:04:24,940
在他们的方法中，三个完全连接的层， 

1306
01:04:25,000 --> 01:04:28,320
您可以在DeepTesla中添加任意数量的图层， 

1307
01:04:29,380 --> 01:04:32,220
这是百万分之二

1308
01:04:32,540 --> 01:04:35,000
要优化的参数

1309
01:04:35,020 --> 01:04:36,920
你要拍的只是一张图片， 

1310
01:04:36,990 --> 01:04:39,220
没有时间信息，单个图像

1311
01:04:39,240 --> 01:04:42,160
并产生转向角，这就是方法， 

1312
01:04:42,190 --> 01:04:44,360
那是DeepTesla的方式， 

1313
01:04:47,500 --> 01:04:49,280
拍摄一张照片

1314
01:04:50,060 --> 01:04:53,800
图像和学习转向角的回归。 

1315
01:04:55,280 --> 01:04:56,840
其中一个

1316
01:04:57,920 --> 01:05:01,320
比赛的奖品是Udacity，自驾车

1317
01:05:01,440 --> 01:05:03,900
汽车工程师nanodegree 

1318
01:05:04,700 --> 01:05:05,940
免费， 

1319
01:05:06,440 --> 01:05:07,620
这件事太棒了， 

1320
01:05:07,720 --> 01:05:09,860
我鼓励大家查看一下， 

1321
01:05:09,860 --> 01:05:11,480
但他们做了一场比赛

1322
01:05:15,080 --> 01:05:17,120
这跟我们的非常相似

1323
01:05:18,580 --> 01:05:22,820
但是一大群痴迷者， 

1324
01:05:23,690 --> 01:05:26,270
他们非常聪明，他们超越了

1325
01:05:26,300 --> 01:05:28,800
预测转向的卷积神经网络， 

1326
01:05:28,820 --> 01:05:31,690
拍摄一系列图像并预测转向， 

1327
01:05:31,820 --> 01:05:34,660
他们所做的是，获奖者， 

1328
01:05:34,660 --> 01:05:39,180
至少第一次，我将谈论明天的第二名， 

1329
01:05:39,820 --> 01:05:43,360
在3D卷积神经网络上， 

1330
01:05:43,420 --> 01:05:46,380
第一名和第三名获奖者使用RNN， 

1331
01:05:46,420 --> 01:05:50,320
使用LSTMs，循环神经网络

1332
01:05:50,320 --> 01:05:53,660
并映射一系列图像

1333
01:05:53,680 --> 01:05:55,380
到一系列转向角。 

1334
01:05:55,880 --> 01:05:57,500
对任何人， 

1335
01:05:58,340 --> 01:06:00,160
从统计学上讲， 

1336
01:06:00,440 --> 01:06:03,560
这里的任何人都不是计算机视觉的人， 

1337
01:06:03,600 --> 01:06:06,580
对于任何应用程序，最有可能你想要使用什么

1338
01:06:06,580 --> 01:06:08,020
你有兴趣， 

1339
01:06:08,060 --> 01:06:09,240
是RNN， 

1340
01:06:09,600 --> 01:06:12,700
世界充满了时间序列数据， 

1341
01:06:13,200 --> 01:06:15,420
我们很少有人在努力

1342
01:06:16,000 --> 01:06:18,680
没有时间序列数据的数据， 

1343
01:06:18,740 --> 01:06:21,120
事实上，只要它只是快照， 

1344
01:06:21,440 --> 01:06:24,460
你真的只是把问题减少到了

1345
01:06:24,460 --> 01:06:26,220
你可以处理的大小

1346
01:06:26,260 --> 01:06:27,380
但大多数数据

1347
01:06:27,460 --> 01:06:29,800
在世界上是时间序列数据。 

1348
01:06:29,860 --> 01:06:32,600
这是您最终使用的方法

1349
01:06:32,680 --> 01:06:36,000
如果你想在自己的研究中应用它， 

1350
01:06:41,320 --> 01:06:43,300
RNN是要走的路。 

1351
01:06:46,700 --> 01:06:49,760
再一次，他们在做什么？ 

1352
01:06:49,960 --> 01:06:53,700
你怎么放图像

1353
01:06:53,780 --> 01:06:56,100
进入一个递归的神经网络？ 

1354
01:06:56,180 --> 01:06:58,200
这是同一件事， 

1355
01:06:58,640 --> 01:06:59,780
你拿， 

1356
01:07:00,300 --> 01:07:02,800
你必须将图像转换为数字

1357
01:07:02,880 --> 01:07:04,300
以某种方式， 

1358
01:07:04,600 --> 01:07:07,980
一种强有力的方法是卷积神经网络， 

1359
01:07:08,040 --> 01:07:09,340
所以你可以参加

1360
01:07:10,300 --> 01:07:13,060
三维卷积神经网络

1361
01:07:13,100 --> 01:07:15,780
或2D卷积神经网络

1362
01:07:15,840 --> 01:07:18,740
一旦需要时间考虑和诸如此类的， 

1363
01:07:18,800 --> 01:07:20,380
处理那个图像

1364
01:07:20,500 --> 01:07:22,980
提取该图像的表示

1365
01:07:23,700 --> 01:07:26,880
这成为LSTM的输入

1366
01:07:27,220 --> 01:07:28,500
和输出

1367
01:07:28,540 --> 01:07:31,360
在每一个单元格，每一个时间步， 

1368
01:07:31,380 --> 01:07:33,020
是预测的转向角， 

1369
01:07:33,050 --> 01:07:35,000
车辆的速度和扭矩

1370
01:07:35,020 --> 01:07:37,020
这是第一名获胜者所做的， 

1371
01:07:37,050 --> 01:07:38,990
他们不只是做转向角， 

1372
01:07:39,030 --> 01:07:41,200
也做了速度和扭矩

1373
01:07:42,000 --> 01:07:44,720
以及他们使用的序列长度

1374
01:07:45,300 --> 01:07:47,640
用于培训和测试， 

1375
01:07:48,060 --> 01:07:49,520
对于输入和输出， 

1376
01:07:49,580 --> 01:07:51,320
序列长度为10 

1377
01:07:52,860 --> 01:07:55,140
他们是否使用过监督学习

1378
01:07:55,180 --> 01:07:57,780
或者他们是否使用过强化学习？ 

1379
01:07:57,780 --> 01:08:00,740
问题是，他们是否使用过监督学习？ 

1380
01:08:00,800 --> 01:08:03,820
是的，他们被给予与DeepTesla相同的东西， 

1381
01:08:03,820 --> 01:08:06,540
一系列帧，其序列为

1382
01:08:06,930 --> 01:08:08,680
转向角，速度和扭矩， 

1383
01:08:08,680 --> 01:08:11,400
我认为还有其他信息， 

1384
01:08:12,720 --> 01:08:15,140
这里没有强化学习。 

1385
01:08:15,180 --> 01:08:15,700
题。 

1386
01:08:15,700 --> 01:08:18,940
你有多少信息感吗？ 

1387
01:08:19,000 --> 01:08:21,780
正在通过，有多少LSTM门

1388
01:08:21,840 --> 01:08:23,640
在这个问题？ 

1389
01:08:27,740 --> 01:08:28,820
问题是， 

1390
01:08:28,820 --> 01:08:31,440
这个问题有多少个LSTM门？ 

1391
01:08:33,820 --> 01:08:36,280
这个网络， 

1392
01:08:41,180 --> 01:08:42,920
这是真的

1393
01:08:43,000 --> 01:08:45,140
这个图有点隐藏

1394
01:08:45,250 --> 01:08:48,210
这里的参数数量，但它是任意的

1395
01:08:48,220 --> 01:08:51,280
就像卷积神经网络是任意的， 

1396
01:08:52,380 --> 01:08:55,060
输入的大小是任意的， 

1397
01:08:55,100 --> 01:08:57,660
Sigmoid函数的大小， 

1398
01:08:57,700 --> 01:08:59,260
TANH是任意的， 

1399
01:08:59,280 --> 01:09:02,440
所以你可以根据自己的需要尽可能大地制作它

1400
01:09:02,440 --> 01:09:04,440
越深越好。 

1401
01:09:05,600 --> 01:09:07,900
这些人实际使用了什么 - 

1402
01:09:08,120 --> 01:09:10,160
这些比赛的运作方式

1403
01:09:10,970 --> 01:09:14,200
如果你对机器学习感兴趣，我鼓励你

1404
01:09:14,200 --> 01:09:15,820
参加Kaggle， 

1405
01:09:15,820 --> 01:09:19,120
我不知道如何发音，比赛

1406
01:09:19,360 --> 01:09:22,040
基本上每个人都在做同样的事情， 

1407
01:09:22,040 --> 01:09:24,040
你正在使用LSTM 

1408
01:09:24,080 --> 01:09:25,660
或者如果它是一对一的映射， 

1409
01:09:25,680 --> 01:09:28,980
使用卷积神经网络完全连接网络

1410
01:09:28,990 --> 01:09:30,740
有一些聪明的预处理

1411
01:09:30,760 --> 01:09:32,840
整个工作需要几个月

1412
01:09:32,870 --> 01:09:34,550
你可能，如果你是研究员， 

1413
01:09:34,580 --> 01:09:36,570
这就是你自己研究的内容， 

1414
01:09:36,580 --> 01:09:37,900
玩参数， 

1415
01:09:37,930 --> 01:09:40,000
玩预处理数据， 

1416
01:09:40,040 --> 01:09:43,400
使用控制网络大小的不同参数

1417
01:09:43,430 --> 01:09:44,560
学习率， 

1418
01:09:44,590 --> 01:09:47,060
我已经提到过，这种类型的优化器， 

1419
01:09:47,060 --> 01:09:49,890
所有这些东西，这就是你正在玩的东西， 

1420
01:09:49,920 --> 01:09:51,550
用你自己的人类直觉

1421
01:09:51,640 --> 01:09:53,780
而你正在使用你的 - 

1422
01:09:56,920 --> 01:09:58,660
无论你能做什么探测

1423
01:09:58,700 --> 01:10:01,020
监控网络的性能

1424
01:10:01,400 --> 01:10:02,600
通过时间。 

1425
01:10:03,940 --> 01:10:04,640
是？ 

1426
01:10:17,020 --> 01:10:18,180
问题是， 

1427
01:10:20,820 --> 01:10:23,040
你说有一个

1428
01:10:23,080 --> 01:10:26,560
在这个LCM中记忆第十， 

1429
01:10:26,660 --> 01:10:30,760
我认为RNN应该是任意的。 

1430
01:10:32,740 --> 01:10:34,580
它必须这样做

1431
01:10:35,480 --> 01:10:36,920
通过培训， 

1432
01:10:37,580 --> 01:10:39,540
如何训练网络。 

1433
01:10:39,600 --> 01:10:41,290
它训练有10个序列。 

1434
01:10:41,340 --> 01:10:44,070
结构仍然相同，你只有一个

1435
01:10:44,420 --> 01:10:46,390
正在相互循环的细胞。 

1436
01:10:46,480 --> 01:10:47,840
但问题是， 

1437
01:10:48,180 --> 01:10:49,860
在什么块， 

1438
01:10:51,480 --> 01:10:53,700
序列的大小是多少

1439
01:10:53,720 --> 01:10:56,620
我们应该在培训和测试中做。 

1440
01:10:57,540 --> 01:10:59,580
它可以是任意长度。 

1441
01:10:59,640 --> 01:11:01,960
保持一致通常更好

1442
01:11:02,020 --> 01:11:03,740
并有一个固定的长度。 

1443
01:11:07,060 --> 01:11:10,440
你没有将10个细胞堆叠在一起。 

1444
01:11:10,560 --> 01:11:12,300
它只是一个单元格。 

1445
01:11:16,460 --> 01:11:18,300
第三名获胜者， 

1446
01:11:19,300 --> 01:11:20,520
司机小组， 

1447
01:11:21,860 --> 01:11:24,100
使用称为转移学习的东西

1448
01:11:24,160 --> 01:11:26,600
这是我认为我没有提到的事情

1449
01:11:27,280 --> 01:11:29,040
但它有点暗示， 

1450
01:11:31,900 --> 01:11:34,140
神经网络的惊人力量。 

1451
01:11:35,000 --> 01:11:37,700
首先，您需要大量数据才能执行任何操作。 

1452
01:11:37,940 --> 01:11:40,960
这就是成本，这是神经网络的限制。 

1453
01:11:40,960 --> 01:11:43,100
但你能做的是， 

1454
01:11:43,160 --> 01:11:45,200
有

1455
01:11:46,840 --> 01:11:49,980
已经在非常大的数据集上训练的神经网络。 

1456
01:11:49,980 --> 01:11:50,760
ImageNet， 

1457
01:11:51,920 --> 01:11:56,060
Vdg Net，AlexNet，ResNet， 

1458
01:11:56,100 --> 01:11:59,480
所有这些网络都接受了大量数据的培训。 

1459
01:12:01,200 --> 01:12:03,640
这些网络经过培训可以讲述

1460
01:12:03,700 --> 01:12:07,440
猫与狗之间的差异具体光学识别

1461
01:12:07,440 --> 01:12:08,380
单个图像。 

1462
01:12:09,200 --> 01:12:11,200
我如何接受该网络

1463
01:12:11,280 --> 01:12:12,680
并将其应用于我的问题， 

1464
01:12:12,720 --> 01:12:14,660
说到驾驶或长度检测， 

1465
01:12:14,740 --> 01:12:18,560
或医学诊断，或癌症与否？ 

1466
01:12:19,460 --> 01:12:21,100
神经网络之美， 

1467
01:12:26,160 --> 01:12:28,200
转学的承诺， 

1468
01:12:28,200 --> 01:12:30,400
是你可以采取该网络， 

1469
01:12:30,400 --> 01:12:32,480
切掉最后一层， 

1470
01:12:33,060 --> 01:12:34,520
完全连接的层

1471
01:12:34,520 --> 01:12:37,000
那些很酷的地图

1472
01:12:37,120 --> 01:12:40,800
您从视觉空间中学到的高维特征， 

1473
01:12:41,360 --> 01:12:44,720
而不是预测猫与狗， 

1474
01:12:44,720 --> 01:12:47,480
你教它预测癌症或没有癌症。 

1475
01:12:47,940 --> 01:12:52,240
你教它预测车道或没有车道，卡车或没有卡车。 

1476
01:12:53,240 --> 01:12:54,980
只要视觉空间

1477
01:12:55,020 --> 01:12:56,750
该网络运作的基础

1478
01:12:56,800 --> 01:13:00,340
是类似的或数据，如果它是音频或其他什么

1479
01:13:00,440 --> 01:13:03,600
如果它是相似的，如果功能有用，那么你学习， 

1480
01:13:04,380 --> 01:13:07,400
深入研究猫与狗的问题， 

1481
01:13:07,860 --> 01:13:10,700
你实际上已经学会了如何看世界。 

1482
01:13:10,700 --> 01:13:13,660
当你要应用那种视觉知识时， 

1483
01:13:13,660 --> 01:13:15,660
你可以转移那种学习

1484
01:13:15,660 --> 01:13:17,360
到另一个域。 

1485
01:13:17,620 --> 01:13:20,240
这就是神经网络的美妙力量

1486
01:13:20,240 --> 01:13:22,240
这是他们可以转让的。 

1487
01:13:23,880 --> 01:13:26,000
他们在这里做的是 - 

1488
01:13:27,960 --> 01:13:31,680
我没有花足够的时间查看代码

1489
01:13:31,760 --> 01:13:34,720
我不确定他们采取了哪一个巨型的纽约

1490
01:13:34,720 --> 01:13:37,540
但他们采用了巨大的卷积神经网络， 

1491
01:13:39,220 --> 01:13:41,020
他们砍掉了

1492
01:13:41,700 --> 01:13:42,960
最后一层， 

1493
01:13:43,200 --> 01:13:45,180
它产生了3000个功能， 

1494
01:13:45,320 --> 01:13:47,180
他们采用了这3000个功能

1495
01:13:47,240 --> 01:13:49,060
到每一个图像帧， 

1496
01:13:49,440 --> 01:13:50,780
这就是Xt。 

1497
01:13:51,040 --> 01:13:54,100
他们把它作为LSTM的输入。 

1498
01:13:54,420 --> 01:13:56,800
在这种情况下，序列长度为50。 

1499
01:13:57,500 --> 01:14:00,200
这个过程非常好

1500
01:14:04,200 --> 01:14:06,760
跨域相似。这就是它的美丽。 

1501
01:14:07,540 --> 01:14:11,640
神经网络的艺术在于 - 

1502
01:14:13,300 --> 01:14:14,620
那是个好兆头

1503
01:14:14,720 --> 01:14:15,300
[笑] 

1504
01:14:15,300 --> 01:14:17,120
我想我应该扭曲它 - 

1505
01:14:27,640 --> 01:14:31,180
神经网络的艺术在于适当的参数调整。 

1506
01:14:31,180 --> 01:14:32,560
这是棘手的部分， 

1507
01:14:32,620 --> 01:14:34,840
那是你无法教授的部分。 

1508
01:14:34,940 --> 01:14:35,960
这是经验， 

1509
01:14:37,280 --> 01:14:38,440
可悲的是。 

1510
01:14:38,680 --> 01:14:40,880
这就是他们谈论的原因

1511
01:14:41,900 --> 01:14:44,240
随机梯度下降SGD， 

1512
01:14:44,240 --> 01:14:47,240
那就是杰弗里·辛顿

1513
01:14:47,240 --> 01:14:49,080
指的是

1514
01:14:49,160 --> 01:14:51,760
随机研究生后裔， 

1515
01:14:53,040 --> 01:14:55,980
意思是你只是招聘研究生

1516
01:14:55,980 --> 01:14:57,680
使用超参数

1517
01:14:57,780 --> 01:14:59,340
直到问题解决了

1518
01:14:59,980 --> 01:15:02,180
[笑声]。 

1519
01:15:06,700 --> 01:15:07,740
我有

1520
01:15:07,980 --> 01:15:11,120
驱动程序状态下100多张幻灯片， 

1521
01:15:11,220 --> 01:15:15,900
这是我最热衷的事情， 

1522
01:15:15,900 --> 01:15:19,120
而且我认为最后会拯救最好的。 

1523
01:15:19,560 --> 01:15:23,220
我明天会谈到这个。我们有演讲嘉宾

1524
01:15:24,020 --> 01:15:25,190
来自白宫， 

1525
01:15:25,220 --> 01:15:28,310
将谈论人工智能的未来

1526
01:15:28,350 --> 01:15:30,100
从政策的角度来看， 

1527
01:15:32,310 --> 01:15:36,010
我希望你先注册的是你注册的学生提交的内容

1528
01:15:36,030 --> 01:15:37,640
两个教程作业， 

1529
01:15:37,660 --> 01:15:38,980
然后接

1530
01:15:40,240 --> 01:15:42,680
我们可以在这里设置盒子还是什么？ 

1531
01:15:42,760 --> 01:15:44,840
只是停下来拿一件衬衫。 

1532
01:15:46,300 --> 01:15:48,080
并在路上给我们一张卡片。 

1533
01:15:48,800 --> 01:15:50,060
多谢你们。 

1534
01:15:52,450 --> 01:15:56,420
[掌声] 

