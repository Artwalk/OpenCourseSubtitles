1
00:00:04,820 --> 00:00:08,440
好的，欢迎大家回来。听起来还好吗？好的。 

2
00:00:10,440 --> 00:00:17,880
所以今天我们将 - 我们谈了一些神经网络，昨天开始讨论神经网络。 

3
00:00:18,200 --> 00:00:25,850
今天我们将继续谈论与图像一起工作的神经网络，卷积神经网络， 

4
00:00:26,620 --> 00:00:37,800
并了解这些类型的网络如何帮助我们驾驶汽车。如果我们有时间，我们将涵盖一个简单的说明性案例研究

5
00:00:38,260 --> 00:00:41,460
检测交通灯。 

6
00:00:41,990 --> 00:00:49,240
检测绿色，黄色，红色的问题。如果我们不能教我们的神经网络这样做，我们就麻烦了， 

7
00:00:49,990 --> 00:00:59,660
但这是一个关于三类分类问题的一个好的，清晰的，说明性的案例研究。好的，接下来就是

8
00:01:00,440 --> 00:01:08,900
DeepTesla在很短的GIF中一遍又一遍地循环。这实际上是在网站上直播。 

9
00:01:08,900 --> 00:01:19,560
我们将在演讲结束时展示它，这再一次就像DeepTraffic是一个学习的神经网络

10
00:01:19,560 --> 00:01:22,600
根据前方道路的视频驾驶车辆。 

11
00:01:22,660 --> 00:01:27,620
再一次，使用javascript在浏览器中完成所有这些操作。 

12
00:01:28,800 --> 00:01:33,560
因此，您将能够使用真实世界数据训练自己的网络。 

13
00:01:34,540 --> 00:01:48,340
我会解释一下。我们还将提供教程和代码。今天在讲座结束时简要介绍， 

14
00:01:48,340 --> 00:01:52,280
如果有时间如何在TensorFlow中做同样的事情。 

15
00:01:52,380 --> 00:02:00,960
因此，如果您想构建一个更大，更深的网络，并且您希望利用GPU来训练该网络， 

16
00:02:00,960 --> 00:02:06,680
如果您不想在浏览器中执行此操作，则需要使用TensorFlow离线执行此操作

17
00:02:07,150 --> 00:02:14,120
并且在您的计算机上拥有强大的GPU，我们将解释如何做到这一点。计算机视觉。 

18
00:02:14,680 --> 00:02:22,280
所以我们谈到了香草机器学习哪里没有 - 昨天的大小， 

19
00:02:22,800 --> 00:02:26,690
其中输入的大小在很大程度上是小的。 

20
00:02:27,700 --> 00:02:34,180
在神经网络的情况下，神经元的数量大约为10,100,1,000。 

21
00:02:35,160 --> 00:02:42,080
当您想到图像时，图像是像素的集合，是计算机视觉中最具代表性的图像之一

22
00:02:42,620 --> 00:02:44,420
在左下方有Lenna。 

23
00:02:45,000 --> 00:02:53,040
我鼓励你谷歌它，并找出该图像背后的故事。我最近发现时非常震惊。 

24
00:02:55,720 --> 00:03:07,480
因此，如今，计算机视觉再次受到机器学习的数据驱动方法的支配

25
00:03:11,260 --> 00:03:19,950
其中所有与其他类型数据相同的方法都用在输入正好的图像上

26
00:03:19,950 --> 00:03:28,580
像素和像素的集合是0到255个离散值的数字。 

27
00:03:29,420 --> 00:03:36,280
所以我们可以准确地思考我们之前谈过的内容，你可以用同样的方式来思考图像。这只是数字

28
00:03:37,060 --> 00:03:41,860
所以我们可以做同样的事情。我们可以在有输入图像的地方进行监督学习

29
00:03:42,180 --> 00:03:48,360
和输出标签。这里的输入图像是女人的照片;标签可能是“女人”。 

30
00:03:49,930 --> 00:03:57,260
关于监督学习，同样的事情。我们将简要地看一下，并将图像聚类到类别中。 

31
00:03:58,080 --> 00:04:04,500
再次半监督和强化学习。事实上，昨天谈到的Atari游戏。 

32
00:04:05,080 --> 00:04:11,720
对图像进行一些预处理。他们正在做计算机视觉;他们正在使用卷积神经网络，我们今天将讨论

33
00:04:13,040 --> 00:04:19,020
监督学习的管道也是一样的：有图像形式的原始数据， 

34
00:04:19,260 --> 00:04:26,180
那些图像上有标签。我们执行机器学习算法，执行特征提取， 

35
00:04:26,520 --> 00:04:33,900
它根据图像的输入和输出以及这些图像的标签进行训练，构建模型

36
00:04:33,900 --> 00:04:35,140
然后测试该模型。 

37
00:04:35,560 --> 00:04:41,500
我们得到了衡量标准和准确性。准确度是用于经常描述模型执行情况的术语。 

38
00:04:42,280 --> 00:04:43,120
百分比。 

39
00:04:47,290 --> 00:04:54,220
我为整个课程中猫的不断存在而道歉。我向你保证，这个课程是关于驾驶，而不是猫。 

40
00:04:56,380 --> 00:05:03,140
但图像是数字。所以对我们来说，我们认为理所当然。 

41
00:05:03,760 --> 00:05:06,520
我们真的很擅长看

42
00:05:06,520 --> 00:05:12,560
将视觉感知转化为人类，将视觉感知转化为语义。 

43
00:05:13,500 --> 00:05:22,040
我们看到这个图像，我们知道它是一只猫，但计算机只能看到数字：彩色图像的RGB值。 

44
00:05:22,560 --> 00:05:26,720
从0到255的每个像素都有三个值。 

45
00:05:27,840 --> 00:05:37,680
因此，鉴于该图像，我们可以想到两个问题：一个是回归，另一个是分类。回归是给出一个图像

46
00:05:38,140 --> 00:05:43,200
我们希望产生一个真正的输出值。所以，如果我们有四个车道的图像， 

47
00:05:43,440 --> 00:05:50,340
我们想要为方向盘角度生成一个值，如果你有一个非常聪明的算法， 

48
00:05:50,840 --> 00:05:53,440
它可以拍摄前方道路的任何图像

49
00:05:53,600 --> 00:05:58,540
并产生完美正确的转向角度，可以安全地驾驶汽车穿越美国。 

50
00:05:59,340 --> 00:06:07,680
我们将讨论如何做到这一点以及失败的地方。分类是指输入再次是图像

51
00:06:08,140 --> 00:06:16,340
输出是一个类标签，一个离散的类标签。在它下面虽然经常仍然是一个回归问题

52
00:06:16,340 --> 00:06:23,640
并且一旦产生了该特定图像属于特定类别的概率。 

53
00:06:24,590 --> 00:06:31,740
我们使用阈值来切断与低概率相关的输出

54
00:06:32,940 --> 00:06:37,980
并获取与高概率相关的标签并将其转换为离散分类。 

55
00:06:40,020 --> 00:06:44,380
我昨天提到了这个，但它再说一遍，计算机视觉很难。 

56
00:06:47,320 --> 00:06:53,220
我们再一次认为这是理所当然的。作为人类，我们非常善于处理所有这些问题。 

57
00:06:53,740 --> 00:07:00,080
存在视点变化：对象在图像背后的数字方面看起来完全不同

58
00:07:00,360 --> 00:07:03,720
在从不同角度观看时的像素方面。 

59
00:07:04,080 --> 00:07:11,200
视点变化：当您站在远离它们或近距离时的物体尺寸完全不同。 

60
00:07:11,800 --> 00:07:14,340
我们擅长检测到有不同的尺寸。 

61
00:07:14,640 --> 00:07:20,960
它仍然是与人类相同的对象，但这仍然是一个非常难的问题，因为这些尺寸可能会有很大差异。 

62
00:07:21,500 --> 00:07:29,400
我们谈到了猫的闭塞和变形;很好理解的问题。有背景杂乱。 

63
00:07:30,390 --> 00:07:38,200
你必须将感兴趣的对象与背景分开，并给出我们世界的三维结构。 

64
00:07:38,200 --> 00:07:44,880
在背景中经常会发生很多事情：混乱，他们的阶级间差异。 

65
00:07:45,000 --> 00:07:51,100
这通常比班级间的变化大;意思是相同类型的对象通常具有更多变化

66
00:07:51,220 --> 00:08:01,060
而不是你试图将它们分开的对象。驾驶很难：照明。 

67
00:08:02,350 --> 00:08:06,800
光是我们感知事物的方式;光线从表面反射出来

68
00:08:07,860 --> 00:08:13,680
并且那光的来源改变了对象出现的方式，我们必须对所有这些都很健壮。 

69
00:08:17,210 --> 00:08:24,640
因此图像分类管道与我提到的相同。有类别， 

70
00:08:25,800 --> 00:08:30,840
这是猫，狗，马克杯，帽子等类别的分类问题。 

71
00:08:30,840 --> 00:08:34,620
您有一堆示例，每个类别的图像示例

72
00:08:34,880 --> 00:08:39,760
因此输入只是那些与类别配对的图像。 

73
00:08:41,210 --> 00:08:48,980
然后你训练地图，估计一个从图像映射到类别的功能。 

74
00:08:52,140 --> 00:08:55,360
对于所有这些，您需要数据;很多。 

75
00:08:56,450 --> 00:09:03,880
不幸的是，有越来越多的数据集，但仍然相对较小。 

76
00:09:05,140 --> 00:09:12,600
我们很兴奋。有数百万张图片，但它们不是数十亿或数万亿的图像，这些是， 

77
00:09:13,440 --> 00:09:17,760
如果您最常阅读学术文献，您将看到的数据集。 

78
00:09:18,280 --> 00:09:20,920
Mnist，一个被打死的人。 

79
00:09:21,510 --> 00:09:34,340
然后我们在本课程中使用手写数字的数据集，其中类别为0到9。 

80
00:09:35,440 --> 00:09:48,120
ImageNet，最大的图像数据集之一;世界上完全标记的图像数据集具有来自Word Net的类别层次结构的图像。 

81
00:09:49,560 --> 00:09:56,180
您所看到的是标记了哪些图像与数据集中存在的单词相关联。 

82
00:09:57,380 --> 00:10:06,240
CIFAR-10和CIFAR-100是微小的图像，用于以非常有效和快速的方式进行验证

83
00:10:06,500 --> 00:10:12,860
你想要发布的算法，或试图给世界留下深刻印象的算法效果很好。 

84
00:10:13,580 --> 00:10:18,040
它很小，它是一个小数据集：CIFAR-10意味着有10个类别。 

85
00:10:19,960 --> 00:10:27,500
地方是自然场景的数据集：森林，自然，城市等。 

86
00:10:27,940 --> 00:10:30,500
那么让我们来看看CIFAR-10 

87
00:10:30,720 --> 00:10:34,620
作为10个类别的数据集：飞机，汽车，鸟，猫等。 

88
00:10:35,540 --> 00:10:38,640
他们在那里展示了样本图像作为玫瑰。 

89
00:10:39,940 --> 00:10:47,220
因此，让我们构建一个分类器，它能够从这10个类别中的一个中获取图像并告诉我们什么

90
00:10:48,300 --> 00:10:56,360
显示在图像中。那我们怎么做呢？再一次，所有算法都看到了数字。 

91
00:10:58,450 --> 00:11:06,740
所以我们必须尝试在核心，我们必须有一个运算符来比较两个图像。 

92
00:11:07,440 --> 00:11:10,020
所以给了一个图像，我想把它保存为猫或狗。 

93
00:11:10,020 --> 00:11:16,280
我想将它与猫的图像进行比较，并将其与狗的图像进行比较，看看哪个匹配得更好。 

94
00:11:16,680 --> 00:11:24,380
所以必须有一个比较运营商。好的，所以一种方法是采取两个图像之间的绝对差异

95
00:11:24,380 --> 00:11:27,660
逐个像素，取之间的差异

96
00:11:29,240 --> 00:11:38,560
幻灯片底部显示的每个像素为4x4图像。然后我们按像素加和

97
00:11:38,960 --> 00:11:45,020
像素方式的绝对差值为单个数字。因此，如果图像在像素方面完全不同， 

98
00:11:45,840 --> 00:11:46,980
这将是一个很高的数字。 

99
00:11:47,200 --> 00:11:53,240
如果它是相同的图像，则数字将为0.哦，这也是差异的绝对值。 

100
00:11:56,420 --> 00:12:05,500
那叫做L1距离。没关系。当我们谈到距离时，我们通常指的是L2距离。 

101
00:12:07,480 --> 00:12:17,960
所以，如果我们尝试 - 所以我们可以构建仅使用此运算符将其与数据集中的每个图像进行比较的分类器

102
00:12:18,160 --> 00:12:23,940
并且说我要选择，我将选择该类别

103
00:12:23,940 --> 00:12:30,020
这是使用这个比较运算符的最接近的。我要找 - 我有一张猫的照片

104
00:12:30,020 --> 00:12:34,100
我将查看数据集并找到最接近此图片的图像

105
00:12:35,270 --> 00:12:37,800
并说这是这张照片所属的类别。 

106
00:12:38,700 --> 00:12:45,280
因此，如果我们只是翻转硬币并随机选择图像所属的类别以获得该精度， 

107
00:12:46,410 --> 00:12:49,700
平均为10％。这是随机的。 

108
00:12:51,100 --> 00:12:59,980
我们精彩的图像差分算法只需经过数据集的准确性

109
00:12:59,980 --> 00:13:08,200
并且发现最接近的是38％这是相当不错的，它超过10％。 

110
00:13:10,580 --> 00:13:13,780
所以你可以考虑一下这个基础的操作

111
00:13:13,780 --> 00:13:19,900
并找到最接近的图像，称为K-Nearest Neighbors 

112
00:13:20,460 --> 00:13:27,340
或者在这种情况下为K.这意味着您可以找到与此图像最近的邻居，您正在询问有关的问题

113
00:13:28,410 --> 00:13:33,480
并接受该图像的标签。你可以做同样的事情来增加K. 

114
00:13:34,650 --> 00:13:38,580
将K增加到2表示您选择两个最近的邻居。 

115
00:13:39,540 --> 00:13:47,540
通过此特定查询图像，您可以找到与像素方式图像差异最接近的两个图像

116
00:13:48,690 --> 00:13:51,260
并找出那些属于哪些类别。 

117
00:13:52,530 --> 00:13:58,780
左边显示的是我们正在使用的数据集：红色，绿色，蓝色。 

118
00:14:00,000 --> 00:14:05,000
中间显示的是最近邻分类器，意思是

119
00:14:05,860 --> 00:14:10,860
这就是你如何划分你可以比较的不同事物的整个空间。 

120
00:14:13,950 --> 00:14:16,900
如果一个点落入这些区域中的任何一个， 

121
00:14:17,240 --> 00:14:23,780
它将立即与最近邻居算法相关联，以属于该图像到该区域。 

122
00:14:25,180 --> 00:14:33,040
与五个最近的邻居，立即出现问题。问题是有白色区域。 

123
00:14:33,040 --> 00:14:43,400
有五个最接近的邻居来自不同类别的领带断路器。所以目前还不清楚你属于哪里。 

124
00:14:45,680 --> 00:14:52,780
所以这是参数调整的一个很好的例子。你有一个参数：K。 

125
00:14:54,800 --> 00:15:03,940
而你作为机器学习老师的任务，你必须教这个算法如何为你学习， 

126
00:15:05,160 --> 00:15:06,880
是找出那个参数。 

127
00:15:07,500 --> 00:15:12,720
这被称为“参数调整”或“超参数调整”，因为它在神经网络中被调用。 

128
00:15:14,730 --> 00:15:25,450
因此，在x轴上滑动的右下角是K.当我们将它从0增加到100时

129
00:15:25,450 --> 00:15:34,060
y轴是分类精度。事实证明，这个数据集的最佳K是7，7年的邻居。 

130
00:15:34,500 --> 00:15:42,460
通过它，我们获得了30％的人类水平表现

131
00:15:44,060 --> 00:15:51,220
而且我应该说我们获得这个数字的方式和我们使用很多机器学习管道一样

132
00:15:52,640 --> 00:16:00,020
过程是您将数据分成用于培训的天数

133
00:16:00,020 --> 00:16:07,320
以及他们用于测试的另一部分。您不能接触测试部件。那是作弊。 

134
00:16:07,500 --> 00:16:14,260
您可以在训练数据集上构建您的世界模型，并使用所谓的交叉验证

135
00:16:15,100 --> 00:16:25,870
你把一小部分训练数据显示为“折叠五”，那里是黄色的，留下那部分

136
00:16:25,870 --> 00:16:37,120
训练然后将其用作超参数调整的一部分。当你训练时，弄清楚黄色部分折五

137
00:16:37,780 --> 00:16:42,740
你做得多好，然后你选择一个不同的折叠，看看你做得多好

138
00:16:42,880 --> 00:16:47,860
并继续使用从未接触测试部件的参数。当你准备好了， 

139
00:16:47,860 --> 00:16:54,360
您在测试数据上运行算法，看看您的真实情况。它将如何真正概括。是的，问题。 

140
00:16:54,500 --> 00:16:56,440
（不可思议的问题） 

141
00:16:59,140 --> 00:17:01,200
所以，问题是：“有没有好办法 - 

142
00:17:02,330 --> 00:17:07,040
好的K背后有什么好的直觉吗？“对于不同的数据集有一般规则

143
00:17:07,340 --> 00:17:13,200
但通常你只需要经历它。网格搜索，蛮力。是的，问题。 

144
00:17:13,300 --> 00:17:14,420
（不可思议的问题） 

145
00:17:14,420 --> 00:17:15,420
（笑着） 

146
00:17:15,420 --> 00:17:17,220
好问题。是。 

147
00:17:17,220 --> 00:17:25,080
（不可思议的问题） 

148
00:17:25,080 --> 00:17:28,580
是的，问题是：“每个像素是1个数字还是3个数字？” 

149
00:17:29,120 --> 00:17:35,040
对于整个历史记录中的大多数计算机视觉使用灰度图像，因此它是1个数字但RGB 

150
00:17:35,040 --> 00:17:40,920
是3个数字，有时也有深度值，所以它是4个数字。所以这是- 

151
00:17:42,120 --> 00:17:45,580
如果您有立体视觉相机，可以为您提供像素的深度信息， 

152
00:17:45,800 --> 00:17:52,140
这是第四个然后如果你把两个图像堆叠在一起可能是6.一般来说， 

153
00:17:52,140 --> 00:17:56,540
我们使用的所有内容都是一个像素的3个数字。 

154
00:18:00,000 --> 00:18:02,980
是的，所以问题是：“关于绝对值只是一个数字？” 

155
00:18:02,980 --> 00:18:05,640
非常正确。那么在那种情况下，那些是灰度图像。 

156
00:18:06,020 --> 00:18:07,480
所以这不是RGB图像。 

157
00:18:10,560 --> 00:18:14,780
所以，你知道，如果我们使用最好的算法，这个算法非常好。 

158
00:18:17,770 --> 00:18:23,200
我们优化了这个算法的超参数，选择K为7， 

159
00:18:23,200 --> 00:18:29,580
似乎适用于这个特定的CIFAR-10数据集。好的，我们得到30％的准确率。 

160
00:18:30,080 --> 00:18:37,900
它令人印象深刻，高于10％。人类的表现约为94，略高于94％ 

161
00:18:37,900 --> 00:18:46,620
CIFAR-10的准确度。所以给定一个图像，这是一个很小的图像。我应该澄清一下，它就像一个小图标。 

162
00:18:49,150 --> 00:18:55,360
鉴于该图像，人类能够准确地确定10个类别中的一个，准确率为94％。 

163
00:18:55,790 --> 00:19:00,460
目前最先进的卷积神经网络是九十五， 

164
00:19:00,460 --> 00:19:06,600
这是95.4％的准确度，不管你信不信，这是一场激烈的战斗

165
00:19:07,530 --> 00:19:13,040
但最重要的是，这里的关键事实是，它最近超越了人类。 

166
00:19:13,640 --> 00:19:22,800
并且肯定超过k近邻算法。那么，这是如何工作的？让我们简要回顾一下。 

167
00:19:23,490 --> 00:19:28,100
这一切仍然归结为这个小家伙：神经元， 

168
00:19:28,150 --> 00:19:36,760
它将输入的权重相加，增加偏差，根据激活产生输出，平滑激活函数。 

169
00:19:38,900 --> 00:19:40,100
是的，问题。 

170
00:19:40,280 --> 00:19:44,520
（不可思议的问题） 

171
00:19:45,880 --> 00:19:48,980
问题是：“你拍摄卡西的照片，你知道它是一只猫， 

172
00:19:50,140 --> 00:19:56,140
但这并没有在任何地方编码，就像你必须把它写在某处。 

173
00:19:56,700 --> 00:19:59,520
所以你必须写一个标题：“这是我的猫。” 

174
00:19:59,880 --> 00:20:05,480
然后不幸的是，鉴于互联网和它是多么木质，你不能相信图像上的字幕。 

175
00:20:06,220 --> 00:20:13,100
因为也许你只是聪明而且它不是一只猫，它是一只装扮成猫的狗。是的，问题。 

176
00:20:14,240 --> 00:20:15,740
（不可思议的问题） 

177
00:20:18,420 --> 00:20:20,500
抱歉。看得更好比什么？ 

178
00:20:26,890 --> 00:20:30,780
是的，所以问题是：“卷积神经网络通常比最近的邻居做得好吗？ 

179
00:20:31,240 --> 00:20:39,020
神经网络没有做得更好的问题很少，是的，它们几乎总能做得更好

180
00:20:39,020 --> 00:20:50,080
除非您几乎没有数据。所以你需要数据。而卷积神经网络并不是一些特殊的神奇之物。 

181
00:20:50,080 --> 00:20:58,060
这只是神经网络，前面有一些作弊，我会解释，尝试减小尺寸的一些技巧

182
00:20:58,300 --> 00:21:00,220
并使其能够处理图像。 

183
00:21:01,680 --> 00:21:06,720
再说一遍。是的，输入是，在这种情况下，我们考虑分类数字的图像， 

184
00:21:07,400 --> 00:21:13,920
而不是做一些花哨的卷积技巧。我们只需要整个28x28 

185
00:21:13,940 --> 00:21:19,940
像素图像为784像素作为输入。 

186
00:21:20,380 --> 00:21:26,880
那是输入中的784个神经元，隐藏层上的15个神经元和输出中的10个神经元。 

187
00:21:28,640 --> 00:21:32,480
现在我们要讨论的一切都具有相同的结构。没有什么花哨。 

188
00:21:34,830 --> 00:21:41,360
有一个正向传递通过网络，您可以在其中获取输入图像并生成输出分类

189
00:21:41,920 --> 00:21:47,960
并且有一个向后传递通过网络进行反向传播，您可以在其中调整权重

190
00:21:47,960 --> 00:21:53,060
当您的预测与地面真相输出不匹配时。 

191
00:21:54,700 --> 00:22:01,220
学习只是归结为优化;它只是优化平滑功能。 

192
00:22:02,130 --> 00:22:06,980
功能差异化;这被定义为丢失的功能。 

193
00:22:07,340 --> 00:22:13,180
这通常就像真实输出之间的平方误差一样简单

194
00:22:13,180 --> 00:22:18,610
而你实际得到的那个。那有什么区别？什么是卷积神经网络？ 

195
00:22:20,650 --> 00:22:32,260
卷积神经网络采用具有一定空间一致性的输入，对空间具有一定的意义 - 

196
00:22:34,800 --> 00:22:42,620
它们具有一些空间意义，如图像。还有其他的东西，你可以想到时间的维度。 

197
00:22:43,620 --> 00:22:54,560
您可以将音频信号输入卷积神经网络。所以输入通常是针对每一层， 

198
00:22:54,800 --> 00:22:59,860
这是一个卷积层，输入是一个3D体积，输出是一个3D体积。 

199
00:23:01,080 --> 00:23:06,840
我正在简化，因为你可以称之为4D但它是3D。有高度，宽度和深度。 

200
00:23:07,820 --> 00:23:11,840
这是一张图片。高度和宽度是图像的宽度和高度。 

201
00:23:12,480 --> 00:23:19,220
然后灰度图像的深度为1;对于RGB图像是3; 

202
00:23:20,320 --> 00:23:26,440
对于灰度图像的十帧视频，深度为10。 

203
00:23:27,670 --> 00:23:36,420
它只是一个体积，一个三维的数字矩阵。和所有- 

204
00:23:36,420 --> 00:23:43,670
卷积层唯一能做的就是获取3D体积的输入，生成3D体积作为输出

205
00:23:44,470 --> 00:23:46,180
并具有一些平滑的功能。 

206
00:23:47,610 --> 00:23:51,240
在输入上操作，输入总和， 

207
00:23:52,900 --> 00:24:00,500
您尝试优化的可能是也可能不是您调整的参数。而已。 

208
00:24:01,740 --> 00:24:05,380
所以Lego的碎片就像我们之前谈到的那样堆叠在一起。 

209
00:24:07,220 --> 00:24:11,500
那么卷积神经网络有哪些层次类型？有投入。 

210
00:24:11,920 --> 00:24:19,160
因此，例如32x32的彩色图像将是32x32x3的体积。 

211
00:24:21,770 --> 00:24:34,080
卷积层利用输入神经元和卷积层的空间关系， 

212
00:24:35,880 --> 00:24:37,720
它是完全相同的神经元

213
00:24:38,000 --> 00:24:41,580
至于完全连接的网络，我们之前谈过的常规。 

214
00:24:41,930 --> 00:24:52,000
但它有一个较窄的感受野，它更集中，是卷积层神经元的输入

215
00:24:52,980 --> 00:24:57,840
来自前一层的特定区域。和参数

216
00:24:57,840 --> 00:25:03,000
在每个过滤器上，您可以将其视为过滤器，因为您将其滑过整个图像。 

217
00:25:05,860 --> 00:25:12,360
这些参数是共享的。所以假设你已经采取了 - 如果你考虑两层， 

218
00:25:12,780 --> 00:25:20,660
而不是将第一层中的每个像素连接到下一层中的每个单个神经元。 

219
00:25:20,920 --> 00:25:30,070
您只需将输入层中彼此接近的神经元连接到输出层，然后再连接到您

220
00:25:30,070 --> 00:25:37,620
强制权重在空间上捆绑在一起。 

221
00:25:39,070 --> 00:25:45,080
结果是输出上的每一层过滤器， 

222
00:25:45,080 --> 00:25:49,940
你可以把它想象成一个过滤器，他们会因为边缘而感到兴奋

223
00:25:50,800 --> 00:25:55,480
当它在图像中看到这种特殊的边缘时，它会变得兴奋。 

224
00:25:55,770 --> 00:26:01,540
而且它会在图像的左上角，右上角，左下角，右下角激动。 

225
00:26:02,930 --> 00:26:08,960
假设有一个检测猫的强大功能

226
00:26:09,700 --> 00:26:12,800
无论图像在哪里，它都同样重要。 

227
00:26:14,270 --> 00:26:25,780
这可以让你切断神经元之间的大量连接，但它仍然在右边， 

228
00:26:26,640 --> 00:26:30,760
作为总结输入集合的神经元

229
00:26:31,300 --> 00:26:43,300
并对它们施加权重。输出音量相对于输入音量的空间排列

230
00:26:43,300 --> 00:26:51,360
由三件事控制。过滤器的数量。所以对于每一个“过滤器” 

231
00:26:51,630 --> 00:26:58,560
你在输出上得到一个额外的图层。所以如果输入， 

232
00:26:58,830 --> 00:27:05,870
让我们来谈谈第一层，输入是32x32x3。它是RGB 

233
00:27:05,870 --> 00:27:13,280
32x32的图像。如果过滤器的数量是10， 

234
00:27:14,310 --> 00:27:27,060
然后得到的深度，输出中产生的堆叠通道数将为10.给出了步幅。 

235
00:27:27,660 --> 00:27:36,380
是您沿图像滑动的滤镜的步长。通常只有1或3 

236
00:27:37,200 --> 00:27:44,280
这直接减小了输出图像的大小，空间大小，宽度和高度。 

237
00:27:45,130 --> 00:27:53,220
然后有一个方便的事情，它经常做填充。外面的零上的图像。 

238
00:27:54,080 --> 00:28:05,420
这样输入和输出具有相同的高度和宽度。所以这是卷积的可视化。 

239
00:28:06,250 --> 00:28:10,540
我鼓励你，有点离线，想想发生了什么。 

240
00:28:11,060 --> 00:28:19,460
它与人类视觉的工作方式类似，粗略地说，如果观众中有任何专家。 

241
00:28:20,410 --> 00:28:30,580
所以左边的输入是一组数字：0,1,2和一个过滤器

242
00:28:36,720 --> 00:28:41,250
或者有两个过滤器显示为W1- 

243
00:28:41,250 --> 00:28:49,380
W0和W1。以红色显示的那些过滤器是在这些过滤器中应用的不同权重。 

244
00:28:50,530 --> 00:28:55,200
并且每个滤波器都有一定的深度;就像输入深度为3。 

245
00:28:56,040 --> 00:29:02,580
所以每列中有三个，所以， 

246
00:29:04,200 --> 00:29:09,800
所以你沿着图像滑动死亡过滤器，保持重量相同。 

247
00:29:09,800 --> 00:29:11,520
这是权重的共享

248
00:29:12,700 --> 00:29:19,240
所以你选择权重的第一个过滤器，这是一个优化问题。你以这种方式挑选权重

249
00:29:19,240 --> 00:29:24,620
它会激发，它会变得兴奋，因为有用的功能而且不会因为没有用的功能而激发。 

250
00:29:25,140 --> 00:29:28,980
然后有第二个过滤器可以激发有用的功能，而不是。 

251
00:29:29,840 --> 00:29:40,180
并根据正数在输出上产生一个信号，这意味着该区域有一个强大的特征， 

252
00:29:40,520 --> 00:29:44,360
如果没有，则为负数，但过滤器相同。 

253
00:29:44,720 --> 00:29:49,820
这样可以大大减少参数，因此您可以处理

254
00:29:50,540 --> 00:29:59,260
投入。例如，有一千个像素的图像或视频。那里有一个非常强大的概念。 

255
00:30:02,750 --> 00:30:04,700
权重的空间共享。 

256
00:30:04,980 --> 00:30:11,420
这意味着您正在检测的功能存在空间不变性。它允许您从任意图像中学习

257
00:30:11,960 --> 00:30:17,580
所以你不必担心以一种聪明的方式预处理图像， 

258
00:30:17,700 --> 00:30:23,840
你只需要提供原始图像。还有另一个操作：汇集。 

259
00:30:24,880 --> 00:30:30,500
这是一种减少图层大小的方法，例如在这种情况下， 

260
00:30:30,500 --> 00:30:35,810
它是获取输出集合并选择x1的最大池

261
00:30:35,810 --> 00:30:48,170
并总结那些像素集合，使得池化操作的输出远小于输入。 

262
00:30:49,970 --> 00:30:59,120
因为有理由认为你不需要高分辨率。 

263
00:31:00,340 --> 00:31:08,020
根据您所知，图像中的哪个像素的本地化很重要

264
00:31:08,020 --> 00:31:14,740
您不需要确切知道哪个像素与猫耳或猫脸相关联。 

265
00:31:15,840 --> 00:31:18,460
只要你，那种，知道它就是那个部分

266
00:31:18,640 --> 00:31:22,720
这减少了操作中的大量复杂性。是的，问题。 

267
00:31:27,280 --> 00:31:32,340
问题是：“什么时候汇集太多，什么时候停止汇集？” 

268
00:31:35,780 --> 00:31:46,480
所以汇集是一个非常粗糙的操作，没有任何东西，你需要知道的一件事，它没有任何东西

269
00:31:46,480 --> 00:31:49,060
可学习的参数。 

270
00:31:49,580 --> 00:31:56,740
所以你无法学到任何关于汇集的聪明之处。在这种情况下，你只是选择

271
00:31:56,740 --> 00:32:04,840
max pool，所以你选的是最大的数字。所以你降低了分辨率，你丢失了很多信息。 

272
00:32:05,460 --> 00:32:10,680
有一种观点认为，只要你没有把整个信息汇集起来，你就不会丢失那么多信息

273
00:32:10,680 --> 00:32:12,200
将图像转换为单个值

274
00:32:12,940 --> 00:32:22,460
但是你获得了培训效率，你正在获得内存大小，减少了网络的规模。 

275
00:32:22,980 --> 00:32:30,560
所以，这绝对是人们争论的事情，它是你玩的参数，看看什么对你有用。 

276
00:32:33,290 --> 00:32:40,500
好吧，那么这个东西整体来说是一个卷积神经网络，输入是一个图像

277
00:32:41,700 --> 00:32:52,360
通常有一个卷积层，有一个池操作，另一个卷积层，另一个池操作等等。 

278
00:32:54,370 --> 00:33:01,700
最后，如果任务是分类，则会有一堆卷积层和池化层。 

279
00:33:02,020 --> 00:33:04,520
有几个完全连接的层。 

280
00:33:05,120 --> 00:33:14,440
因此，您可以从那些空间卷积操作中完全连接图层中的每个神经元

281
00:33:14,440 --> 00:33:15,280
以下层。 

282
00:33:15,720 --> 00:33:22,920
并且你这样做，以便最终，你有一组神经元，每个神经元都与一个特定的类相关联。 

283
00:33:23,700 --> 00:33:30,940
所以我们昨天看到的是输入，是一个0到9的图像。 

284
00:33:31,300 --> 00:33:41,020
这里的输出将是10个神经元。所以你用一组卷积层来吹倒那个图像， 

285
00:33:41,840 --> 00:33:47,800
在末端有1或2或3个完全连接的层，所有层都导致10个神经元

286
00:33:48,320 --> 00:33:53,560
而且每个神经元的工作都要被解雇

287
00:33:54,260 --> 00:34:02,020
当它看到一个特定的数字，而其他的则产生一个低概率。所以这种过程

288
00:34:02,620 --> 00:34:10,900
是如何在CIFAR-10问题上获得95％的精确度。 

289
00:34:10,900 --> 00:34:19,360
这是我提到的ImageNet数据集。这就是你如何拍摄豹子，集装箱船的形象， 

290
00:34:20,160 --> 00:34:24,140
并产生一个集装箱船或豹子的可能性。 

291
00:34:25,430 --> 00:34:32,120
还显示了在其置信度方面存在其他最近神经元的输出。 

292
00:34:37,300 --> 00:34:44,440
现在，您可以通过在末端切断完全连接的层来使用相同的精确操作

293
00:34:44,860 --> 00:34:53,260
而不是从图像映射到图像中包含的内容的预测，您将从图像映射到另一个图像。 

294
00:34:54,520 --> 00:35:00,640
而且你可以训练那个令人兴奋的形象

295
00:35:01,720 --> 00:35:11,940
在空间上，意味着它为包含感兴趣对象的图像区域提供高，接近一个值

296
00:35:13,380 --> 00:35:19,760
然后是不太可能包含该图像的图像区域的低数字。 

297
00:35:20,870 --> 00:35:23,240
所以你可以从左边开始， 

298
00:35:23,240 --> 00:35:31,200
马的女人的原始图像，知道女人在哪里以及马在哪里的分段图像

299
00:35:31,200 --> 00:35:38,830
以及背景的位置。可以进行相同的过程来检测对象。 

300
00:35:39,520 --> 00:35:42,080
所以你可以把场景分成几堆

301
00:35:42,300 --> 00:35:53,520
有趣的对象，有趣的对象的候选人，然后逐个审查这些候选人

302
00:35:53,520 --> 00:36:00,460
并执行与上一步骤相同的分类，其中它只是作为图像的输入和作为分类的输出。 

303
00:36:01,050 --> 00:36:09,160
通过这个跳过图像的过程，你可以准确地找出分割牛的最佳方法

304
00:36:09,160 --> 00:36:17,360
出于图像。这就是所谓的物体检测。可以，然后呢

305
00:36:18,230 --> 00:36:28,320
这些神奇的卷积神经网络如何帮助我们驾驶？这是一个前进道路的视频

306
00:36:28,320 --> 00:36:35,020
我们将看到的数据集，我们从特斯拉收集的数据集。但首先让我看一下驾驶。 

307
00:36:35,820 --> 00:36:47,850
简而言之，从人的角度来看一般的驾驶任务。平均而言是美国的一名美国司机

308
00:36:47,850 --> 00:36:58,040
每年驾驶10,000英里。农村多一点，城市少一点。大约有30,000人

309
00:36:58,040 --> 00:37:06,040
致命的撞车事故，每年有32,000人死亡，有时高达38,000人死亡。 

310
00:37:06,790 --> 00:37:14,460
这包括汽车乘客，行人，骑自行车者和摩托车骑手。 

311
00:37:16,460 --> 00:37:25,040
这可能是一个令人惊讶的事实，但在自驾车的课堂上我们应该记住这一点。 

312
00:37:25,040 --> 00:37:34,440
所以忽略59.9％，那是另一个。美国最受欢迎的汽车是皮卡车：福特F-1系列， 

313
00:37:34,900 --> 00:37:44,160
Ram，雪佛兰Silverado。重要的一点是，我们仍然嫁给了我们， 

314
00:37:46,160 --> 00:37:56,200
想要控制，所以我们看到的有趣的汽车之一

315
00:37:56,980 --> 00:38:02,900
从特斯拉那里收集我们提供给班级的日子。 

316
00:38:03,890 --> 00:38:07,290
这是福特F-150的交叉点

317
00:38:08,280 --> 00:38:17,080
和右边可爱的小型自动驾驶汽车。它很快，它让你有一种控制感

318
00:38:17,080 --> 00:38:22,120
但如果需要的话，它也可以在高速公路上行驶数百英里。 

319
00:38:24,020 --> 00:38:27,500
它允许您按下按钮，汽车接管。 

320
00:38:28,280 --> 00:38:33,280
将控制从人类转移到汽车是一种非常有趣的权衡。 

321
00:38:35,290 --> 00:38:37,380
这是一种信任转移

322
00:38:38,260 --> 00:38:48,600
这是我们研究人类心理的一个机会，因为它们与每小时60英里以上的机器有关。 

323
00:38:51,670 --> 00:39:02,220
万一你不知道人类的一点点总结，分心的事情：想要发短信，使用智能手机， 

324
00:39:03,190 --> 00:39:11,000
观看视频，新郎，与乘客交谈，吃饭，喝酒，发短信。 

325
00:39:12,380 --> 00:39:18,440
2014年，美国每个月都发送了1690亿条文本。 

326
00:39:20,170 --> 00:39:27,760
平均而言，我们的眼睛在发短信时花了5秒钟 -  5秒钟。 

327
00:39:30,360 --> 00:39:41,830
这是自动化介入的机会。更重要的是，NHTSA称之为4 D：醉酒，吸毒， 

328
00:39:41,830 --> 00:39:42,850
心烦意乱，昏昏欲睡。 

329
00:39:43,300 --> 00:39:53,180
这些机会中的每一个都是为了让自动化步入。醉酒驾驶显着受益

330
00:39:53,710 --> 00:40:06,640
或许来自自动化。所以英里，让我们看看里程。数据。有3万亿（300万） 

331
00:40:06,640 --> 00:40:14,820
每年驾驶300万英里和TESLA自动驾驶仪，我们的课程研究， 

332
00:40:15,080 --> 00:40:20,660
并且人类在完全自动驾驶模式下驾驶。 

333
00:40:20,660 --> 00:40:27,600
因此，截至2016年12月，它自行驾驶3亿英里

334
00:40:28,740 --> 00:40:36,400
而人类控制车辆的死亡人数为1：90,000,000。 

335
00:40:36,820 --> 00:40:46,300
每年约有30,000人死亡，目前在TESLA自动驾驶仪下有一人死亡。 

336
00:40:47,440 --> 00:40:55,320
你可以通过很多方式撕掉这个统计数据，但这是一个值得考虑的方法。已经，也许是自动化

337
00:40:55,700 --> 00:41:02,620
导致更安全的驾驶。问题是，我们不了解自动化， 

338
00:41:03,520 --> 00:41:12,940
因为我们没有数据：我们没有关于前进道路视频的数据，我们没有关于驱动程序的数据

339
00:41:13,650 --> 00:41:20,500
而我们今天在路上没有那么多自行驾驶的汽车。所以我们需要大量的数据。 

340
00:41:21,150 --> 00:41:29,580
我们将在课堂上为您提供部分内容，作为麻省理工学院研究的一部分，我们正在收集大量的内容， 

341
00:41:30,200 --> 00:41:38,180
汽车驾驶自己，收集数据是我们理解的方式。 

342
00:41:38,820 --> 00:41:51,220
所以谈论数据以及我们将要对我们的算法进行训练的内容，这里是特斯拉模型S，模型X. 

343
00:41:51,600 --> 00:41:58,120
我们已经对其中的17个进行了检测，已经收集了超过5,000小时和70,000英里。 

344
00:41:58,560 --> 00:42:08,420
我会谈谈我们放入它们的相机。我们正在收集前进道路的视频。 

345
00:42:08,490 --> 00:42:19,230
这是从波士顿到佛罗里达的一个驾驶特斯拉的人的一个亮点。什么也用蓝色显示是

346
00:42:19,230 --> 00:42:26,900
自动驾驶仪使用的时间：目前为0分钟，然后它会增长和增长。 

347
00:42:28,220 --> 00:42:34,160
长时间，如此数百英里，人们使用自动驾驶仪。超过13亿

348
00:42:34,590 --> 00:42:41,960
英特尔驾驶特斯拉，300,000,000正在自动驾驶。无论如何你做数学，25％。 

349
00:42:44,280 --> 00:42:50,820
所以我们正在收集司机前方道路的数据。我们在驱动程序上有2个摄像头。 

350
00:42:51,220 --> 00:43:05,370
出于隐私考虑，我们提供的课程是前进道路的时间史诗。使用相机

351
00:43:05,370 --> 00:43:13,240
记录是你的常规网络摄像头，计算机视觉社区的工作马。 C920， 

352
00:43:14,300 --> 00:43:16,820
我们在它上面有一些特殊的镜头。 

353
00:43:17,120 --> 00:43:23,120
现在这些网络摄像头有什么特别之处？没花70美元可以那么好，对吧？ 

354
00:43:24,180 --> 00:43:34,440
它们的特殊之处在于它们可以进行板载压缩并允许您收集大量数据

355
00:43:34,440 --> 00:43:42,520
并使用合理大小的存储容量来存储数据并训练您的算法。 

356
00:43:46,860 --> 00:43:54,720
那么在自动驾驶方面我们必须与之合作？我们如何建造一辆自动驾驶汽车？ 

357
00:43:55,920 --> 00:44:00,340
有这些传感器：雷达，激光雷达，视觉， 

358
00:44:01,180 --> 00:44:09,980
音频 - 所有外观都可以帮助您检测外部环境中的对象以自行定位等等。 

359
00:44:10,360 --> 00:44:15,520
面向内部的传感器：可见光摄像头，再次音频， 

360
00:44:15,860 --> 00:44:18,700
和红外摄像头，以帮助检测人民。 

361
00:44:19,750 --> 00:44:30,240
因此，我们可以将自动驾驶汽车任务分解为4个步骤：本地化，回答我在哪里;场景理解， 

362
00:44:30,900 --> 00:44:38,700
使用周围场景信息的纹理，来解释中不同对象的身份

363
00:44:38,700 --> 00:44:45,700
场景和那些物体的语义，它们的运动。 

364
00:44:46,780 --> 00:44:53,500
有运动计划 - 一旦你想到这一切，发现所有的行人，发现所有其他车， 

365
00:44:53,500 --> 00:45:04,900
我如何以安全合法的方式浏览这个迷宫，一堆杂物。还有司机状态， 

366
00:45:05,360 --> 00:45:08,940
如何使用视频或其他信息进行检测。 

367
00:45:10,480 --> 00:45:16,820
驾驶员的视频检测有关他们的情绪状态或他们的分心程度的信息。是的，问题。 

368
00:45:17,360 --> 00:45:21,680
（不可思议的问题） 

369
00:45:22,200 --> 00:45:30,080
是的，这是激光雷达的实时数字。激光雷达是为您提供3D点云的传感器

370
00:45:31,480 --> 00:45:37,680
外部场景。所以激光雷达是使用的技术

371
00:45:39,740 --> 00:45:49,480
大多数人使用自动驾驶汽车为您提供物体的强大地面真相。它可能是我们拥有的最好的传感器

372
00:45:49,920 --> 00:45:59,540
获取3D信息，关于外部环境的3D信息最少。题。 

373
00:46:07,380 --> 00:46:16,540
所以自动驾驶仪总是在变化。关于这款车的最令人惊奇的事情之一是自动驾驶仪的更新进来了

374
00:46:17,040 --> 00:46:18,400
软件的形式。 

375
00:46:18,940 --> 00:46:24,160
因此，随着时间的推移，可用于更改的时间变得更加保守。 

376
00:46:25,160 --> 00:46:27,720
但在此，这是早期版本之一， 

377
00:46:28,060 --> 00:46:37,220
并且它显示，黄色的第二行显示自动驾驶仪可用但未开启的频率。 

378
00:46:37,720 --> 00:46:44,180
因此，总驾驶时间为10小时，自动驾驶仪可用7小时，并且需要一小时。 

379
00:46:44,180 --> 00:46:49,640
这个特别的人是一个负责任的司机，因为你看到或

380
00:46:50,540 --> 00:46:56,140
是一个更谨慎的司机。你看到的是下雨，自动驾驶仪仍然可用

381
00:46:56,620 --> 00:46:56,960
但- 

382
00:46:57,260 --> 00:47:00,380
（不可思议的问题） 

383
00:47:00,660 --> 00:47:07,690
评论是你不应该相信那个死亡人数作为安全的指示，因为司机

384
00:47:07,690 --> 00:47:15,340
选择只在安全的情况下参与系统。这是完全开放的， 

385
00:47:16,260 --> 00:47:27,560
对于那个数字而言，有一个更大的论点而不仅仅是那一个，问题是这是否是一个坏的

386
00:47:27,560 --> 00:47:32,960
所以也许我们可以信任人类参与，你知道， 

387
00:47:34,230 --> 00:47:42,020
尽管YouTube视频播放效果不佳，尽管媒体大肆炒作，但你仍然是一个人。 

388
00:47:42,030 --> 00:47:46,100
乘坐金属盒每小时60英里，生活就行了。 

389
00:47:46,540 --> 00:47:54,180
除非您知道系统完全安全，否则您不会参与系统，除非您与系统建立了关系。 

390
00:47:54,380 --> 00:47:59,980
这并不是你看到一个人在特斯拉后面开始睡觉或下棋的所有东西， 

391
00:48:00,000 --> 00:48:05,140
管他呢。这就是YouTube的全部内容，现实情况是，只有你在车里

392
00:48:05,140 --> 00:48:09,970
它仍然是你的生活，所以你要做负责任的事情，除非你是一个少年

393
00:48:09,970 --> 00:48:13,260
等等，但无论你在什么地方，都不会改变。 

394
00:48:14,620 --> 00:48:15,120
题。 

395
00:48:15,120 --> 00:48:17,100
（不可思议的问题） 

396
00:48:17,100 --> 00:48:19,720
问题是：“你需要看什么？ 

397
00:48:19,860 --> 00:48:26,030
或感觉外部环境能够成功驾驶？你需要车道标记吗？你还需要其他 - 

398
00:48:26,030 --> 00:48:29,840
您根据哪些地标进行本地化和导航？“ 

399
00:48:29,840 --> 00:48:37,420
这取决于传感器。因此，在阳光明媚的加利福尼亚州使用Google自动驾驶汽车

400
00:48:37,740 --> 00:48:45,760
它以高分辨率的方式依赖激光雷达，映射环境以便能够本地化

401
00:48:46,040 --> 00:48:54,060
基于激光雷达。和激光雷达，现在我不知道激光雷达失效的具体细节， 

402
00:48:54,870 --> 00:48:59,700
但是下雨不好，雪也不好，不好

403
00:48:59,700 --> 00:49:06,790
当环境发生变化时。所以雪是什么，它改变了视觉，外观，反射纹理

404
00:49:06,790 --> 00:49:11,040
周围的表面。我们人类仍然能够解决问题

405
00:49:11,290 --> 00:49:18,540
但是一辆严重依赖激光雷达的汽车将无法使用这些地标进行本地化

406
00:49:18,540 --> 00:49:20,860
它以前检测到，因为它们看起来不同

407
00:49:20,860 --> 00:49:26,940
现在有了雪。计算机视觉可以帮助我们使用车道

408
00:49:27,410 --> 00:49:35,120
或者跟着一辆车。我们在车道上使用的两个地标跟随着你前面的车

409
00:49:35,120 --> 00:49:41,680
或者呆在两条车道之间。这对我们的道路来说是件好事，因为它们是为人眼设计的。 

410
00:49:42,060 --> 00:49:49,800
因此，您可以使用计算机视觉用于车道和前方车辆跟随它们。还有雷达。 

411
00:49:49,820 --> 00:49:58,560
这是一个原始但可靠的距离信息来源，可以让你不与金属物体碰撞。 

412
00:49:58,560 --> 00:50:04,760
因此，根据您想要依赖的内容，所有这些共同为您提供了大量信息。 

413
00:50:04,940 --> 00:50:13,480
问题是当现实生活的混乱发生时， 

414
00:50:13,800 --> 00:50:26,220
它在城市环境中的可靠性等等。本地化 - 深度学习如何帮助？ 

415
00:50:26,220 --> 00:50:40,640
首先，只是视觉测距的快速总结。它使用视频图像的单眼或立体声输入

416
00:50:41,780 --> 00:50:51,380
确定你在世界上的方向。在这种情况下，在世界框架中的车辆的方向

417
00:50:52,120 --> 00:50:55,180
而你所要处理的只是前进道路的视频

418
00:50:57,180 --> 00:51:03,180
通过立体声，您可以获得有关不同物体有多远的额外信息。 

419
00:51:06,820 --> 00:51:14,820
所以这是我们周五的一位演讲者将谈论他的专业知识（SLAM）同步本地化和制图。 

420
00:51:14,820 --> 00:51:24,160
这是一个非常充分研究和理解的检测外部场景中的独特特征的问题

421
00:51:25,290 --> 00:51:31,620
并根据这些独特功能的轨迹进行本地化。 

422
00:51:32,020 --> 00:51:36,460
当特征数量足够高时，它成为优化问题。 

423
00:51:37,140 --> 00:51:41,820
您知道这个特定的通道在帧与帧之间移动了一点点，您可以跟踪该信息。 

424
00:51:43,170 --> 00:51:49,860
并将所有内容融合在一起，以便能够通过三维空间估计您的轨迹。 

425
00:51:50,680 --> 00:51:59,660
您还有其他传感器可以帮助您。你有GPS非常准确，不完美，但非常准确。 

426
00:51:59,660 --> 00:52:01,980
这是帮助您自己进行本地化的另一个信号。 

427
00:52:02,560 --> 00:52:05,120
你也有IMU。加速度计

428
00:52:05,120 --> 00:52:15,200
告诉你你的加速度，从陀螺仪，加速度计，你有六个自由度

429
00:52:15,500 --> 00:52:24,160
关于移动物体，汽车如何在太空中航行的运动信息。 

430
00:52:25,910 --> 00:52:34,740
所以你可以用旧式的优化方式来做到这一点。 

431
00:52:36,030 --> 00:52:46,620
给定一组独特的功能，如筛选功能，该步骤涉及立体声输入的输入和输入

432
00:52:46,620 --> 00:52:53,300
并纠正图像。您有两个图像，从两个图像计算深度图，但每个像素

433
00:52:53,300 --> 00:52:54,150
计算

434
00:52:54,600 --> 00:53:06,340
最佳估计该像素的深度，三维位置，相对于相机然后你

435
00:53:06,340 --> 00:53:13,080
计算，这是你计算视差图的地方，也就是所谓的距离

436
00:53:14,010 --> 00:53:19,480
然后你会发现场景中独特，有趣的功能。筛选是一个受欢迎的。 

437
00:53:20,300 --> 00:53:25,700
这是一种用于检测独特功能的流行算法，随着时间的推移，您可以跟踪这些功能。 

438
00:53:26,160 --> 00:53:33,090
而这种跟踪可以让您通过视觉来获取有关您的轨迹的信息

439
00:53:33,090 --> 00:53:40,660
通过三维空间。你估计那个轨迹。有很多假设，假设身体是僵化的。 

440
00:53:42,630 --> 00:53:49,620
所以你必须弄清楚一个大物体是否正好在你面前，你必须弄清楚它是什么。 

441
00:53:52,520 --> 00:53:59,100
你必须弄清楚场景中的移动对象。那些是静止的。 

442
00:54:00,880 --> 00:54:08,720
或者你可以作弊或者我们将讨论并使用端到端的神经网络来做。 

443
00:54:09,380 --> 00:54:15,920
现在端到端意味着什么？在整个班级和今天，这将会出现很多次。端到端意味着， 

444
00:54:17,000 --> 00:54:28,020
我把它称为作弊，因为它消除了许多panageneric功能的艰苦工作。你接受原始输入

445
00:54:28,020 --> 00:54:37,580
任何传感器。在这种情况下，它从立体视觉相机获取立体声输入，因此两个图像，两个序列

446
00:54:37,580 --> 00:54:46,960
来自立体视觉相机的图像，输出是对空间轨迹的估计。 

447
00:54:47,630 --> 00:54:53,580
所以它应该是SLAM的艰苦工作，检测独特的功能，本地化自己，跟踪那些

448
00:54:53,580 --> 00:54:58,300
功能和确定你的轨迹在哪里。你只需训练网络。 

449
00:54:59,780 --> 00:55:03,600
通过一些地面真相，您可以形成更精确的传感器，如激光雷达， 

450
00:55:04,830 --> 00:55:08,920
你在一组输入上训练它，立体视觉输入， 

451
00:55:09,340 --> 00:55:17,580
和输出是通过空间的轨迹。你有一个单独的卷积神经网络的速度

452
00:55:18,020 --> 00:55:24,860
并为方向。这非常有效。不幸的是，不太好

453
00:55:25,930 --> 00:55:35,460
约翰伦纳德将谈论这个。 SLAM是深度学习无法超越的地方之一

454
00:55:35,460 --> 00:55:36,460
以前的方法。 

455
00:55:37,540 --> 00:55:44,440
深度学习真正有用的是场景理解部分。它正在解释场景中的对象。 

456
00:55:45,390 --> 00:55:50,620
它正在检测场景的各个部分，对它们进行分割

457
00:55:51,060 --> 00:55:58,300
并用光流确定它们的运动。所以以前检测对象的方法

458
00:56:00,400 --> 00:56:08,460
像交通信号一样，我们有TensorFlow教程的检测分类

459
00:56:09,100 --> 00:56:17,340
或使用类似汽车的功能或其他类型的功能，这些功能是从图像中精心设计的。 

460
00:56:18,760 --> 00:56:24,100
现在我们可以使用卷积神经网络来取代这些特征​​的提取。 

461
00:56:30,490 --> 00:56:34,200
而且还有SegNet的TensorFlow实现

462
00:56:35,060 --> 00:56:44,000
这与我所谈到的完全相同的神经网络。这是同样的事情，美丽就是你申请

463
00:56:44,280 --> 00:56:47,020
类似的网络类型到不同的问题

464
00:56:47,300 --> 00:56:55,150
并且根据问题的复杂性，可以获得相当惊人的性能。在这种情况下，我们卷积

465
00:56:55,150 --> 00:57:02,000
网络，意思是输出是图像，输入是图像，单个单眼图像。输出是一个

466
00:57:03,360 --> 00:57:10,120
分段图像，其中颜色表示您对该部分中对象的最佳逐像素估计。 

467
00:57:10,480 --> 00:57:15,840
这不使用任何空间信息，它不使用任何时间信息。 

468
00:57:16,260 --> 00:57:25,000
所以它分别处理每一帧，它能够将道路与树木分开， 

469
00:57:25,310 --> 00:57:29,280
来自行人，其他汽车等。 

470
00:57:30,710 --> 00:57:39,420
这是为了在雷达/激光雷达类型的技术之上，为您提供三维技术

471
00:57:39,420 --> 00:57:45,730
或立体视觉有关场景的三维信息。你是那样的，用那个身份画那个场景

472
00:57:45,730 --> 00:57:49,880
其中的对象，您对它的最佳估计。 

473
00:57:50,600 --> 00:57:56,140
我明天会谈到的是反复出现的神经网络

474
00:57:56,340 --> 00:58:02,500
我们可以使用与时间数据一起处理视频的重复神经网络

475
00:58:02,900 --> 00:58:11,600
并处理音频。在这种情况下，我们可以处理底部显示的内容

476
00:58:11,900 --> 00:58:15,360
湿路和干路的音频谱图。 

477
00:58:16,260 --> 00:58:20,660
您可以将该频谱图视为图像

478
00:58:22,700 --> 00:58:30,300
并使用重复神经网络以时间方式处理它。只需将其滑过并继续将其送入网络即可。 

479
00:58:31,570 --> 00:58:39,940
它在简单的任务上做得非常好，当然是干路和湿路。这个很重要， 

480
00:58:39,940 --> 00:58:49,700
一个微妙但非常重要的任务，有许多人喜欢它来了解道路，质地，质量。 

481
00:58:50,440 --> 00:58:53,920
道路的特点，潮湿是一个关键的。 

482
00:58:53,920 --> 00:58:57,820
当它不下雨但道路仍然潮湿时，这些​​信息非常重要。 

483
00:59:00,800 --> 00:59:13,200
好的，所以对于运动规划。同样的方法。右边是我们其他发言者的作品

484
00:59:13,200 --> 00:59:22,620
塞特卡拉曼。我们通过友好竞争解决流量的方法相同

485
00:59:23,060 --> 00:59:32,780
我们可以用它来克服Chris Gerdes用他的赛车来计划高速运动的轨迹

486
00:59:33,140 --> 00:59:37,960
沿着复杂的曲线。 

487
00:59:40,430 --> 00:59:46,700
所以我们可以使用优化来解决这个问题，使用优化解决控制问题， 

488
00:59:46,940 --> 00:59:50,460
或者我们可以通过跑步来加强学习

489
00:59:51,900 --> 00:59:56,080
通过模拟这条曲线，数以千万计，数亿次

490
00:59:56,520 --> 00:59:59,760
并且学习哪个轨迹都没有优化

491
00:59:59,760 --> 01:00:03,180
转弯的速度

492
01:00:03,180 --> 01:00:10,940
和车辆的安全性。与您用于流量完全相同的事情。 

493
01:00:12,690 --> 01:00:19,740
对于驾驶员状态，下周将讨论这个问题。这是所有有趣的面孔：眼睛，脸，情感。 

494
01:00:21,490 --> 01:00:29,900
这是驾驶员的视频，驾驶员身体的视频，驾驶员面部的视频。左边是其中一个助教

495
01:00:30,220 --> 01:00:42,980
在他年轻的时候。看起来仍然一样。他在那。所以在那种特殊情况下， 

496
01:00:43,780 --> 01:00:51,740
你正在做一个比较简单的问题，就是检测头部和眼睛的位置。 

497
01:00:51,920 --> 01:00:54,960
头部和眼睛的姿势。你知道它决定了什么叫做

498
01:00:55,700 --> 01:01:01,760
他凝视着司机，司机在看，瞥了一眼。所以， 

499
01:01:01,760 --> 01:01:09,770
我们将谈论这些问题。从左到右：绿色左边是更容易出问题;在红色

500
01:01:09,770 --> 01:01:12,300
从计算机视觉方面来说更难。 

501
01:01:13,110 --> 01:01:18,200
所以在左边是身体姿势，头部姿势。物体越大，检测越容易

502
01:01:18,200 --> 01:01:20,300
并且它的方向更容易检测。 

503
01:01:20,960 --> 01:01:28,760
然后有瞳孔直径。检测瞳孔，瞳孔的特征，位置，大小。 

504
01:01:29,280 --> 01:01:31,410
还有微观的扫视，发生的事情

505
01:01:31,410 --> 01:01:41,120
在一毫秒的频率，眼睛的震颤。确定驱动程序状态的所有重要信息。 

506
01:01:41,980 --> 01:01:51,500
有些是可能的计算机视觉，有些则不是。我想，这是我们周四要谈的内容。 

507
01:01:52,340 --> 01:01:58,600
是检测驾驶员的样子。所以，这是我们在特斯拉的一堆相机。这是

508
01:01:58,600 --> 01:02:05,100
这是Dan驾驶特斯拉并准确探测六个地区之一的位置

509
01:02:05,160 --> 01:02:12,030
我们已经转换成左，右，后视镜仪表组中心堆栈的分类问题

510
01:02:12,030 --> 01:02:15,260
或前进道路。因此，我们必须确定这六个类别

511
01:02:15,260 --> 01:02:23,380
司机看哪个方向。这对驾驶很重要。我们并不关心X，Y，Z 

512
01:02:23,380 --> 01:02:27,480
司机所在的位置。我们关心的是他们正在寻找道路。 

513
01:02:27,480 --> 01:02:31,060
他们是在看着他们的手机还是看着前方的车道？ 

514
01:02:31,220 --> 01:02:35,400
我们将能够使用卷积神经网络非常有效地回答这个问题。 

515
01:02:45,360 --> 01:02:51,280
您还可以使用CNN来提取情绪， 

516
01:02:52,860 --> 01:03:00,520
再次将情感，复杂的情感世界转化为沮丧与满足的二元问题。 

517
01:03:02,040 --> 01:03:08,270
这是与语音导航系统交互的驱动程序的视频。如果你曾经使用过一个， 

518
01:03:08,270 --> 01:03:10,760
你知道这可能是人们沮丧的根源。 

519
01:03:12,120 --> 01:03:16,180
所以这是自我报道，这是一个很难知道的驱动程序

520
01:03:16,180 --> 01:03:23,960
如果你正处于所谓的“有效计算”中，那就是情感。这是从计算方面研究情感的领域。 

521
01:03:25,760 --> 01:03:28,820
如果你在那个领域工作， 

522
01:03:28,820 --> 01:03:32,560
你知道情绪的注释方面真正具有挑战性。 

523
01:03:32,560 --> 01:03:36,720
因此，获得地面真相，好吧，因为这家伙的笑容

524
01:03:37,620 --> 01:03:45,260
所以我可以将其标记为快乐或者他皱眉，因为这意味着他很伤心。最有效的计算人员就是这么做的。 

525
01:03:46,240 --> 01:03:52,520
在这种情况下，我们自我报告询问人们他们在1到10的范围内是多么沮丧。 

526
01:03:53,180 --> 01:04:00,060
Dan up top报告了一个“1”因为没有沮丧，他对这种互动感到满意， 

527
01:04:00,440 --> 01:04:06,320
另一名司机报告为“9”，他对这种互动感到非常沮丧。而你注意到了什么

528
01:04:06,530 --> 01:04:11,900
Dan的脸上是否有一种非常寒冷，坚忍的表情，这表明幸福。 

529
01:04:12,900 --> 01:04:18,000
在沮丧的情况下，司机正在微笑。 

530
01:04:18,580 --> 01:04:24,180
所以这是一个很好的提醒，我们不能相信我们自己的人类本能。 

531
01:04:24,180 --> 01:04:34,120
这是一项工程特色。设计基本事实。我们必须信任这些数据，相信地面真相

532
01:04:34,580 --> 01:04:39,920
我们相信这是对场景中发生的事实的实际语义的最接近的反映。 

533
01:04:44,600 --> 01:04:49,460
好的，所以端到端的驾驶。进入项目和教程。 

534
01:04:49,960 --> 01:05:04,600
因此，如果驾驶就像一次谈话，并且感谢有人澄清，那就是凯旋门

535
01:05:04,600 --> 01:05:07,720
在这个视频中的巴黎。 

536
01:05:08,780 --> 01:05:18,820
如果驾驶就像是一种自然语言对话，那么我们可以将端到端驾驶视为跳过整个图灵测试

537
01:05:18,820 --> 01:05:23,960
组件并将其视为端到端的自然语言生成。 

538
01:05:24,570 --> 01:05:29,340
所以我们做的是将外部传感器作为输入

539
01:05:29,800 --> 01:05:36,300
和输出，车辆的控制。魔术发生在中间。 

540
01:05:38,050 --> 01:05:41,840
我们用神经网络替换整个步骤。 

541
01:05:43,310 --> 01:05:53,380
TA告诉我不要包含这张图片，因为它是我们见过的最好的图片。我道歉。谢谢谢谢。 

542
01:05:56,720 --> 01:05:58,020
我一点儿都不后悔。 

543
01:06:00,820 --> 01:06:06,240
所以这是展示我们自动驾驶汽车的道路

544
01:06:06,240 --> 01:06:12,140
但它仍然解释了我们拥有大量地面真相数据集的观点。 

545
01:06:13,000 --> 01:06:17,120
如果我们要制定驾驶任务来简单地拍摄外部图像

546
01:06:18,220 --> 01:06:24,780
并产生转向指令，加速制动指令，然后我们有很多地面真相。 

547
01:06:25,280 --> 01:06:31,800
我们每天都有大量的司机在路上行驶

548
01:06:31,980 --> 01:06:38,360
因此，我们收集了我们的基本真相，因为他们是生产转向命令的感兴趣的一方

549
01:06:38,580 --> 01:06:44,280
让它们保持活力，因此，如果我们要记录这些数据，它就变成了真相。 

550
01:06:44,760 --> 01:06:46,520
所以，如果有可能学到这一点， 

551
01:06:46,700 --> 01:06:50,680
我们能做的是我们可以为手动控制的车辆收集数据

552
01:06:50,920 --> 01:06:58,360
并使用该数据训练算法以控制自动驾驶车辆。 

553
01:07:01,060 --> 01:07:09,680
好吧，所以第一个做这件事的人之一就是Nvidia，他们实际上在外部图像中训练，即前方道路的图像。 

554
01:07:10,760 --> 01:07:17,900
和神经网络，卷积网络，简单的香草卷积神经网络我将简要概述： 

555
01:07:18,900 --> 01:07:22,900
拍摄图像，产生转向命令

556
01:07:23,530 --> 01:07:33,960
并且他们能够在某种程度上成功地学习导航基本转弯，曲线甚至停止

557
01:07:33,960 --> 01:07:42,560
或者在更敏感的部分急转弯。所以现在这个工作很简单。 

558
01:07:43,000 --> 01:07:51,300
底部有输入，顶部输出。输入是一个66x200像素的图像RGB。 

559
01:07:54,020 --> 01:07:58,560
左边显示的是原始输入，然后稍微裁剪它并调整大小

560
01:07:59,380 --> 01:08:07,520
66x200。这就是我们在代码中以及我们将为您提供的两个代码版本中的内容。 

561
01:08:07,700 --> 01:08:19,560
它们都在浏览器和TensorFlow中运行。它有几层。一些卷积层，一些完全连接的层。 

562
01:08:20,200 --> 01:08:23,940
并输出。这是一个回归网络。 

563
01:08:24,000 --> 01:08:29,060
它产生的不是猫与狗的分类，而是产生转向命令。 

564
01:08:29,300 --> 01:08:39,040
我该如何转动方向盘？而已。剩下的就是魔术，我们在人力投入上进行训练。 

565
01:08:44,890 --> 01:08:53,580
我们这里有一个项目，是在您的浏览器中运行的ConvNetJS系统的实现。 

566
01:08:54,080 --> 01:09:07,220
这是要遵循的教程和要采取的项目。因此，与DeepTraffic游戏不同，这是现实。 

567
01:09:07,560 --> 01:09:10,700
这是来自真实车辆的真实输入。 

568
01:09:11,940 --> 01:09:21,080
所以你可以去这个链接。演示昨天非常精彩，所以让我们看看，也许是两个人。 

569
01:09:32,880 --> 01:09:39,900
有教程，然后是实际游戏，实际模拟是在DeepTesla.JS上，我道歉。 

570
01:09:46,680 --> 01:09:53,760
现在每个人都去那里，不是吗？它适用于手机吗？它确实很好。 

571
01:09:57,740 --> 01:10:06,580
顶部的类似结构再次是网络学习时丢失功能的可视化

572
01:10:06,780 --> 01:10:08,060
而且它总是在训练。 

573
01:10:12,880 --> 01:10:21,020
接下来是网络布局的输入，输入200x66的规格。 

574
01:10:22,680 --> 01:10:30,800
有一个卷积层。有一个池层，输出是一个回归层。一个神经元。 

575
01:10:31,430 --> 01:10:38,580
这是一个很小的版本，DeepTiny，对吗？这是一个很小的版本

576
01:10:40,180 --> 01:10:49,980
Nvidia架构，然后您可以在真实视频上可视化此网络的运行。 

577
01:10:53,480 --> 01:10:59,380
由驾驶员，自动驾驶系统产生的实际车轮值， 

578
01:11:00,780 --> 01:11:04,060
为蓝色，网络输出为白色。 

579
01:11:08,980 --> 01:11:18,380
而绿色表示的是图像的裁剪，然后调整大小以生成66x200 

580
01:11:18,380 --> 01:11:29,020
输入到网络。再一次，令人惊讶的是，这是在您的浏览器中运行，对现实世界视频进行培训。 

581
01:11:29,620 --> 01:11:35,100
所以今天你可以进入你的车里输入它，也许可以像你一样教一个神经网络。 

582
01:11:36,050 --> 01:11:38,000
我们在ConvNetJS中有代码

583
01:11:38,000 --> 01:11:49,480
和TensorFlow这样做和教程。好吧，让我简要介绍一下这里的一些工作。 

584
01:11:50,910 --> 01:11:54,360
因此将网络输入作为单个图像。 

585
01:11:54,680 --> 01:12:02,000
这适用于DeepTesla.JS，单个图像，输出是-20到20之间的方向盘值。 

586
01:12:02,280 --> 01:12:07,300
这是度数。我们记录， 

587
01:12:08,750 --> 01:12:16,280
就像我说的，数千小时，但我们公开提供了10个特斯拉公路驾驶的视频剪辑。 

588
01:12:17,290 --> 01:12:27,680
一半由自动驾驶仪驱动，一半由人类驱动。从完美同步中提取的车轮值

589
01:12:29,390 --> 01:12:33,260
CAN，我们正在收集来自CAN的所有消息， 

590
01:12:33,920 --> 01:12:40,840
其中包含方向盘值并与视频同步。我们裁剪，提取窗口。我提到的绿色。 

591
01:12:42,460 --> 01:12:44,360
然后将其作为网络的输入提供。 

592
01:12:46,010 --> 01:12:53,030
所以这与DeepTraffic的区别在于红色汽车在交通中编织，因为有杂乱

593
01:12:53,030 --> 01:12:56,520
现实世界的照明条件。 

594
01:12:57,720 --> 01:13:04,740
在这个简单的转向任务中，你的大部分工作就是留在车道内， 

595
01:13:05,400 --> 01:13:14,310
在车道标记内。以端到端的方式，学会做到这一点。所以ConvNetJS 

596
01:13:14,310 --> 01:13:25,400
是卷积神经网络的CNN的javascript实现。它支持真正的任意网络。 

597
01:13:26,200 --> 01:13:32,200
我的意思是所有神经网络都很简单，但因为它在javascript中运行，所以它不使用GPU。 

598
01:13:33,020 --> 01:13:39,280
网络越大，计算量就越大。 

599
01:13:40,600 --> 01:13:44,900
现在不像DeepTraffic，这不是竞争

600
01:13:45,460 --> 01:13:50,840
但如果您是注册该课程的学生，您仍然需要提交代码，您仍然需要提交自己的代码

601
01:13:50,840 --> 01:14:01,800
汽车作为班级的一部分。题。所以问题是需要的数据量。 

602
01:14:03,680 --> 01:14:10,380
例如，驾驶特定任务所需的数据量是否有一般的经验法则？ 

603
01:14:11,570 --> 01:14:12,700
这是一个很好的问题。 

604
01:14:13,960 --> 01:14:23,240
你通常不得不像我说的那样，神经网络是很好的存储器，所以你必须让每个案例都代表

605
01:14:23,240 --> 01:14:32,160
训练说你感兴趣。尽可能的，这意味着，一般来说，如果你想要一张照片，如果你想

606
01:14:32,160 --> 01:14:37,190
分类猫狗之间的差异，你想拥有至少一千只猫和一千只狗

607
01:14:37,190 --> 01:14:46,740
而且他们做得很好。驾驶的问题是双重的：一，驾驶的大部分时间看起来都是一样的。 

608
01:14:47,380 --> 01:14:51,580
而你真正关心的是驾驶时看起来与众不同。这是所有边缘情况。 

609
01:14:52,220 --> 01:14:59,880
因此，我们对神经网络的不善，从普通情况到边缘情况，再到异常值。 

610
01:15:00,670 --> 01:15:02,050
所以避免崩溃

611
01:15:02,050 --> 01:15:07,700
只因为你能够成功地站在高速公路上数千小时并不意味着你可以避免撞车

612
01:15:07,700 --> 01:15:09,680
有人跑在你面前的路上

613
01:15:10,280 --> 01:15:20,120
而驾驶的另一部分是你必须达到的准确度非常高。所以对于猫与狗， 

614
01:15:20,360 --> 01:15:28,380
不，生活不依赖于你的错误。关于你在车道内驾驶汽车的能力。 

615
01:15:28,740 --> 01:15:36,120
你最好接近100％准确。有一个用于设计网络的盒子。 

616
01:15:36,870 --> 01:15:41,400
可以看到衡量网络性能的指标。 

617
01:15:42,060 --> 01:15:50,960
有一个可视化，层可视化，网络在每个卷积层提取的功能

618
01:15:51,120 --> 01:15:58,800
和每个完全连接的层。有能力重新开始培训。 

619
01:15:59,610 --> 01:16:15,300
可视化在真实视频上执行的网络。有输入层，卷积层。 

620
01:16:19,980 --> 01:16:33,600
视频可视化，右下方有趣的花絮是Will巧妙设计的条形码。 

621
01:16:36,460 --> 01:16:39,900
我如何清楚地解释为什么这么酷？ 

622
01:16:40,300 --> 01:16:47,460
这是一种通过视频同步多个数据流的方式， 

623
01:16:47,980 --> 01:16:53,640
因此，对于那些使用多模态数据的人来说，这里有很多数据流非常容易

624
01:16:53,960 --> 01:17:01,580
因为它们变得不同步，特别是当训练神经网络的一个重要组成部分正在改变数据时。 

625
01:17:02,000 --> 01:17:04,800
因此，您必须以巧妙的方式对数据进行洗牌

626
01:17:05,080 --> 01:17:11,000
因此，您不会过度拟合视频的任何一个方面，而是保持数据完全同步。 

627
01:17:11,540 --> 01:17:16,220
所以他做的是做连接方向盘的艰苦工作

628
01:17:16,220 --> 01:17:23,300
并且在视频中实际上将转向作为条形码放在视频之上。 

629
01:17:25,400 --> 01:17:36,680
最终的结果是你可以观察网络的运行情况，并且随着时间的推移，它会越来越多地学习正确驾驶。 

630
01:17:37,000 --> 01:17:41,370
为了时间的缘故，我会稍微通过这一点，只是总结一些你可以玩的东西

631
01:17:41,370 --> 01:17:49,580
在教程方面，让你们去。这是与端到端驱动相同的过程

632
01:17:49,960 --> 01:17:57,360
所以我们在GetHub上提供了代码。你刚刚放上我的GetHub和DeepTesla。 

633
01:17:57,760 --> 01:18:03,320
这需要一个视频或任意数量的视频列车

634
01:18:03,860 --> 01:18:09,820
并产生一个可视化，比较方向盘，实际方向盘和预测的方向盘。 

635
01:18:11,060 --> 01:18:16,260
方向盘，当它与人类驾驶员或自动驾驶系统一起点亮为绿色时

636
01:18:16,260 --> 01:18:20,320
当它不同意时，点亮为红色。希望不要经常。 

637
01:18:21,480 --> 01:18:26,160
同样，这是TensorFlow中如何完成的一些细节。 

638
01:18:26,500 --> 01:18:28,510
这是香草卷积神经网络。 

639
01:18:29,180 --> 01:18:39,060
指定一堆图层，卷积图层，完全连接的图层，训练模型，以便迭代批量图像。 

640
01:18:42,420 --> 01:18:53,960
在一组测试图像上运行模型并获得此结果。我们有一个教程

641
01:18:55,720 --> 01:18:59,420
在iPython Notebook上进入这个教程。 

642
01:18:59,420 --> 01:19:06,020
这可能是我们课堂上开始使用卷积神经网络的最佳方法。它在看着

643
01:19:06,280 --> 01:19:14,020
最简单的图像分类问题，交通灯分类。所以我们有这些交通信号灯的图像。 

644
01:19:14,930 --> 01:19:17,040
我们为您做了很好的检测。 

645
01:19:17,680 --> 01:19:22,800
所以现在你必须弄清楚，你必须构建得到的卷积网络

646
01:19:24,800 --> 01:19:33,460
弄清楚颜色的概念，当它看到红色，黄色或绿色时会变得兴奋。如果有人有疑问， 

647
01:19:34,260 --> 01:19:39,600
我会欢迎那些。如果您对Docker有任何疑虑，可以在课后留下， 

648
01:19:39,900 --> 01:19:50,520
与TensorFlow一起，如何赢得DeepTraffic。下课后或5点到7点周五来。请明天见。 

