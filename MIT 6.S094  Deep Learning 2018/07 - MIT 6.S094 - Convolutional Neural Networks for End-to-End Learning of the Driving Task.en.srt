1
00:00:04,820 --> 00:00:08,440
Alright, welcome back everyone.  Sound okay?  Alright.

2
00:00:10,440 --> 00:00:17,880
So today we will-  We talked a little bit about neural networks, started to talk about neural networks yesterday.

3
00:00:18,200 --> 00:00:25,850
Today we'll continue to talk about neural networks that work with images, convolutional neural networks,

4
00:00:26,620 --> 00:00:37,800
and see how those types of networks can help us drive a car.  If we have time we'll cover a simple illustrative case study

5
00:00:38,260 --> 00:00:41,460
of detecting traffic lights.

6
00:00:41,990 --> 00:00:49,240
The problem of detecting green, yellow, red.  If we can't teach our neural networks to do that, we're in trouble,

7
00:00:49,990 --> 00:00:59,660
but it's a good, clear, illustrative case study of a three-class classification problem. Okay, next there's

8
00:01:00,440 --> 00:01:08,900
DeepTesla here looped over and over in a very short GIF.  This is actually running live in a website right now.

9
00:01:08,900 --> 00:01:19,560
We'll show it towards the end of the lecture, this once again just like DeepTraffic is a neural network that learns to

10
00:01:19,560 --> 00:01:22,600
steer a vehicle based on the video of the forward road way.

11
00:01:22,660 --> 00:01:27,620
And once again, doing all of that in the browser using javascript.

12
00:01:28,800 --> 00:01:33,560
So you'll be able to train your own very network to drive using real world data.

13
00:01:34,540 --> 00:01:48,340
I'll explain how.  We will also have a tutorial and code.  Briefly described today at the end of the lecture,

14
00:01:48,340 --> 00:01:52,280
if there's time how to do the same thing in TensorFlow.

15
00:01:52,380 --> 00:02:00,960
So if you want to build a network that's bigger, deeper and you want to utilize GPUs to train that network,

16
00:02:00,960 --> 00:02:06,680
you want to not do it in your browser, you want to do it offline using TensorFlow

17
00:02:07,150 --> 00:02:14,120
and having a powerful GPU on your computer and we'll explain how to do that. Computer vision.

18
00:02:14,680 --> 00:02:22,280
So we talked about vanilla machine learning where there's no-  Where the size, yesterday,

19
00:02:22,800 --> 00:02:26,690
where the size of the input is small for the most part.

20
00:02:27,700 --> 00:02:34,180
The number of neurons, in the case the neural networks, is on the order of 10, 100, 1,000.

21
00:02:35,160 --> 00:02:42,080
When you think of images, images are a collection of pixels, one of the most iconic images from computer vision

22
00:02:42,620 --> 00:02:44,420
on the bottom left there is Lenna.

23
00:02:45,000 --> 00:02:53,040
I encourage you to Google it and figure out the story behind that image.  It's quite shocking when I found out recently.

24
00:02:55,720 --> 00:03:07,480
So once again, computer vision is, these days, dominated by data driven approaches by machine learning

25
00:03:11,260 --> 00:03:19,950
where all of the same methods that are used on other types of data are used on images where the input is just

26
00:03:19,950 --> 00:03:28,580
a collection of pixels and pixels are numbers from 0  to 255 discrete values.

27
00:03:29,420 --> 00:03:36,280
So we can think exactly what we've talked about previously, you could think of images in the same exact way.  It's just numbers

28
00:03:37,060 --> 00:03:41,860
and so we can do the same kind of thing.  We could do supervised learning where you have an input image

29
00:03:42,180 --> 00:03:48,360
and output label.  The input image here is a picture of a woman; the label might be "woman".

30
00:03:49,930 --> 00:03:57,260
On supervised learning, same thing.  We'll look at that briefly as well as clustering images into categories.

31
00:03:58,080 --> 00:04:04,500
Again semi-supervised and reinforcement learning.  In fact, the Atari games that talked about yesterday.

32
00:04:05,080 --> 00:04:11,720
do some pre-processing on the images.  They're doing computer vision;  they're using convolutional neural networks as we'll discuss today

33
00:04:13,040 --> 00:04:19,020
and the pipeline for supervised learning is again the same:  there's raw data in the form of images,

34
00:04:19,260 --> 00:04:26,180
there's labels on those images.  We perform a machine learning algorithm, performs feature extraction,

35
00:04:26,520 --> 00:04:33,900
it trains given the inputs and outputs on the images and the labels of those images, constructs the model

36
00:04:33,900 --> 00:04:35,140
and then test that model.

37
00:04:35,560 --> 00:04:41,500
And we get a metric and accuracy.  Accuracy is the term that's used to often describe how well the model performs.

38
00:04:42,280 --> 00:04:43,120
The percentage.

39
00:04:47,290 --> 00:04:54,220
I apologise for the constant presence of cats throughout this course.  I assure you this course is about driving, not cats.

40
00:04:56,380 --> 00:05:03,140
but images are numbers.  So for us we take it for granted.

41
00:05:03,760 --> 00:05:06,520
We're really good at looking

42
00:05:06,520 --> 00:05:12,560
and converting visual perception as human beings, converting visual perception, into semantics.

43
00:05:13,500 --> 00:05:22,040
We see this image and we know it's a cat but a computer only sees numbers:  RGB values for a color image.

44
00:05:22,560 --> 00:05:26,720
There's three values for every single pixel from 0 to 255.

45
00:05:27,840 --> 00:05:37,680
And so given that image, we can think of two problems:  one is regression and the other is classification.  Regression is when given an image

46
00:05:38,140 --> 00:05:43,200
we want to produce a real value of output put back. So if we have an image of the four roadway,

47
00:05:43,440 --> 00:05:50,340
we want to produce a value for the steering wheel angle and if you have an algorithm that's really smart,

48
00:05:50,840 --> 00:05:53,440
It can take any image of the forward roadway

49
00:05:53,600 --> 00:05:58,540
and produce the perfectly correct steering angle that drives the car safely across the United States.

50
00:05:59,340 --> 00:06:07,680
We'll talk about how to do that and where that fails.  Classification is when the input again is an image

51
00:06:08,140 --> 00:06:16,340
and the output is a class label, a discrete class label.  Underneath it though often is still a regression problem

52
00:06:16,340 --> 00:06:23,640
and once produced is a probability that this particular image belongs to a particular category.

53
00:06:24,590 --> 00:06:31,740
And we use a threshold to chop off the outputs associated with low probabilities

54
00:06:32,940 --> 00:06:37,980
and take the labels associated with high probabilities and convert it into a discrete classification.

55
00:06:40,020 --> 00:06:44,380
I mentioned this yesterday but it bears saying again, computer vision is hard.

56
00:06:47,320 --> 00:06:53,220
We, once again, take it for granted.  As human beings, we're really good at dealing with all these problems.

57
00:06:53,740 --> 00:07:00,080
There's viewpoint variation:  the object looks wholly different in terms of the numbers behind the images

58
00:07:00,360 --> 00:07:03,720
in terms of the pixels when viewed from a different angle.

59
00:07:04,080 --> 00:07:11,200
Viewpoint variation:  objects when you're standing far away from them or up close are totally different size.

60
00:07:11,800 --> 00:07:14,340
We're good at detecting that there are different size.

61
00:07:14,640 --> 00:07:20,960
It's still the same object as human beings but that's still a really hard problem because those sizes can vary drastically.

62
00:07:21,500 --> 00:07:29,400
We talked about occlusions and deformations with cats; well understood problem. There's background clutter.

63
00:07:30,390 --> 00:07:38,200
You have to separate the object of interest from the background and given the three dimensional structure of our world.

64
00:07:38,200 --> 00:07:44,880
There's a lot of stuff often going on in the background:  the clutter, their inter-class variation.

65
00:07:45,000 --> 00:07:51,100
That's often greater than inter-class variation; meaning objects of the same type often have more variation

66
00:07:51,220 --> 00:08:01,060
than the objects that you're trying to separate them from.  There is the hard one for driving:  illumination.

67
00:08:02,350 --> 00:08:06,800
Light is the way we perceive things; the reflection of light off the surface

68
00:08:07,860 --> 00:08:13,680
and the source of that light changes the way that object appears and we have to be robust to all of that.

69
00:08:17,210 --> 00:08:24,640
So the image classification pipeline is the same as I mentioned.  There are categories,

70
00:08:25,800 --> 00:08:30,840
It's the classification problems for those categories of cat, dog, mug, hat.

71
00:08:30,840 --> 00:08:34,620
You have a bunch of examples, image examples of each of those categories

72
00:08:34,880 --> 00:08:39,760
and so the input is just those images paired with the category.

73
00:08:41,210 --> 00:08:48,980
And you train to map, to estimate a function that maps from the images to the categories.

74
00:08:52,140 --> 00:08:55,360
For all of that you need data; a lot of it.

75
00:08:56,450 --> 00:09:03,880
There is, unfortunately, a growing number of data sets but there are still relatively small.

76
00:09:05,140 --> 00:09:12,600
We get excited.  There are millions of images but they're not billions or trillions of images and these are,

77
00:09:13,440 --> 00:09:17,760
the data sets that you will see if you read academic literature most often.

78
00:09:18,280 --> 00:09:20,920
Mnist, the one that's been beaten to death.

79
00:09:21,510 --> 00:09:34,340
And then we use as well in this course the data set of handwritten digits where the categories are 0 to 9.

80
00:09:35,440 --> 00:09:48,120
ImageNet, one of the largest image data sets; fully labeled image data sets in the world has images with a hierarchy of categories from Word Net.

81
00:09:49,560 --> 00:09:56,180
And what you see there is a labeling of what image is associated with which words are present in the data set.

82
00:09:57,380 --> 00:10:06,240
CIFAR-10 and CIFAR-100 are tiny images that are used to prove in a very efficient and quick way

83
00:10:06,500 --> 00:10:12,860
offhand that your algorithm that you're trying to publish on, or trying to impress the world with, works well.

84
00:10:13,580 --> 00:10:18,040
It's small, it's a small data set:  CIFAR-10 means there's 10 categories.

85
00:10:19,960 --> 00:10:27,500
And places is a data set of natural scenes: woods, nature, city, and so on.

86
00:10:27,940 --> 00:10:30,500
So let's look at CIFAR-10

87
00:10:30,720 --> 00:10:34,620
as a data set of 10 categories: airplane, automobile, bird, cat, and so on.

88
00:10:35,540 --> 00:10:38,640
They're shown there with sample images as the rose.

89
00:10:39,940 --> 00:10:47,220
And so let's build a classifier that's able to take images from one of these 10 categories and tell us what

90
00:10:48,300 --> 00:10:56,360
is shown in the image.  So how do we do that?  Once again, all the algorithm sees is numbers.

91
00:10:58,450 --> 00:11:06,740
So we have to try to have at the very core, we have to have an operator for comparing two images.

92
00:11:07,440 --> 00:11:10,020
So given an image and I want to save it as a cat or dog.

93
00:11:10,020 --> 00:11:16,280
I want to compare it to images of cats and compare it to images of dogs and see which one matches better.

94
00:11:16,680 --> 00:11:24,380
So there has to be a comparative operator.  Okay so one way to do that is take the absolute difference between the two images

95
00:11:24,380 --> 00:11:27,660
pixel by pixel, take the difference between

96
00:11:29,240 --> 00:11:38,560
each individual pixel shown on the bottom of the slide for a 4x4 image.  And then we sum that pixel-wise

97
00:11:38,960 --> 00:11:45,020
pixel-wise absolute difference into a single number.  So if the image is totally different pixel-wise,

98
00:11:45,840 --> 00:11:46,980
that will be a high number.

99
00:11:47,200 --> 00:11:53,240
If it's the same image, the number will be 0.  Oh, it's the absolute value too of the difference.

100
00:11:56,420 --> 00:12:05,500
And that's called L1 distance.  It doesn't matter.  When we speak of distance, we usually mean L2 distance.

101
00:12:07,480 --> 00:12:17,960
And so, if we try to-  So we can build the classifier that just uses this operator to compare it to every single image in the data set

102
00:12:18,160 --> 00:12:23,940
and say I'm going to pick the, I'm going to pick the category

103
00:12:23,940 --> 00:12:30,020
that's the closest using this comparative operator.  I'm going to find-  I have a picture of a cat

104
00:12:30,020 --> 00:12:34,100
and I'm going to look through the dataset and find the image that's the closest to this picture

105
00:12:35,270 --> 00:12:37,800
and say that is the category that this picture belongs to.

106
00:12:38,700 --> 00:12:45,280
So if we just flip the coin and randomly pick which category an image belongs to get that accuracy,

107
00:12:46,410 --> 00:12:49,700
would be on average 10%.  It's random.

108
00:12:51,100 --> 00:12:59,980
The accuracy with which our brilliant image difference algorithm that just goes through the data set

109
00:12:59,980 --> 00:13:08,200
and finds the closest one is 38% which is pretty good, it's way above 10%.

110
00:13:10,580 --> 00:13:13,780
So you can think about this operation of look into the base

111
00:13:13,780 --> 00:13:19,900
and finding the closest image as what's called K-Nearest Neighbors

112
00:13:20,460 --> 00:13:27,340
or K in that case.  Meaning you find the one closest neighbor to this image that you're asking questions about

113
00:13:28,410 --> 00:13:33,480
and accept the label from that image.  You could do the same thing increasing K.

114
00:13:34,650 --> 00:13:38,580
Increasing K to 2 means you take the two nearest neighbors.

115
00:13:39,540 --> 00:13:47,540
You find the two closest in terms of pixel-wise image difference through this particular query image

116
00:13:48,690 --> 00:13:51,260
and find which categories did those belong to.

117
00:13:52,530 --> 00:13:58,780
What's shown up top on the left is the data set we're working with:  red, green, blue.

118
00:14:00,000 --> 00:14:05,000
What's shown in the middle is the one nearest neighbor classifier, meaning

119
00:14:05,860 --> 00:14:10,860
this is how you segment the entire space of different things that you can compare.

120
00:14:13,950 --> 00:14:16,900
And if a point falls into any of these regions,

121
00:14:17,240 --> 00:14:23,780
it will be immediately associated with the nearest neighbor algorithm to belong to that image, to that region.

122
00:14:25,180 --> 00:14:33,040
With the five nearest neighbors, there's immediately an issue.  The issue is that there is white regions.

123
00:14:33,040 --> 00:14:43,400
There's tie breakers where your five closest neighbors are from various categories.  So it's unclear where you belong to.

124
00:14:45,680 --> 00:14:52,780
So this is a good example of parameter tuning.  You have one parameter:  K.

125
00:14:54,800 --> 00:15:03,940
And your task as a teacher of machine learning, you have to teach this algorithm how to do your learning for you,

126
00:15:05,160 --> 00:15:06,880
is to figure out that parameter.

127
00:15:07,500 --> 00:15:12,720
That's called "parameter tuning" or "hyper-parameter tuning" as it's called in neural networks.

128
00:15:14,730 --> 00:15:25,450
And so on the bottom right of the slide on the x-axis is K.  As we increase it from 0 to 100 and

129
00:15:25,450 --> 00:15:34,060
the y-axis is classification accuracy.  It turns out that the best K for this data set is 7, 7 years neighbors.

130
00:15:34,500 --> 00:15:42,460
With that we get a performance of 30% human level performance

131
00:15:44,060 --> 00:15:51,220
and I should say that the way we get that number as we do with a lot of the machine learning pipeline

132
00:15:52,640 --> 00:16:00,020
process is you separate the data into the parts of days that you use for training

133
00:16:00,020 --> 00:16:07,320
and another part they use for testing.  You're not allowed to touch the testing part.  That's cheating.

134
00:16:07,500 --> 00:16:14,260
You construct your model of the world on the training data set and you use what's called cross validation

135
00:16:15,100 --> 00:16:25,870
where you take a small part of the training data shown "fold five" there in yellow to leave that part out from

136
00:16:25,870 --> 00:16:37,120
the training and then use it as part of the hyper-parameter tuning.  As you train, figure out with that yellow part fold five

137
00:16:37,780 --> 00:16:42,740
how well you're doing  and then you choose a different fold and see how well you're doing

138
00:16:42,880 --> 00:16:47,860
And keep playing with parameters never touching the test part.  And when you're ready,

139
00:16:47,860 --> 00:16:54,360
you run the algorithm on a test data to see how well you really do.  How will it really generalizes.  Yes, question.

140
00:16:54,500 --> 00:16:56,440
(INAUDIBLE QUESTION)

141
00:16:59,140 --> 00:17:01,200
So, the question was:  "is there a good way to-

142
00:17:02,330 --> 00:17:07,040
Is any good intuition behind what a good K is?"  There are general rules for different data sets

143
00:17:07,340 --> 00:17:13,200
but usually you just have to run through it.  Grid search, brute force.  Yes, question.

144
00:17:13,300 --> 00:17:14,420
(INAUDIBLE QUESTION)

145
00:17:14,420 --> 00:17:15,420
(CHUCKLING)

146
00:17:15,420 --> 00:17:17,220
Good question.  Yes.

147
00:17:17,220 --> 00:17:25,080
(INAUDIBLE QUESTION)

148
00:17:25,080 --> 00:17:28,580
Yes, the question was:  "is each pixel 1 number or 3 numbers?"

149
00:17:29,120 --> 00:17:35,040
For majority of computer vision throughout its history used grayscale images so it's 1 number but RGB

150
00:17:35,040 --> 00:17:40,920
is 3 numbers and there's sometimes a depth value too, so it's 4 numbers. So it's-

151
00:17:42,120 --> 00:17:45,580
If you have a stereo vision camera that gives you the depth information of the pixels,

152
00:17:45,800 --> 00:17:52,140
that's a fourth and then if you stack two images together there could be 6.  In general,

153
00:17:52,140 --> 00:17:56,540
everything we work with will be 3 numbers for a pixel.

154
00:18:00,000 --> 00:18:02,980
Yes, so the question:  "as to the absolute value is just one number?"

155
00:18:02,980 --> 00:18:05,640
Exactly right.  So in that case, those are grayscale images.

156
00:18:06,020 --> 00:18:07,480
So it's not RGB images.

157
00:18:10,560 --> 00:18:14,780
So, you know, this algorithm is pretty good if we use the best.

158
00:18:17,770 --> 00:18:23,200
We optimize the hyper-parameters of this algorithm, choose K of 7,

159
00:18:23,200 --> 00:18:29,580
seems to work well for this particular CIFAR-10 data set.  Okay, we get 30% accuracy.

160
00:18:30,080 --> 00:18:37,900
It's impressive, higher than 10%.  Human beings perform at about 94, slightly above 94%

161
00:18:37,900 --> 00:18:46,620
accuracy for CIFAR-10.  So given an image and it's a tiny image.  I should clarify it, it's like a little icon.

162
00:18:49,150 --> 00:18:55,360
Given that image, human beings are able to determine accurately one of the 10 categories with 94% accuracy.

163
00:18:55,790 --> 00:19:00,460
And the currently state-of-the-art convolutional neural networks is ninety five,

164
00:19:00,460 --> 00:19:06,600
it's 95.4% accuracy and, believe it or not, it's a heated battle

165
00:19:07,530 --> 00:19:13,040
but the most important, the critical fact here, is it's recently surpassed humans.

166
00:19:13,640 --> 00:19:22,800
And certainly surpass the k-nearest neighbors algorithm.  So,how does this work?  Let's briefly look back.

167
00:19:23,490 --> 00:19:28,100
It all still boils down to this little guy:  the neuron,

168
00:19:28,150 --> 00:19:36,760
that sums the weights of its inputs, adds a bias, produces an output based on an activation, a smooth activation function.

169
00:19:38,900 --> 00:19:40,100
Yes, question.

170
00:19:40,280 --> 00:19:44,520
(INAUDIBLE QUESTION)

171
00:19:45,880 --> 00:19:48,980
The question was:  "do you take a picture of Cassie, you know it's a cat,

172
00:19:50,140 --> 00:19:56,140
but that's not encoded anywhere, like you have to write that down somewhere.

173
00:19:56,700 --> 00:19:59,520
So you have to write as a caption:  "This is my cat."

174
00:19:59,880 --> 00:20:05,480
And then the unfortunate thing, given the internet and how woody it is, you can't trust the captions on images.

175
00:20:06,220 --> 00:20:13,100
because maybe you're just being clever and it's not a cat all, it's a dog dressed as a cat.  Yes, question.

176
00:20:14,240 --> 00:20:15,740
(INAUDIBLE QUESTION)

177
00:20:18,420 --> 00:20:20,500
Sorry.  Seen as do better than what?

178
00:20:26,890 --> 00:20:30,780
Yes, so the question was:  "do convolutional neural networks generally do better than nearest neighbors?

179
00:20:31,240 --> 00:20:39,020
There's very few problems on which neural networks don't do better, yes ,they almost always do better

180
00:20:39,020 --> 00:20:50,080
except when you have almost no data.  So you need data.  And convolutional neural networks isn't some special magical thing.

181
00:20:50,080 --> 00:20:58,060
It's just neural networks with some cheating up front that I'll explain, some tricks to try to reduce the size

182
00:20:58,300 --> 00:21:00,220
and make it capable to deal with images.

183
00:21:01,680 --> 00:21:06,720
So again.  Yes, the input is, in this case that we looked at classifying an image of a number,

184
00:21:07,400 --> 00:21:13,920
as opposed to doing some fancy convolutional tricks.  We just take the the entire 28x28

185
00:21:13,940 --> 00:21:19,940
pixel image that's 784 pixels as the input.

186
00:21:20,380 --> 00:21:26,880
That's 784 neurons in the input, 15 neurons on the hidden layer and 10 neurons in the output.

187
00:21:28,640 --> 00:21:32,480
Now everything we'll talk about has the same exact structure.  Nothing fancy.

188
00:21:34,830 --> 00:21:41,360
There is a forward pass through the network where you take an input image and produce an output classification

189
00:21:41,920 --> 00:21:47,960
and there's a backward pass through the network for Back Propagation where you adjust the weights

190
00:21:47,960 --> 00:21:53,060
when your prediction doesn't match the Ground Truth output.

191
00:21:54,700 --> 00:22:01,220
And learning just boils down to optimization; it's just optimizing a smooth function.

192
00:22:02,130 --> 00:22:06,980
Differentiable function; that's defined as the lost function.

193
00:22:07,340 --> 00:22:13,180
That's usually as simple as a squared error between the true output

194
00:22:13,180 --> 00:22:18,610
and the one you actually got.  So what's the difference?  What are convolutional neural networks?

195
00:22:20,650 --> 00:22:32,260
Convolutional neural networks take inputs that have some spatial consistency, have some meaning to the spatial-

196
00:22:34,800 --> 00:22:42,620
Has some spatial meaning in them like images.  There's other things, you can think of the dimension of time.

197
00:22:43,620 --> 00:22:54,560
And you can input audio signal into a convolutional neural network.  And so the input is, usually for every single layer,

198
00:22:54,800 --> 00:22:59,860
that's a convolutional layer, the input is a 3D volume and the output is a 3D volume.

199
00:23:01,080 --> 00:23:06,840
I'm simplifying because you can call it 4D too but it's 3D.  There's height, width and depth.

200
00:23:07,820 --> 00:23:11,840
So that's an image.  The height and the width is the width and the height of the image.

201
00:23:12,480 --> 00:23:19,220
And then the depth for grayscale image is 1; for an RGB image is 3;

202
00:23:20,320 --> 00:23:26,440
for a ten-frame video of greyscale images the depth is 10.

203
00:23:27,670 --> 00:23:36,420
It's just a volume, a three-dimensional matrix of numbers.  And everything-

204
00:23:36,420 --> 00:23:43,670
The only thing that a convolutional layer does is take a 3D volume's input, produce a 3D volume as output

205
00:23:44,470 --> 00:23:46,180
and has some smooth function.

206
00:23:47,610 --> 00:23:51,240
Operating on the inputs, on the sum of the inputs,

207
00:23:52,900 --> 00:24:00,500
that may or may not be a parameter that you tune, that you try to optimize.  That's it.

208
00:24:01,740 --> 00:24:05,380
So Lego pieces that you stack together in the same way as we talked about before.

209
00:24:07,220 --> 00:24:11,500
So what are the types of layers that a convolutional neural networks have?  There's inputs.

210
00:24:11,920 --> 00:24:19,160
So for example a color image of 32x32 will be a volume of 32x32x3.

211
00:24:21,770 --> 00:24:34,080
A convolutional layer takes advantage of the spatial relationships of the input neurons and a convolutional layer,

212
00:24:35,880 --> 00:24:37,720
it's the same exact neuron

213
00:24:38,000 --> 00:24:41,580
as for fully connected network, the regular we talked about before.

214
00:24:41,930 --> 00:24:52,000
But it has a narrower receptive field, it's more focused, the inputs to a neuron on the convolutional layer

215
00:24:52,980 --> 00:24:57,840
come from a specific region from the previous layer.  And the parameters

216
00:24:57,840 --> 00:25:03,000
on each filter, you can think of this as a filter, because you slide it across the entire image.

217
00:25:05,860 --> 00:25:12,360
And those parameters are shared.  So supposed you've taken the-  If you think about two layers,

218
00:25:12,780 --> 00:25:20,660
as opposed to connecting every single pixel in the first layer to every single neuron in the following layer.

219
00:25:20,920 --> 00:25:30,070
You only connect the neurons in the input layer that are close to each other, to the output layer, and then you

220
00:25:30,070 --> 00:25:37,620
enforce the weights to be tied together spatially.

221
00:25:39,070 --> 00:25:45,080
And what that results in is a filter every single layer on the output,

222
00:25:45,080 --> 00:25:49,940
you can think of as a filter, they get excited for example for an edge

223
00:25:50,800 --> 00:25:55,480
and when it sees this particular kind of edge in the image, it will get excited.

224
00:25:55,770 --> 00:26:01,540
And it'll get excited in the top left of the image, on the top right, bottom left, bottom right.

225
00:26:02,930 --> 00:26:08,960
The assumption there is that a powerful feature for detecting a cat

226
00:26:09,700 --> 00:26:12,800
is just as important no matter where in the image it is.

227
00:26:14,270 --> 00:26:25,780
And this allows you to cut away a huge number of connections between neurons but it still boils down on the right,

228
00:26:26,640 --> 00:26:30,760
as a neuron that sums a collection of inputs

229
00:26:31,300 --> 00:26:43,300
and applies weights to them.  The spatial arrangement of the output volume relative to the input volume

230
00:26:43,300 --> 00:26:51,360
is controlled by three things.  The number of filters.  So for every single "filter"

231
00:26:51,630 --> 00:26:58,560
you get an extra layer on the output.  So if the input,

232
00:26:58,830 --> 00:27:05,870
let's talk about the very first layer, the input is 32x32x3.  It's in RGB

233
00:27:05,870 --> 00:27:13,280
image of 32x32.  If the number of filters is 10,

234
00:27:14,310 --> 00:27:27,060
then the resulting depth the resulting number of stacked channels in the output will be 10.  Stride is given.

235
00:27:27,660 --> 00:27:36,380
is the step size of the filter that you slide along the image.  Often times as just 1 or 3

236
00:27:37,200 --> 00:27:44,280
and that directly reduces the size, the spatial size the width and the height, of the output image.

237
00:27:45,130 --> 00:27:53,220
and then there is a convenient thing that it's often done is padding.  The image on the outside zeros.

238
00:27:54,080 --> 00:28:05,420
So that the input and the output have the same height and width.  So this is a visualization of convolution.

239
00:28:06,250 --> 00:28:10,540
I encourage you to, kind of maybe offline, think about what's happening.

240
00:28:11,060 --> 00:28:19,460
It's similar to the way human vision works, crudely so, if there's any experts in the audience.

241
00:28:20,410 --> 00:28:30,580
So the input here on the left is a collection of numbers:  0, 1, 2.  And a filter

242
00:28:36,720 --> 00:28:41,250
or there are two filters shown as W1-

243
00:28:41,250 --> 00:28:49,380
W0 and W1.  Those filters shown in red, are the different weights applied in those filters.

244
00:28:50,530 --> 00:28:55,200
And each of the filters have a certain depth; just like the input a depth of 3.

245
00:28:56,040 --> 00:29:02,580
So there are three of them in each column and so,

246
00:29:04,200 --> 00:29:09,800
so you slide death filter along the image keeping the weights the same.

247
00:29:09,800 --> 00:29:11,520
this is the sharing of the weights

248
00:29:12,700 --> 00:29:19,240
and so your first filter you pick the weights, this is an optimization problem. you pick the weights in such a way

249
00:29:19,240 --> 00:29:24,620
that it fires, it gets excited, for useful features and doesn't fire for not useful features.

250
00:29:25,140 --> 00:29:28,980
And then there's a second filter that fires for useful features and not.

251
00:29:29,840 --> 00:29:40,180
And produces a signal on the output depending on a positive number, meaning there's a strong feature in that region,

252
00:29:40,520 --> 00:29:44,360
and negative number if there isn't but the filter is the same.

253
00:29:44,720 --> 00:29:49,820
This allows for a drastic reduction in the parameters and so you can deal with

254
00:29:50,540 --> 00:29:59,260
inputs.  There are a thousand by thousand pixel image, for example, or video. There's a really powerful concept there.

255
00:30:02,750 --> 00:30:04,700
The spatial sharing of weights.

256
00:30:04,980 --> 00:30:11,420
That means there's a spatial invariance to the features you're detecting.  It allows you to learn from arbitrary images

257
00:30:11,960 --> 00:30:17,580
so you don't have to be concerned about pre-processing the images in some clever way,

258
00:30:17,700 --> 00:30:23,840
you just give the raw image.  There is another operation:  pooling.

259
00:30:24,880 --> 00:30:30,500
It's a way to reduce the size of the layers by, for example in this case,

260
00:30:30,500 --> 00:30:35,810
it's max pooling for taking a collection of outputs and choose x1

261
00:30:35,810 --> 00:30:48,170
and summarizing those collection of pixels such that the output of the pooling operation is much smaller than the input.

262
00:30:49,970 --> 00:30:59,120
Because the justification there is that you don't need a high resolution.

263
00:31:00,340 --> 00:31:08,020
Localization of which pixel is important in the image or according to, you know,

264
00:31:08,020 --> 00:31:14,740
you don't need to know exactly which pixel is associated with the cat ear or a cat face.

265
00:31:15,840 --> 00:31:18,460
As long as you, kind of, know it's around that part

266
00:31:18,640 --> 00:31:22,720
and that reduces a lot of complexity in the operations. Yes, question.

267
00:31:27,280 --> 00:31:32,340
The question was:  "when is too much pooling, when do you stop pooling?"

268
00:31:35,780 --> 00:31:46,480
So pooling is a very crude operation that doesn't have any, one thing you need to know, is it doesn't have any

269
00:31:46,480 --> 00:31:49,060
parameters that are learnable.

270
00:31:49,580 --> 00:31:56,740
So you can't learn anything clever about pooling.  You're just picking, in this case

271
00:31:56,740 --> 00:32:04,840
max pool, so you're picking the largest number.  So you're reducing the resolution, you're losing a lot of information.

272
00:32:05,460 --> 00:32:10,680
There's an argument that you're not, you know, losing that much information as long as you're not pooling the entire

273
00:32:10,680 --> 00:32:12,200
image into a single value

274
00:32:12,940 --> 00:32:22,460
but you're gaining training efficiency, you're gaining the memory size, reducing the size of the network.

275
00:32:22,980 --> 00:32:30,560
So, it's definitely a thing that people debate and it's a parameter that you play with to see what works for you.

276
00:32:33,290 --> 00:32:40,500
Okay, so how does this thing look like as a whole, a convolutional neural network, the input is an image

277
00:32:41,700 --> 00:32:52,360
there's usually a convolutional layer, there is a pooling operation, another convolutional layer, another pooling operation and so on.

278
00:32:54,370 --> 00:33:01,700
At the very end, if the task is classification you have a stack of convolutional layers and pooling layers.

279
00:33:02,020 --> 00:33:04,520
There are several fully connected layers.

280
00:33:05,120 --> 00:33:14,440
So, you go from those spatial convolutional operations to fully connecting every single neuron in a layer to the

281
00:33:14,440 --> 00:33:15,280
following layer.

282
00:33:15,720 --> 00:33:22,920
And you do this so that by the end, you have a collection of neurons each one is associated with a particular class.

283
00:33:23,700 --> 00:33:30,940
So in what we looked at yesterday is the input, is an image of a number 0 through 9.

284
00:33:31,300 --> 00:33:41,020
The output here would be 10  neurons. So you blow down that image with a collection of convolutional layers,

285
00:33:41,840 --> 00:33:47,800
with 1 or 2 or 3 fully connected layers at the end that all lead to 10 neurons

286
00:33:48,320 --> 00:33:53,560
and each of those neuron's job is to get fired up

287
00:33:54,260 --> 00:34:02,020
when it sees a particular number and for the other ones to produce a low probability.  And so this kind of process

288
00:34:02,620 --> 00:34:10,900
is how you have the 95 percentile accuracy on the CIFAR-10 problem.

289
00:34:10,900 --> 00:34:19,360
This here is ImageNet data set that I mentioned.  It's how you take this image of a leopard, of a container ship,

290
00:34:20,160 --> 00:34:24,140
and produce a probability that that is a container ship or a leopard.

291
00:34:25,430 --> 00:34:32,120
Also shown there are the outputs of the other nearest neurons in terms of their confidence.

292
00:34:37,300 --> 00:34:44,440
Now you can use the same exact operation by chopping off the fully connected layer at the end

293
00:34:44,860 --> 00:34:53,260
and as opposed to mapping from image to a prediction of what's contained in the image, you map from the image to another image.

294
00:34:54,520 --> 00:35:00,640
And you can train that image to be one that gets excited

295
00:35:01,720 --> 00:35:11,940
spatially, meaning it gives you a high, close to one value, for areas of the image that contain the object of interest

296
00:35:13,380 --> 00:35:19,760
and then a low number for areas of the image that are unlikely to contain that image.

297
00:35:20,870 --> 00:35:23,240
And so from this you can go on the left,

298
00:35:23,240 --> 00:35:31,200
an original image of a woman on a horse, to a segmented image of knowing where the woman is and where the horse is

299
00:35:31,200 --> 00:35:38,830
and where the background is.  The same process can be done for detecting the object.

300
00:35:39,520 --> 00:35:42,080
So you can segment the scene into a bunch of

301
00:35:42,300 --> 00:35:53,520
interesting objects, candidates for interesting objects and then go through those candidates one by one

302
00:35:53,520 --> 00:36:00,460
and perform the same kind of classification as in the previous step where it's just an input as an image and the output as a classification.

303
00:36:01,050 --> 00:36:09,160
And through this process of hopping around an image, you can figure out exactly where is the best way to segment the cow

304
00:36:09,160 --> 00:36:17,360
out of the image.  That's called object detection.  Okay, so

305
00:36:18,230 --> 00:36:28,320
how can these magical convolutional neural networks help us in driving?  This is a video of the forward road way from a

306
00:36:28,320 --> 00:36:35,020
data set that we'll look at, that we've collected from a Tesla.  But first let me look at driving.

307
00:36:35,820 --> 00:36:47,850
Briefly, the general driving task from the human perspective.  On average an American driver in the United States

308
00:36:47,850 --> 00:36:58,040
drives 10,000 miles a year.  A little more for rural, a little less for urban.  There is about 30,000

309
00:36:58,040 --> 00:37:06,040
fatal crashes and >32,000 sometimes as high as 38,000 fatalities a year.

310
00:37:06,790 --> 00:37:14,460
This includes car occupants, pedestrians, bicyclists and motorcycle riders.

311
00:37:16,460 --> 00:37:25,040
This may be a surprising fact but in a class on self-driving cars we should remember that.

312
00:37:25,040 --> 00:37:34,440
So ignore the 59.9%, that's other.  The most popular cars in the United States are pickup trucks:  Ford F-1 Series,

313
00:37:34,900 --> 00:37:44,160
Chevy Silverado, Ram.  It's an important point that we're still married to our,

314
00:37:46,160 --> 00:37:56,200
to wanting to be in control and so one of the interesting cars that we look at

315
00:37:56,980 --> 00:38:02,900
and the car that is the days that we provide to the class is collected from is a Tesla.

316
00:38:03,890 --> 00:38:07,290
It's the one that comes at the intersection of the Ford F-150

317
00:38:08,280 --> 00:38:17,080
and the cute, little Google self-driving car on the right.  It's fast, it allows you to have a feeling of control

318
00:38:17,080 --> 00:38:22,120
but it can also drive itself for hundreds of miles on the highway, if need be.

319
00:38:24,020 --> 00:38:27,500
It allows you to press a button and the car takes over.

320
00:38:28,280 --> 00:38:33,280
It's a fascinating trade-off, of transferring control from the human to the car.

321
00:38:35,290 --> 00:38:37,380
It's a transfer of trust

322
00:38:38,260 --> 00:38:48,600
and it's a chance for us to study the psychology of human beings as they relate to machines at >60 miles an hour.

323
00:38:51,670 --> 00:39:02,220
In case you're not aware a little summary of human beings, where distracted things:  would like to text, use the smartphone,

324
00:39:03,190 --> 00:39:11,000
watch videos, groom, talk to passengers, eat, drink, texting.

325
00:39:12,380 --> 00:39:18,440
169 billion texts were sent in the US every single month in 2014.

326
00:39:20,170 --> 00:39:27,760
On average, 5 seconds our eyes spent off the road while texting -  5 seconds.

327
00:39:30,360 --> 00:39:41,830
That's the opportunity for automation to step in.  More than that, there's what NHTSA refers to as the 4 D's:  drunk, drugged,

328
00:39:41,830 --> 00:39:42,850
distracted and drowsy.

329
00:39:43,300 --> 00:39:53,180
Each one of those opportunity is for automation to step in.  Drunk driving stands to benefit significantly

330
00:39:53,710 --> 00:40:06,640
from automation, perhaps.  So the miles, let's look at the miles.  The data.  There's 3 trillion (3 million million)

331
00:40:06,640 --> 00:40:14,820
3 million million miles driven every year and TESLA autopilot, our case study for this class,

332
00:40:15,080 --> 00:40:20,660
and as human beings is driven on full auto-pilot mode.

333
00:40:20,660 --> 00:40:27,600
So it's driving by itself 300 million miles as of December 2016

334
00:40:28,740 --> 00:40:36,400
and the fatalities for human control vehicles is 1:90,000,000.

335
00:40:36,820 --> 00:40:46,300
It's about >30,000 fatalities a year and currently under TESLA auto-pilot there's one fatality.

336
00:40:47,440 --> 00:40:55,320
There's a lot of ways you could tear that statistic apart but it's one to think about.  Already, perhaps automation

337
00:40:55,700 --> 00:41:02,620
results in safer driving.  The thing is, we don't understand automation,

338
00:41:03,520 --> 00:41:12,940
because we don't have the data:  we don't have the data on the forward roadway video, we don't have the data on the driver

339
00:41:13,650 --> 00:41:20,500
and we just don't have that many cars on the road today that drive themselves.  So we need a lot of data.

340
00:41:21,150 --> 00:41:29,580
We'll provide some of it to you in the class and as part of our research at MIT were collecting huge amounts of it,

341
00:41:30,200 --> 00:41:38,180
of cars driving themselves, and collecting that data is how we get to understanding.

342
00:41:38,820 --> 00:41:51,220
So talking about the data and what we'll be doing training our algorithms on, here is a Tesla Model S, Model X

343
00:41:51,600 --> 00:41:58,120
we've instrumented 17 of them, have collected over 5,000 hours and 70,000 miles.

344
00:41:58,560 --> 00:42:08,420
And I'll talk about the cameras that we've put in them.  We're collecting video of the forward road way.

345
00:42:08,490 --> 00:42:19,230
This is a highlight of a trip from Boston to Florida of one of the people driving a Tesla.  What's also shown in blue is the

346
00:42:19,230 --> 00:42:26,900
amount of time that autopilot was engaged:  currently 0 minutes and then it grows and grows.

347
00:42:28,220 --> 00:42:34,160
For prolonged periods of time, so hundreds of miles, people engage autopilot.  Out of 1.3 billion

348
00:42:34,590 --> 00:42:41,960
miles driven a Tesla, 300,000,000 are on autopilot.  You do the math whatever that is, 25%.

349
00:42:44,280 --> 00:42:50,820
So we are collecting data of the forward roadway, of the driver.  We have 2 cameras on the driver.

350
00:42:51,220 --> 00:43:05,370
What we're providing with the class is epics of time of the forward roadway, for privacy considerations.  Cameras used

351
00:43:05,370 --> 00:43:13,240
to record are your regular Webcam, the work horse of the computer vision community.  The C920,

352
00:43:14,300 --> 00:43:16,820
and we have some special lenses on top of it.

353
00:43:17,120 --> 00:43:23,120
Now what's special about these webcams?  Nothing that costs $70 can be that good, right?

354
00:43:24,180 --> 00:43:34,440
What's special about them is that they do onboard compression and allow you to collect huge amounts of data

355
00:43:34,440 --> 00:43:42,520
and use reasonably sized storage capacity to store that data and train your algorithms on.

356
00:43:46,860 --> 00:43:54,720
So what on the self-driving side do we have to work with?  How do we build a self-driving car?

357
00:43:55,920 --> 00:44:00,340
There is these sensors:  radar, lidar, vision,

358
00:44:01,180 --> 00:44:09,980
audio - all looking outside helping you detect the objects in the external environment to localize yourself and so on.

359
00:44:10,360 --> 00:44:15,520
And there's the sensors facing inside:  visible light camera, audio again,

360
00:44:15,860 --> 00:44:18,700
and infrared camera to help detect peoples.

361
00:44:19,750 --> 00:44:30,240
So we can decompose the self-driving car task into 4 steps:  localization, answering where am I; scene understanding,

362
00:44:30,900 --> 00:44:38,700
using the texture of the information of the scene around, to interpret the identity of the different objects in the

363
00:44:38,700 --> 00:44:45,700
scene and the semantic meaning of those objects, of their movement.

364
00:44:46,780 --> 00:44:53,500
There's movement planning - once you figured all that out, found all the pedestrians, found all the other cars,

365
00:44:53,500 --> 00:45:04,900
how do I navigate through this maze, a clutter of objects in a safe and legal way.  And there's driver state,

366
00:45:05,360 --> 00:45:08,940
how do I detect using video or other information.

367
00:45:10,480 --> 00:45:16,820
The video of the driver detect information about their emotional state or their distraction level.  Yes, question.

368
00:45:17,360 --> 00:45:21,680
(INAUDIBLE QUESTION)

369
00:45:22,200 --> 00:45:30,080
Yes, that's the real-time figure from lidar.  Lidars are sensors that provides you the 3D point cloud

370
00:45:31,480 --> 00:45:37,680
of the external scene.  So lidar is the technology used by

371
00:45:39,740 --> 00:45:49,480
most folks working with self-driving cars to give you a strong Ground Truth of the objects.  It's probably the best sensor we have

372
00:45:49,920 --> 00:45:59,540
for getting 3D information, the least noisy 3D information about the external environment.  Question.

373
00:46:07,380 --> 00:46:16,540
So autopilot is always changing.  One of the most amazing things about this vehicle is that the updates to autopilot come in

374
00:46:17,040 --> 00:46:18,400
the form of software.

375
00:46:18,940 --> 00:46:24,160
So the amount of time it's available to changes has become more conservative with time.

376
00:46:25,160 --> 00:46:27,720
But in this, this one of the earlier versions,

377
00:46:28,060 --> 00:46:37,220
and it shows, the second line in yellow, shows how often autopilot was available but not turned on.

378
00:46:37,720 --> 00:46:44,180
So the total driving time was 10 hours, autopilot was available 7 hours and was engaged an hour.

379
00:46:44,180 --> 00:46:49,640
This particular person is a responsible driver because what you see or

380
00:46:50,540 --> 00:46:56,140
is a more cautious driver.  What you see is it's raining, autopilot is still available

381
00:46:56,620 --> 00:46:56,960
but-

382
00:46:57,260 --> 00:47:00,380
(INAUDIBLE QUESTION)

383
00:47:00,660 --> 00:47:07,690
the comment was that you shouldn't trust that one fatality number as an indication of safety because the drivers

384
00:47:07,690 --> 00:47:15,340
elect to only engage the system when it's safe to do so.  It's a totally open,

385
00:47:16,260 --> 00:47:27,560
there's a lot bigger arguments for that number than just that one, the question is whether that's a bad

386
00:47:27,560 --> 00:47:32,960
thing so maybe we can trust human beings to engage, you know,

387
00:47:34,230 --> 00:47:42,020
despite the poorly filmed YouTube videos, despite the hype in the media, you're still a human being.

388
00:47:42,030 --> 00:47:46,100
riding 60 miles an hour in a metal box with your life on the line.

389
00:47:46,540 --> 00:47:54,180
You won't engage the system unless you know it's completely safe unless you've built up a relationship with it.

390
00:47:54,380 --> 00:47:59,980
It's not all the stuff you see where a person gets in the back of a Tesla and start sleeping or is playing chess,

391
00:48:00,000 --> 00:48:05,140
or whatever.  That's all for YouTube, the reality is when it's just you in the car

392
00:48:05,140 --> 00:48:09,970
it's still your life on the line and so you're going to do the responsible thing unless perhaps you're a teenager

393
00:48:09,970 --> 00:48:13,260
and so on but that never changes no matter what you're in.

394
00:48:14,620 --> 00:48:15,120
Question.

395
00:48:15,120 --> 00:48:17,100
(INAUDIBLE QUESTION)

396
00:48:17,100 --> 00:48:19,720
The question was:  "what do you need to see

397
00:48:19,860 --> 00:48:26,030
or sense about the external environment to be able to successfully drive?  Do you need lane markings?  Do you need other-  

398
00:48:26,030 --> 00:48:29,840
what are the landmarks based on which you do the localization and navigation?"

399
00:48:29,840 --> 00:48:37,420
And that depends on the sensors.  So with the Google self-driving car in sunny California,

400
00:48:37,740 --> 00:48:45,760
it depends on lidar in a high-resolution way, map the environment in order to be able to localize itself

401
00:48:46,040 --> 00:48:54,060
based on lidar.  And lidar, now I don't know the details of exactly where lidar fails,

402
00:48:54,870 --> 00:48:59,700
but it's not good with rain, it's not good with snow, it's not good 

403
00:48:59,700 --> 00:49:06,790
when the environment is changing.  So what snow does is it changes the visual, the appearance, the reflective texture

404
00:49:06,790 --> 00:49:11,040
of the surfaces around.  Us human beings are still able to figure stuff out

405
00:49:11,290 --> 00:49:18,540
but a car that's relying heavily on lidar won't be able to localize itself using the landmarks

406
00:49:18,540 --> 00:49:20,860
it previously has detected because they look different

407
00:49:20,860 --> 00:49:26,940
now with the snow.  Computer vision can help us with lanes

408
00:49:27,410 --> 00:49:35,120
or following a car.  The two landmarks that we used in a lane is following the car in front of you

409
00:49:35,120 --> 00:49:41,680
or staying between two lanes.  That's the nice thing about our roadways it's they're designed for human eyes.

410
00:49:42,060 --> 00:49:49,800
So you can use computer vision for lanes and for cars in front to follow them. And there is radar.

411
00:49:49,820 --> 00:49:58,560
That's a crude but a reliable source of distance information that allows you to not collide with metal objects.

412
00:49:58,560 --> 00:50:04,760
So all that together depending on what you want to rely on more gives you a lot of information.

413
00:50:04,940 --> 00:50:13,480
The question is when its messy complexity of real life occurs,

414
00:50:13,800 --> 00:50:26,220
how reliable it would be in the urban environment and so on.  So localization-  How can deep learning help?

415
00:50:26,220 --> 00:50:40,640
So first, just a quick summary of visual odometry.  It's using a monocular or stereo input of video images

416
00:50:41,780 --> 00:50:51,380
to determine your orientation in the world.  The orientation, in this case, of a vehicle in the frame of the world

417
00:50:52,120 --> 00:50:55,180
and all you have to work with is a video of the forward roadway

418
00:50:57,180 --> 00:51:03,180
and with stereo you get a little extra information of how far away different objects are.

419
00:51:06,820 --> 00:51:14,820
And so this is where one of our speakers on Friday will talk about his expertise (SLAM) Simultaneous Localization and Mapping.

420
00:51:14,820 --> 00:51:24,160
This is a very well-studied and understood problem of detecting unique features in the external scene

421
00:51:25,290 --> 00:51:31,620
and localizing yourself based on the trajectory of those unique features.

422
00:51:32,020 --> 00:51:36,460
When the number of features is high enough it becomes an optimization problem.

423
00:51:37,140 --> 00:51:41,820
You know this particular lane moved a little bit from frame to frame you can track that information.

424
00:51:43,170 --> 00:51:49,860
And fuse everything together in order to be able to estimate your trajectory through the three dimensional space.

425
00:51:50,680 --> 00:51:59,660
You also have other sensors to help you out.  You have GPS which is pretty accurate, not perfect but pretty accurate.

426
00:51:59,660 --> 00:52:01,980
It's another signal to help you localize yourself.

427
00:52:02,560 --> 00:52:05,120
You also have IMU.  Accelerometer

428
00:52:05,120 --> 00:52:15,200
tells you your acceleration, from the gyroscope, the accelerometer, you have the six degree of freedom

429
00:52:15,500 --> 00:52:24,160
of movement information about how the moving object, the car, is navigating through space.

430
00:52:25,910 --> 00:52:34,740
So you can do that using the old school way of optimization.

431
00:52:36,030 --> 00:52:46,620
Given a unique set of features, like sift features, and that step involves with stereo input understorting and

432
00:52:46,620 --> 00:52:53,300
and rectifying the images.  You have two images, from the two images compute the depth map but for every single pixel

433
00:52:53,300 --> 00:52:54,150
computing the

434
00:52:54,600 --> 00:53:06,340
best estimate of the depth of that pixel, the three dimensional position, relative to the camera then you

435
00:53:06,340 --> 00:53:13,080
compute, that's where you compute the disparity map, that's what that's called, from which you get the distance

436
00:53:14,010 --> 00:53:19,480
then you detect unique, interesting features in the scene.  Sift is a popular one.

437
00:53:20,300 --> 00:53:25,700
It's a popular algorithm for detecting unique features and you, over time, track those features.

438
00:53:26,160 --> 00:53:33,090
And that tracking is what allows you through the vision alone to get information about your trajectory

439
00:53:33,090 --> 00:53:40,660
through three-dimensional space.  You estimate that trajectory.  There's a lot of assumptions, assumptions that bodies are rigid.

440
00:53:42,630 --> 00:53:49,620
So you have to figure out if a large object passes right in front of you, you have to figure out what that was.

441
00:53:52,520 --> 00:53:59,100
You have to figure out mobile objects in the scene.  And those are the stationary.

442
00:54:00,880 --> 00:54:08,720
Or you can cheat or we'll talk about and do it using neural networks end-to-end.

443
00:54:09,380 --> 00:54:15,920
Now what does end-to-end mean?  And this will come up a bunch of times throughout this class and today.  End-to-end means,

444
00:54:17,000 --> 00:54:28,020
and I refer to it as cheating because it takes away a lot of the hard work of panageneric features.  You take the raw input

445
00:54:28,020 --> 00:54:37,580
of whatever sensors.  In this case, it's taking stereo input from a stereo vision cameras so two images, a sequence of two

446
00:54:37,580 --> 00:54:46,960
images coming from a stereo vision camera, and the output is a estimate of your trajectory through space.

447
00:54:47,630 --> 00:54:53,580
So it's supposed to be doing the hard work of SLAM, of detecting unique features, of localizing yourself, of tracking those

448
00:54:53,580 --> 00:54:58,300
features and figuring out where your trajectory is.  You simply train the network.

449
00:54:59,780 --> 00:55:03,600
With some Ground Truth, you have form a more accurate sensor like lidar,

450
00:55:04,830 --> 00:55:08,920
and you train it on a set of inputs, the stereo vision inputs,

451
00:55:09,340 --> 00:55:17,580
and outputs is the trajectory through space.  You have a separate convolutional neural networks for the velocity

452
00:55:18,020 --> 00:55:24,860
and for the orientation.  And this works pretty well.  Unfortunately, not quite well

453
00:55:25,930 --> 00:55:35,460
and John Leonard will talk about that.  SLAM is one of the places were deep learning is not being able to outperform the

454
00:55:35,460 --> 00:55:36,460
previous approaches.

455
00:55:37,540 --> 00:55:44,440
Where deep learning really helps is the scene understanding part.  It's interpreting the objects in the scene.

456
00:55:45,390 --> 00:55:50,620
It's detecting the various parts of the scene, segmenting them

457
00:55:51,060 --> 00:55:58,300
and with optical flow determining their movement.  So previous approaches for detecting objects

458
00:56:00,400 --> 00:56:08,460
like the traffic signal, the classification of detection that we have the TensorFlow tutorial for

459
00:56:09,100 --> 00:56:17,340
or to use car-like features or other types of features that are hard-engineered from the images.

460
00:56:18,760 --> 00:56:24,100
Now we can use convolutional neural networks to replace the extraction of those features.

461
00:56:30,490 --> 00:56:34,200
And there's TensorFlow implementation of SegNet

462
00:56:35,060 --> 00:56:44,000
which is taking the exact same neural network that I talked about.  It's the same thing, the beauty is you just apply

463
00:56:44,280 --> 00:56:47,020
similar types of networks to different problems

464
00:56:47,300 --> 00:56:55,150
and depending on the complexity of the problem, can get quite amazing performance.  In this case, we convolutionize

465
00:56:55,150 --> 00:57:02,000
network, meaning the output is an image, input is an image, a single monocular image.  The output is a

466
00:57:03,360 --> 00:57:10,120
segmented image where the colors indicate your best pixel-by-pixel estimate of what object is in that part.

467
00:57:10,480 --> 00:57:15,840
This is not using any spatial information, it's not using any temporal information.

468
00:57:16,260 --> 00:57:25,000
So it's processing every single frame separately and it's able to separate the road from the trees,

469
00:57:25,310 --> 00:57:29,280
from the pedestrians, other cars, and so on.

470
00:57:30,710 --> 00:57:39,420
This is intended to lie on top of a radar / lidar type of technology that's giving you the three dimensional

471
00:57:39,420 --> 00:57:45,730
or stereo vision three-dimensional information about the scene.  You're, sort of, painting that scene with the identity of

472
00:57:45,730 --> 00:57:49,880
the objects that are in it, your best estimate of it.

473
00:57:50,600 --> 00:57:56,140
This is something I'll talk about tomorrow is recurring neural networks

474
00:57:56,340 --> 00:58:02,500
and we can use recurring neural networks that work with temporal data to process video

475
00:58:02,900 --> 00:58:11,600
and also process audio.  In this case, we can process what's shown on the bottom is

476
00:58:11,900 --> 00:58:15,360
a spectrogram of audio for a wet road and a dry road.

477
00:58:16,260 --> 00:58:20,660
You can look at that spectrogram as an image

478
00:58:22,700 --> 00:58:30,300
and process it in a temporal way using recurring neural networks.  Just slide it across and keep feeding it to a network.

479
00:58:31,570 --> 00:58:39,940
And it does incredibly well on the simple tasks, certainly of dry road versus wet road.  This is important,

480
00:58:39,940 --> 00:58:49,700
a subtle, but very important task and there's many like it to know the road, the texture, the quality.,

481
00:58:50,440 --> 00:58:53,920
the characteristics of the road, wetness being a critical one.

482
00:58:53,920 --> 00:58:57,820
When it's not raining but the road is still wet, that information is very important.

483
00:59:00,800 --> 00:59:13,200
Okay, so for movement planning.  The same kind of approach.  On the right is work from one of our other speakers

484
00:59:13,200 --> 00:59:22,620
Sertec Karaman.  The same approach we're using to solve traffic through friendly competition

485
00:59:23,060 --> 00:59:32,780
is the same that we can use for what Chris Gerdes does with his race cars for planning trajectories in high speed movement

486
00:59:33,140 --> 00:59:37,960
along complex curve.

487
00:59:40,430 --> 00:59:46,700
So we can solve that problem using optimization, solve the control problem using optimization,

488
00:59:46,940 --> 00:59:50,460
or we can use it with reinforcement learning by running

489
00:59:51,900 --> 00:59:56,080
tens of millions, hundreds of millions of times through that simulation of taking that curve

490
00:59:56,520 --> 00:59:59,760
and learning which trajectory doesn't both optimizes

491
00:59:59,760 --> 01:00:03,180
the speed at which you take the turn

492
01:00:03,180 --> 01:00:10,940
and the safety of the vehicle.  Exactly the same thing that you're using for traffic.

493
01:00:12,690 --> 01:00:19,740
And for driver state, this is what will talk about next week.  It's all the fun face stuff:  eyes, face, emotion.

494
01:00:21,490 --> 01:00:29,900
This is with video of the driver, video of the driver's body, video the driver's face.  On the left is one of the TAs

495
01:00:30,220 --> 01:00:42,980
in his younger days.  Still looks the same.  There he is.  So in that particular case,

496
01:00:43,780 --> 01:00:51,740
you're doing one of the easier problems which is one of detecting where the head and the eyes are positioned.

497
01:00:51,920 --> 01:00:54,960
The head and eye pose.  You know it determine what's called

498
01:00:55,700 --> 01:01:01,760
he gaze of the driver, where the driver's looking, glance.  And so,

499
01:01:01,760 --> 01:01:09,770
we'll talk about these problems.  From the left to the right:  on the left in green are the easier problems; on the red

500
01:01:09,770 --> 01:01:12,300
are the harder from the computer vision aspect.

501
01:01:13,110 --> 01:01:18,200
So on the left is body pose, head pose.  The larger the object the easier it is the detect

502
01:01:18,200 --> 01:01:20,300
and the orientation of it is easier to detect.

503
01:01:20,960 --> 01:01:28,760
And then there is pupil diameter.  Detecting the pupil, the characteristics, the position, the size of the pupil.

504
01:01:29,280 --> 01:01:31,410
And there's micro saccade, things that happen

505
01:01:31,410 --> 01:01:41,120
at one millisecond frequency, the tremors of the eye.  All important information to determine the state of the driver.

506
01:01:41,980 --> 01:01:51,500
Some are possible computer vision, some are not.  This is something that we'll talk about, I think, on Thursday.

507
01:01:52,340 --> 01:01:58,600
Is the detection of where the driver's looking.  So, this is a bunch of the cameras that we have in the Tesla.  This is

508
01:01:58,600 --> 01:02:05,100
This is Dan driving a Tesla and detecting exactly where of one of six regions

509
01:02:05,160 --> 01:02:12,030
We've converted into a classification problem of left, right, rear view mirror instrument cluster center stack

510
01:02:12,030 --> 01:02:15,260
or forward roadway. So we have to determine out of those six categories

511
01:02:15,260 --> 01:02:23,380
which direction is the driver looking at.  This is important for driving.  We don't care exactly the X, Y, Z

512
01:02:23,380 --> 01:02:27,480
position of where the driver is looking at.  We care that they're looking at the road or not.

513
01:02:27,480 --> 01:02:31,060
Are they looking at their cell phone in their lap or are they looking at the forward roadway?

514
01:02:31,220 --> 01:02:35,400
And we'll be able to answer that pretty effectively using convolutional neural networks.

515
01:02:45,360 --> 01:02:51,280
You can also look at emotion using CNNs to extract,

516
01:02:52,860 --> 01:03:00,520
again converting emotion, the complex world of emotion, into a binary problem of frustrated versus satisfied.

517
01:03:02,040 --> 01:03:08,270
This is the video of drivers interacting with a voice navigation system.  If you've ever used one,

518
01:03:08,270 --> 01:03:10,760
you know that may be a source of frustration from folks.

519
01:03:12,120 --> 01:03:16,180
And so this is self reported, this is one of the hard, you know, driver

520
01:03:16,180 --> 01:03:23,960
emotion if you're in what's called "Effective Computing."  It's the field of studying emotion from the computational side.

521
01:03:25,760 --> 01:03:28,820
If you work in that field,

522
01:03:28,820 --> 01:03:32,560
you know that the annotation side of emotion is really challenging one.

523
01:03:32,560 --> 01:03:36,720
So getting the Ground Truth of, well okay since this guy's smiling

524
01:03:37,620 --> 01:03:45,260
so can I label that as happy or he's frowning because that mean he's sad.  Most effective computing folks do just that.

525
01:03:46,240 --> 01:03:52,520
In this case we self report ask people how frustrated they're were in a scale of 1 to 10.

526
01:03:53,180 --> 01:04:00,060
Dan up top reported a "1" for not frustrated, he's satisfied with the interaction,

527
01:04:00,440 --> 01:04:06,320
and the other driver reported as a "9" he was very frustrated with the interaction.  And what you notice

528
01:04:06,530 --> 01:04:11,900
is there is a very cold, stoic look on Dan's face which is an indication of happiness.

529
01:04:12,900 --> 01:04:18,000
And in the case of frustration, the driver is smiling.

530
01:04:18,580 --> 01:04:24,180
So this is a sort of a good reminder that we can't trust our own human instincts.

531
01:04:24,180 --> 01:04:34,120
It's an engineering feature.  Engineering the ground truth.  We have to trust the data, trust the Ground Truth

532
01:04:34,580 --> 01:04:39,920
that we believe is the closest reflection of the actual semantics of what's going on in the scene.

533
01:04:44,600 --> 01:04:49,460
Okay, so end-to-end driving.  Getting to the the project and the tutorial.

534
01:04:49,960 --> 01:05:04,600
So if driving is like a conversation and, thank you for someone to clarifying, that this is the Arch of Triumph

535
01:05:04,600 --> 01:05:07,720
in Paris in this video.

536
01:05:08,780 --> 01:05:18,820
If driving is like a natural language conversation, then we can think of end-to-end driving as skipping the entire Turing Test

537
01:05:18,820 --> 01:05:23,960
components and treating it as an end-to-end natural language generation.

538
01:05:24,570 --> 01:05:29,340
So what we do is we take as input the external sensors

539
01:05:29,800 --> 01:05:36,300
and output, the control of the vehicle.  And the magic happens in the middle.

540
01:05:38,050 --> 01:05:41,840
We replace that entire step with a neural network.

541
01:05:43,310 --> 01:05:53,380
TAs told me to not include this image because it's the cheesiest we've ever seen.  I apologize.  Thank you, thank you.

542
01:05:56,720 --> 01:05:58,020
I regret nothing.

543
01:06:00,820 --> 01:06:06,240
So this is to show our path to self-driving cars

544
01:06:06,240 --> 01:06:12,140
but it still explain a point that we have a large data set of Ground Truth.

545
01:06:13,000 --> 01:06:17,120
If we were to formulate the driving task to simply taking external images

546
01:06:18,220 --> 01:06:24,780
and producing steering commands, acceleration of braking commands, then we have a lot of Ground Truth.

547
01:06:25,280 --> 01:06:31,800
We have a large number of drivers on the road every day driving

548
01:06:31,980 --> 01:06:38,360
and, therefore, collecting our Ground Truth for us because they're an interested party in producing the steering commands

549
01:06:38,580 --> 01:06:44,280
that keep them alive and, therefore, if we were to record that data it becomes Ground Truth.

550
01:06:44,760 --> 01:06:46,520
So if it's possible to learn this,

551
01:06:46,700 --> 01:06:50,680
what we can do is we can collect data for the manually controlled vehicles

552
01:06:50,920 --> 01:06:58,360
and use that data to train an algorithm to control a self-driving vehicle.

553
01:07:01,060 --> 01:07:09,680
Okay, so one of the first folks who did this is Nvidia where they actually train in an external image, the image of the forward roadway.

554
01:07:10,760 --> 01:07:17,900
and a neural network, a convolutional network, a simple vanilla convolutional neural network I'll briefly outline:

555
01:07:18,900 --> 01:07:22,900
take an image in, produce a steering command out

556
01:07:23,530 --> 01:07:33,960
and they're able to successfully, to some degree, learn to navigate basic turns, curves and even stop

557
01:07:33,960 --> 01:07:42,560
or make sharp turns at a keener section.  So this this now work is simple.

558
01:07:43,000 --> 01:07:51,300
There is input on the bottom, output up top.  The input is a 66x200 pixel image, RGB.

559
01:07:54,020 --> 01:07:58,560
Shown on the left is the raw input and then you crop it a little bit and resize it down

560
01:07:59,380 --> 01:08:07,520
66x200.   That's what we have in the code as well in the two versions of the code we'll provide to you.

561
01:08:07,700 --> 01:08:19,560
Both that runs in the browser and in TensorFlow.  It has a few layers.  A few convolutional layers, a few fully connected layers.

562
01:08:20,200 --> 01:08:23,940
And an output.  This is a regression network.

563
01:08:24,000 --> 01:08:29,060
It's producing not a classification of cat versus dog, it's producing a steering command.

564
01:08:29,300 --> 01:08:39,040
How do I turn the steering wheel?  That's it.  The rest is magic and we train it on a human input.

565
01:08:44,890 --> 01:08:53,580
What we have here is a project, is an implementation of the system in ConvNetJS that runs in your browser.

566
01:08:54,080 --> 01:09:07,220
This is the tutorial to follow and the project to take on.  So unlike the DeepTraffic game, this is reality.

567
01:09:07,560 --> 01:09:10,700
This is a real input from real vehicles.

568
01:09:11,940 --> 01:09:21,080
So you can go to this link.  Demo went wonderfully yesterday so let's see, maybe two for two.

569
01:09:32,880 --> 01:09:39,900
There's the tutorial and then the actual game, the actual simulation is on DeepTesla.JS, I apologize.

570
01:09:46,680 --> 01:09:53,760
Everyone is going there now, aren't they?  Does it work on a phone?  It does, great.

571
01:09:57,740 --> 01:10:06,580
Again similar structure up top is the visualization of the lost function as the network is learning

572
01:10:06,780 --> 01:10:08,060
and it's always training.

573
01:10:12,880 --> 01:10:21,020
Next is the input for the layout of the network, there's the specification of the input 200x66.

574
01:10:22,680 --> 01:10:30,800
There's a convolutional layer. There's a pooling layer and the output is a regression layer.  A single neuron.

575
01:10:31,430 --> 01:10:38,580
This is a tiny version, DeepTiny, right?  It's a tiny version of

576
01:10:40,180 --> 01:10:49,980
the Nvidia architecture and then you can visualize the operation of this network on real video.

577
01:10:53,480 --> 01:10:59,380
The actual wheel value that produced by the driver, by the autopilot system,

578
01:11:00,780 --> 01:11:04,060
is in blue and the output of the network is in white.

579
01:11:08,980 --> 01:11:18,380
And what's indicated by green is the cropping of the image that is then resized to produce the 66x200

580
01:11:18,380 --> 01:11:29,020
input to the network.  So once again, amazingly, this is running in your browser, training on real world video.

581
01:11:29,620 --> 01:11:35,100
So you can get in your car today input it and maybe teach a neural network to drive like you.

582
01:11:36,050 --> 01:11:38,000
We have the code in ConvNetJS

583
01:11:38,000 --> 01:11:49,480
and TensorFlow to do that and the tutorial.  Well, let me briefly describe some of the work here.

584
01:11:50,910 --> 01:11:54,360
So the input to the network as a single image.

585
01:11:54,680 --> 01:12:02,000
This is for DeepTesla.JS, single image and the output is a steering wheel value between -20 and 20.

586
01:12:02,280 --> 01:12:07,300
That's in degrees.  We record,

587
01:12:08,750 --> 01:12:16,280
like I said, thousands of hours but we provide publicly 10 video clips of highway driving from a Tesla.

588
01:12:17,290 --> 01:12:27,680
Half are driven by autopilot, half are driven by human.  The wheel values extracted from a perfectly synchronized

589
01:12:29,390 --> 01:12:33,260
CAN, we are collecting all of the messages from CAN,

590
01:12:33,920 --> 01:12:40,840
which contains steering wheel value and that's synchronized to the video.  We crop, extract the window.  The green one I mentioned.

591
01:12:42,460 --> 01:12:44,360
And then provide that as input to the network.

592
01:12:46,010 --> 01:12:53,030
So this is a slight difference from DeepTraffic with the red car weaving through traffic because there is the messy

593
01:12:53,030 --> 01:12:56,520
reality of real world lighting conditions.

594
01:12:57,720 --> 01:13:04,740
And your task for the most part, in this simple steering task, is to stay inside the lane,

595
01:13:05,400 --> 01:13:14,310
inside the lane markings.  In an end-to-end way, learn to do just that.  So ConvNetJS

596
01:13:14,310 --> 01:13:25,400
is a javascript implementation of CNNs, of convolutional neural networks.  It supports really arbitrary networks.

597
01:13:26,200 --> 01:13:32,200
I mean all neural networks are simple but because it runs in javascript it's not utilizing GPU.

598
01:13:33,020 --> 01:13:39,280
The larger the network the more it's going to be weighed down computationally.

599
01:13:40,600 --> 01:13:44,900
Now unlike DeepTraffic, this isn't a competition

600
01:13:45,460 --> 01:13:50,840
but if you are a student registered for the course you still do have to submit the code, you still have to submit your own

601
01:13:50,840 --> 01:14:01,800
car as part of the class.  Question.  So the question was the amount of data that's needed.

602
01:14:03,680 --> 01:14:10,380
Is there a general rules of thumb for the amount of data needed for a particular task in driving for example?

603
01:14:11,570 --> 01:14:12,700
It's a good question.

604
01:14:13,960 --> 01:14:23,240
You generally have to, like I said, neural networks are good memorizers so you have to just have every case represented in the

605
01:14:23,240 --> 01:14:32,160
training said that you're interested in.  As much as possible, so that means, in general if you want a picture, if you want to

606
01:14:32,160 --> 01:14:37,190
classify the difference between cats and dogs, you want to have at least a thousand cats and a thousand dogs

607
01:14:37,190 --> 01:14:46,740
and they do really well.  The problem with driving is twofold:  one, is that most of the time driving looks the same.

608
01:14:47,380 --> 01:14:51,580
And the stuff you really care about is when driving looks different.  It's all the edge cases.

609
01:14:52,220 --> 01:14:59,880
So we're not good with neural networks is generalizing from the common case to the edge cases, to the outliers.

610
01:15:00,670 --> 01:15:02,050
So avoiding a crash

611
01:15:02,050 --> 01:15:07,700
just because you can stand the highway for thousands of hours successfully doesn't mean you can avoid a crash with

612
01:15:07,700 --> 01:15:09,680
somebody runs in front of you on the road

613
01:15:10,280 --> 01:15:20,120
and the other part with driving is the accuracy you have to achieve is really high.  So for cat versus dog,

614
01:15:20,360 --> 01:15:28,380
No, life doesn't depend on your error.  On your ability to steer a car inside of the lane.

615
01:15:28,740 --> 01:15:36,120
You better be very close to 100% accurate. There's a box for designing the network.

616
01:15:36,870 --> 01:15:41,400
There's a visualization of the metrics measuring the performance of the network as it trains.

617
01:15:42,060 --> 01:15:50,960
There is a visualization, layer visualization, of what features the network is extracting at every convolutional layer

618
01:15:51,120 --> 01:15:58,800
and every fully connected layer.  There is ability to restart the training.

619
01:15:59,610 --> 01:16:15,300
Visualize the network performing on real video.  There is the input layer, the convolutional layers.

620
01:16:19,980 --> 01:16:33,600
The video visualization, an interesting tidbit on the bottom right is a barcode that Will has ingeniously designed.

621
01:16:36,460 --> 01:16:39,900
How do I clearly explain why this is so cool?

622
01:16:40,300 --> 01:16:47,460
It's a way to through video synchronized multiple streams of data together,

623
01:16:47,980 --> 01:16:53,640
so it's very easy for those who have worked with multi-modal data where there are several streams of data

624
01:16:53,960 --> 01:17:01,580
for them to become unsynchronized especially when a big component of training a neural network is shuffling the data.

625
01:17:02,000 --> 01:17:04,800
So you have to shuffle the data in clever ways

626
01:17:05,080 --> 01:17:11,000
so you're not overfitting any one little aspect of the video and yet maintain the data perfectly synchronized.

627
01:17:11,540 --> 01:17:16,220
So what he did instead doing the hard work of connecting the steering wheel

628
01:17:16,220 --> 01:17:23,300
and in the video is actually putting the steering on top of the video as a barcode.

629
01:17:25,400 --> 01:17:36,680
The final result is you can watch the network operate and over time it learns more and more to steer correctly.

630
01:17:37,000 --> 01:17:41,370
I'll fly through this a little bit in the interest of time just kind of summarize some of the things that you can play

631
01:17:41,370 --> 01:17:49,580
with in terms of tutorials and let you guys go.  This is the same kind of process end-to-end driving with

632
01:17:49,960 --> 01:17:57,360
So we have code available on GetHub.  You just put up on my GetHub and the DeepTesla.

633
01:17:57,760 --> 01:18:03,320
That takes in a single video or an arbitrary number of videos trains on them

634
01:18:03,860 --> 01:18:09,820
and produces a visualization that compares the steering wheel, the actual steering wheel and the predicted steering wheel.

635
01:18:11,060 --> 01:18:16,260
The steering wheel, when it agrees with the human driver or the autopilot system lighting up as green

636
01:18:16,260 --> 01:18:20,320
and when it disagrees, lighting up as red.  Hopefully not too often.

637
01:18:21,480 --> 01:18:26,160
Again, this is some of the details of how that's exactly done in TensorFlow.

638
01:18:26,500 --> 01:18:28,510
This is vanilla convolution neural networks.

639
01:18:29,180 --> 01:18:39,060
Specifying a bunch of layers, convolutional layers, a fully connected layer, train the model, so you iterate over the batches of images.

640
01:18:42,420 --> 01:18:53,960
Run the model over a test set of images and get this result.  We have a tutorial

641
01:18:55,720 --> 01:18:59,420
on iPython Notebook into the tutorial up on this.

642
01:18:59,420 --> 01:19:06,020
This is perhaps the best way to get started with convolutional neural networks in terms of our class.  It's looking at the

643
01:19:06,280 --> 01:19:14,020
simplest image classification problem, of traffic light classification.  So we have these images of traffic lights.

644
01:19:14,930 --> 01:19:17,040
We did the hard work of detecting them for you.

645
01:19:17,680 --> 01:19:22,800
So now you have to figure out, you have to build the convolutional network that gets

646
01:19:24,800 --> 01:19:33,460
figures out the concept of color and gets excited when it sees red, yellow or green.  If anyone has questions,

647
01:19:34,260 --> 01:19:39,600
I'll welcome those.  You can stay after class if you have any concerns with Docker,

648
01:19:39,900 --> 01:19:50,520
with TensorFlow, with how to win DeepTraffic.  Just stay after class or come by Friday, 5 to 7.  See you guys tomorrow.

